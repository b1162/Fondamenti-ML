{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparazione e Pre-elaborazione dei Dati\n",
    "## Esercitazioni Pratiche\n",
    "\n",
    "Questo notebook contiene esercitazioni pratiche sui tre temi principali del corso:\n",
    "1. Raccolta e pulizia dei dati\n",
    "2. Feature engineering\n",
    "3. Tecniche di suddivisione e validazione dei dati\n",
    "\n",
    "Attraverso esempi concreti e dataset reali, metteremo in pratica le tecniche apprese durante le lezioni teoriche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione dell'ambiente\n",
    "\n",
    "Iniziamo installando e importando le librerie necessarie per le nostre esercitazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installazione delle librerie necessarie\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn missingno yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importazione delle librerie\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import missingno as msno\n",
    "import warnings\n",
    "\n",
    "# Configurazione del notebook\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Raccolta e Pulizia dei Dati\n",
    "\n",
    "In questa prima parte, ci concentreremo su:\n",
    "- Caricamento di dati da diverse fonti\n",
    "- Esplorazione e comprensione dei dati\n",
    "- Identificazione e gestione dei valori mancanti\n",
    "- Rilevamento e trattamento degli outlier\n",
    "- Normalizzazione e standardizzazione\n",
    "- Gestione dei dati duplicati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Caricamento dei dati\n",
    "\n",
    "Iniziamo caricando un dataset reale. Utilizzeremo il dataset \"House Prices\" di Kaggle, che contiene informazioni sulle case vendute ad Ames, Iowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scaricare il dataset\n",
    "!wget https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv -O titanic.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Caricare il dataset\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Visualizzare le prime righe\n",
    "print(f\"Dimensioni del dataset: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Esplorazione e comprensione dei dati\n",
    "\n",
    "Prima di iniziare la pulizia, è importante esplorare e comprendere i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Informazioni generali sul dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Statistiche descrittive\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la distribuzione della variabile target (sopravvissuti)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='Survived', data=df)\n",
    "plt.title('Distribuzione dei sopravvissuti')\n",
    "plt.xlabel('Sopravvissuto (1 = Sì, 0 = No)')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare la relazione tra classe e sopravvivenza\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Pclass', hue='Survived', data=df)\n",
    "plt.title('Sopravvivenza per classe')\n",
    "plt.xlabel('Classe (1 = Prima classe, 2 = Seconda classe, 3 = Terza classe)')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.legend(title='Sopravvissuto', labels=['No', 'Sì'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare la relazione tra sesso e sopravvivenza\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Sex', hue='Survived', data=df)\n",
    "plt.title('Sopravvivenza per sesso')\n",
    "plt.xlabel('Sesso')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.legend(title='Sopravvissuto', labels=['No', 'Sì'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribuzione dell'età\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['Age'].dropna(), kde=True, bins=30)\n",
    "plt.title('Distribuzione dell\\'età')\n",
    "plt.xlabel('Età')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Identificazione e gestione dei valori mancanti\n",
    "\n",
    "Ora analizziamo i valori mancanti nel dataset e applichiamo strategie appropriate per gestirli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Conteggio dei valori mancanti per colonna\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valori mancanti': missing_values,\n",
    "    'Percentuale': missing_percent\n",
    "})\n",
    "\n",
    "missing_df[missing_df['Valori mancanti'] > 0].sort_values('Percentuale', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzazione dei valori mancanti\n",
    "msno.matrix(df)\n",
    "plt.title('Matrice dei valori mancanti')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "msno.heatmap(df)\n",
    "plt.title('Heatmap delle correlazioni tra valori mancanti')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategie per gestire i valori mancanti\n",
    "\n",
    "Implementiamo diverse strategie per gestire i valori mancanti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Creare una copia del dataframe per non modificare l'originale\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# 1. Eliminazione: rimuovere la colonna 'Cabin' (troppi valori mancanti)\n",
    "df_cleaned.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "# 2. Imputazione: sostituire i valori mancanti in 'Age' con la mediana\n",
    "df_cleaned['Age'].fillna(df_cleaned['Age'].median(), inplace=True)\n",
    "\n",
    "# 3. Imputazione: sostituire i valori mancanti in 'Embarked' con la moda\n",
    "most_common_embarked = df_cleaned['Embarked'].mode()[0]\n",
    "df_cleaned['Embarked'].fillna(most_common_embarked, inplace=True)\n",
    "\n",
    "# 4. Imputazione avanzata: sostituire i valori mancanti in 'Fare' con la mediana per classe\n",
    "for pclass in [1, 2, 3]:\n",
    "    median_fare = df_cleaned[df_cleaned['Pclass'] == pclass]['Fare'].median()\n",
    "    df_cleaned.loc[(df_cleaned['Fare'].isnull()) & (df_cleaned['Pclass'] == pclass), 'Fare'] = median_fare\n",
    "\n",
    "# Verificare che non ci siano più valori mancanti\n",
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Rilevamento e trattamento degli outlier\n",
    "\n",
    "Ora identifichiamo e gestiamo gli outlier nei dati numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la distribuzione delle variabili numeriche con box plot\n",
    "numeric_cols = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.boxplot(y=df_cleaned[col], ax=axes[i])\n",
    "    axes[i].set_title(f'Box Plot di {col}')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identificare gli outlier usando il metodo IQR\n",
    "def identify_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Identificare gli outlier nella tariffa (Fare)\n",
    "fare_outliers, lower_bound, upper_bound = identify_outliers(df_cleaned, 'Fare')\n",
    "print(f\"Numero di outlier in 'Fare': {len(fare_outliers)}\")\n",
    "print(f\"Limite inferiore: {lower_bound:.2f}, Limite superiore: {upper_bound:.2f}\")\n",
    "print(\"\\nEsempi di outlier:\")\n",
    "fare_outliers[['PassengerId', 'Pclass', 'Fare']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trattare gli outlier con diverse strategie\n",
    "df_no_outliers = df_cleaned.copy()\n",
    "\n",
    "# 1. Rimozione degli outlier\n",
    "# df_no_outliers = df_no_outliers[(df_no_outliers['Fare'] >= lower_bound) & (df_no_outliers['Fare'] <= upper_bound)]\n",
    "\n",
    "# 2. Capping (winsorization)\n",
    "df_no_outliers['Fare_capped'] = df_no_outliers['Fare'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# 3. Trasformazione logaritmica\n",
    "df_no_outliers['Fare_log'] = np.log1p(df_no_outliers['Fare'])  # log(1+x) per gestire valori zero\n",
    "\n",
    "# Visualizzare l'effetto delle trasformazioni\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(df_no_outliers['Fare'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribuzione originale di Fare')\n",
    "axes[0].set_xlabel('Fare')\n",
    "\n",
    "sns.histplot(df_no_outliers['Fare_capped'], kde=True, ax=axes[1])\n",
    "axes[1].set_title('Distribuzione di Fare dopo capping')\n",
    "axes[1].set_xlabel('Fare_capped')\n",
    "\n",
    "sns.histplot(df_no_outliers['Fare_log'], kde=True, ax=axes[2])\n",
    "axes[2].set_title('Distribuzione di Fare dopo trasformazione logaritmica')\n",
    "axes[2].set_xlabel('Fare_log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Normalizzazione e standardizzazione\n",
    "\n",
    "Applichiamo tecniche di scaling ai dati numerici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selezionare le feature numeriche\n",
    "numeric_features = ['Age', 'Fare_log', 'SibSp', 'Parch']\n",
    "X_numeric = df_no_outliers[numeric_features].copy()\n",
    "\n",
    "# Applicare diverse tecniche di scaling\n",
    "# 1. Standardizzazione (Z-score)\n",
    "scaler = StandardScaler()\n",
    "X_standardized = pd.DataFrame(\n",
    "    scaler.fit_transform(X_numeric),\n",
    "    columns=[f\"{col}_standardized\" for col in X_numeric.columns]\n",
    ")\n",
    "\n",
    "# 2. Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_min_max = pd.DataFrame(\n",
    "    min_max_scaler.fit_transform(X_numeric),\n",
    "    columns=[f\"{col}_min_max\" for col in X_numeric.columns]\n",
    ")\n",
    "\n",
    "# Combinare i risultati\n",
    "scaling_results = pd.concat([X_numeric, X_standardized, X_min_max], axis=1)\n",
    "scaling_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare l'effetto dello scaling\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 15))\n",
    "\n",
    "# Dati originali\n",
    "scaling_results[numeric_features].boxplot(ax=axes[0])\n",
    "axes[0].set_title('Dati originali')\n",
    "\n",
    "# Dati standardizzati\n",
    "standardized_cols = [f\"{col}_standardized\" for col in numeric_features]\n",
    "scaling_results[standardized_cols].boxplot(ax=axes[1])\n",
    "axes[1].set_title('Dati standardizzati (Z-score)')\n",
    "\n",
    "# Dati normalizzati (Min-Max)\n",
    "min_max_cols = [f\"{col}_min_max\" for col in numeric_features]\n",
    "scaling_results[min_max_cols].boxplot(ax=axes[2])\n",
    "axes[2].set_title('Dati normalizzati (Min-Max)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Gestione dei dati duplicati\n",
    "\n",
    "Identifichiamo e gestiamo eventuali dati duplicati nel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificare se ci sono duplicati nel dataset\n",
    "duplicates = df_no_outliers.duplicated()\n",
    "print(f\"Numero di righe duplicate: {duplicates.sum()}\")\n",
    "\n",
    "# Se ci sono duplicati, visualizzarli\n",
    "if duplicates.sum() > 0:\n",
    "    df_no_outliers[df_no_outliers.duplicated(keep=False)].sort_values(by=df_no_outliers.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificare duplicati parziali (basati su sottoinsiemi di colonne)\n",
    "# Ad esempio, cerchiamo passeggeri con stesso nome, età e classe\n",
    "subset_cols = ['Name', 'Age', 'Pclass', 'Sex']\n",
    "partial_duplicates = df_no_outliers.duplicated(subset=subset_cols, keep=False)\n",
    "print(f\"Numero di righe con duplicati parziali: {partial_duplicates.sum()}\")\n",
    "\n",
    "# Visualizzare i duplicati parziali\n",
    "if partial_duplicates.sum() > 0:\n",
    "    df_no_outliers[partial_duplicates].sort_values(by=subset_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Rimuovere i duplicati (se necessario)\n",
    "df_no_duplicates = df_no_outliers.drop_duplicates()\n",
    "print(f\"Dimensioni del dataset originale: {df_no_outliers.shape}\")\n",
    "print(f\"Dimensioni del dataset senza duplicati: {df_no_duplicates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Feature Engineering\n",
    "\n",
    "In questa seconda parte, ci concentreremo su:\n",
    "- Trasformazione delle variabili\n",
    "- Creazione di nuove feature\n",
    "- Selezione delle feature\n",
    "- Riduzione della dimensionalità\n",
    "- Encoding di variabili categoriche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Trasformazione delle variabili\n",
    "\n",
    "Applichiamo diverse trasformazioni alle variabili esistenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Creare una copia del dataframe pulito\n",
    "df_fe = df_no_duplicates.copy()\n",
    "\n",
    "# Trasformazioni matematiche\n",
    "# 1. Trasformazione logaritmica (già applicata a Fare)\n",
    "# 2. Trasformazione radice quadrata\n",
    "df_fe['Fare_sqrt'] = np.sqrt(df_fe['Fare'])\n",
    "\n",
    "# Visualizzare l'effetto delle trasformazioni\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.histplot(df_fe['Fare'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribuzione originale di Fare')\n",
    "axes[0].set_xlabel('Fare')\n",
    "\n",
    "sns.histplot(df_fe['Fare_log'], kde=True, ax=axes[1])\n",
    "axes[1].set_title('Trasformazione logaritmica')\n",
    "axes[1].set_xlabel('Fare_log')\n",
    "\n",
    "sns.histplot(df_fe['Fare_sqrt'], kde=True, ax=axes[2])\n",
    "axes[2].set_title('Trasformazione radice quadrata')\n",
    "axes[2].set_xlabel('Fare_sqrt')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creazione di nuove feature\n",
    "\n",
    "Creiamo nuove feature basate su quelle esistenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Feature basate sul dominio\n",
    "# Dimensione della famiglia (numero di fratelli/sorelle/coniugi + genitori/figli + se stessi)\n",
    "df_fe['FamilySize'] = df_fe['SibSp'] + df_fe['Parch'] + 1\n",
    "\n",
    "# 2. Feature binaria: viaggia da solo?\n",
    "df_fe['IsAlone'] = (df_fe['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# 3. Estrarre il titolo dal nome\n",
    "df_fe['Title'] = df_fe['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "# Visualizzare la distribuzione dei titoli\n",
    "print(df_fe['Title'].value_counts())\n",
    "\n",
    "# 4. Raggruppare i titoli meno comuni\n",
    "title_mapping = {\n",
    "    'Mr': 'Mr',\n",
    "    'Miss': 'Miss',\n",
    "    'Mrs': 'Mrs',\n",
    "    'Master': 'Master',\n",
    "    'Dr': 'Rare',\n",
    "    'Rev': 'Rare',\n",
    "    'Col': 'Rare',\n",
    "    'Major': 'Rare',\n",
    "    'Mlle': 'Miss',\n",
    "    'Countess': 'Rare',\n",
    "    'Ms': 'Miss',\n",
    "    'Lady': 'Rare',\n",
    "    'Jonkheer': 'Rare',\n",
    "    'Don': 'Rare',\n",
    "    'Dona': 'Rare',\n",
    "    'Mme': 'Mrs',\n",
    "    'Capt': 'Rare',\n",
    "    'Sir': 'Rare'\n",
    "}\n",
    "df_fe['Title'] = df_fe['Title'].map(title_mapping)\n",
    "\n",
    "# 5. Tariffa per persona\n",
    "df_fe['FarePerPerson'] = df_fe['Fare'] / df_fe['FamilySize']\n",
    "\n",
    "# Visualizzare le nuove feature\n",
    "df_fe[['PassengerId', 'Name', 'Title', 'SibSp', 'Parch', 'FamilySize', 'IsAlone', 'Fare', 'FarePerPerson']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analizzare la relazione tra le nuove feature e la sopravvivenza\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Sopravvivenza per dimensione della famiglia\n",
    "sns.countplot(x='FamilySize', hue='Survived', data=df_fe, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Sopravvivenza per dimensione della famiglia')\n",
    "axes[0, 0].set_xlabel('Dimensione della famiglia')\n",
    "axes[0, 0].set_ylabel('Conteggio')\n",
    "axes[0, 0].legend(title='Sopravvissuto', labels=['No', 'Sì'])\n",
    "\n",
    "# Sopravvivenza per viaggiatori solitari vs. in famiglia\n",
    "sns.countplot(x='IsAlone', hue='Survived', data=df_fe, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Sopravvivenza per viaggiatori solitari vs. in famiglia')\n",
    "axes[0, 1].set_xlabel('Viaggia da solo (1 = Sì, 0 = No)')\n",
    "axes[0, 1].set_ylabel('Conteggio')\n",
    "axes[0, 1].legend(title='Sopravvissuto', labels=['No', 'Sì'])\n",
    "\n",
    "# Sopravvivenza per titolo\n",
    "sns.countplot(x='Title', hue='Survived', data=df_fe, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Sopravvivenza per titolo')\n",
    "axes[1, 0].set_xlabel('Titolo')\n",
    "axes[1, 0].set_ylabel('Conteggio')\n",
    "axes[1, 0].legend(title='Sopravvissuto', labels=['No', 'Sì'])\n",
    "\n",
    "# Distribuzione della tariffa per persona in base alla sopravvivenza\n",
    "sns.boxplot(x='Survived', y='FarePerPerson', data=df_fe, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Tariffa per persona in base alla sopravvivenza')\n",
    "axes[1, 1].set_xlabel('Sopravvissuto (1 = Sì, 0 = No)')\n",
    "axes[1, 1].set_ylabel('Tariffa per persona')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Encoding di variabili categoriche\n",
    "\n",
    "Applichiamo diverse tecniche di encoding alle variabili categoriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identificare le variabili categoriche\n",
    "categorical_features = ['Sex', 'Embarked', 'Title']\n",
    "\n",
    "# 1. Label Encoding\n",
    "df_encoded = df_fe.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[f'{feature}_label'] = le.fit_transform(df_encoded[feature])\n",
    "    label_encoders[feature] = le\n",
    "    \n",
    "    # Mostrare la mappatura\n",
    "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"Label Encoding per {feature}:\")\n",
    "    print(mapping)\n",
    "    print()\n",
    "\n",
    "# Visualizzare il risultato\n",
    "df_encoded[[*categorical_features, *[f'{f}_label' for f in categorical_features]]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding\n",
    "df_onehot = df_fe.copy()\n",
    "\n",
    "# Applicare One-Hot Encoding\n",
    "df_onehot = pd.get_dummies(df_onehot, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Visualizzare il risultato\n",
    "print(f\"Dimensioni originali: {df_fe.shape}\")\n",
    "print(f\"Dimensioni dopo One-Hot Encoding: {df_onehot.shape}\")\n",
    "df_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Target Encoding\n",
    "df_target_encoded = df_fe.copy()\n",
    "\n",
    "for feature in categorical_features:\n",
    "    # Calcolare la media della variabile target per ogni categoria\n",
    "    target_means = df_target_encoded.groupby(feature)['Survived'].mean()\n",
    "    \n",
    "    # Applicare l'encoding\n",
    "    df_target_encoded[f'{feature}_target'] = df_target_encoded[feature].map(target_means)\n",
    "    \n",
    "    # Mostrare la mappatura\n",
    "    print(f\"Target Encoding per {feature}:\")\n",
    "    print(target_means)\n",
    "    print()\n",
    "\n",
    "# Visualizzare il risultato\n",
    "df_target_encoded[[*categorical_features, *[f'{f}_target' for f in categorical_features]]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Selezione delle feature\n",
    "\n",
    "Applichiamo diverse tecniche per selezionare le feature più rilevanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preparare i dati per la selezione delle feature\n",
    "# Utilizziamo il dataframe con one-hot encoding\n",
    "X = df_onehot.drop(['Survived', 'Name', 'Ticket', 'PassengerId'], axis=1)\n",
    "y = df_onehot['Survived']\n",
    "\n",
    "print(f\"Feature disponibili: {X.shape[1]}\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Filter Method: Correlazione con la variabile target\n",
    "correlation = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation.plot(kind='bar')\n",
    "plt.title('Correlazione assoluta con la sopravvivenza')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Correlazione assoluta')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selezionare le top 10 feature\n",
    "top_corr_features = correlation.nlargest(10).index.tolist()\n",
    "print(\"Top 10 feature per correlazione:\")\n",
    "print(top_corr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Filter Method: ANOVA F-value\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Ottenere i punteggi e i p-value\n",
    "scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Score': selector.scores_,\n",
    "    'P-value': selector.pvalues_\n",
    "})\n",
    "\n",
    "# Ordinare per punteggio\n",
    "scores = scores.sort_values('Score', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Score', y='Feature', data=scores.head(10))\n",
    "plt.title('Top 10 feature per F-value (ANOVA)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selezionare le top 10 feature\n",
    "top_anova_features = scores.head(10)['Feature'].tolist()\n",
    "print(\"Top 10 feature per F-value:\")\n",
    "print(top_anova_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Embedded Method: Random Forest Feature Importance\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Ottenere l'importanza delle feature\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Ordinare per importanza\n",
    "importances = importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances.head(10))\n",
    "plt.title('Top 10 feature per importanza (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Selezionare le top 10 feature\n",
    "top_rf_features = importances.head(10)['Feature'].tolist()\n",
    "print(\"Top 10 feature per importanza (Random Forest):\")\n",
    "print(top_rf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4. Wrapper Method: Recursive Feature Elimination (RFE)\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(estimator=model, n_features_to_select=10, step=1)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Ottenere il ranking delle feature\n",
    "rfe_ranking = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Ranking': rfe.ranking_,\n",
    "    'Selected': rfe.support_\n",
    "})\n",
    "\n",
    "# Ordinare per ranking\n",
    "rfe_ranking = rfe_ranking.sort_values('Ranking')\n",
    "\n",
    "# Visualizzare le feature selezionate\n",
    "selected_features = rfe_ranking[rfe_ranking['Selected']]\n",
    "print(\"Feature selezionate da RFE:\")\n",
    "print(selected_features['Feature'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Riduzione della dimensionalità\n",
    "\n",
    "Applichiamo tecniche di riduzione della dimensionalità per visualizzare e comprimere i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standardizzare i dati prima della PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Applicare PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualizzare la varianza spiegata\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Varianza individuale')\n",
    "plt.step(range(1, len(cumulative_variance) + 1), cumulative_variance, where='mid', label='Varianza cumulativa')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% varianza spiegata')\n",
    "plt.xlabel('Numero di componenti')\n",
    "plt.ylabel('Proporzione di varianza spiegata')\n",
    "plt.title('Varianza spiegata dalle componenti principali')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinare il numero di componenti per spiegare il 95% della varianza\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Numero di componenti necessarie per spiegare il 95% della varianza: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare PCA con 2 componenti per la visualizzazione\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# Creare un dataframe con le componenti principali\n",
    "pca_df = pd.DataFrame({\n",
    "    'PC1': X_pca_2d[:, 0],\n",
    "    'PC2': X_pca_2d[:, 1],\n",
    "    'Survived': y\n",
    "})\n",
    "\n",
    "# Visualizzare i dati nel nuovo spazio bidimensionale\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Survived', data=pca_df, palette='viridis', alpha=0.8)\n",
    "plt.title('PCA: Visualizzazione dei dati in 2D')\n",
    "plt.xlabel('Prima componente principale')\n",
    "plt.ylabel('Seconda componente principale')\n",
    "plt.legend(title='Sopravvissuto', labels=['No', 'Sì'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analizzare il contributo delle feature alle componenti principali\n",
    "components = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=X.columns\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(components.iloc[:5], annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Contributo delle feature alle prime 5 componenti principali')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Componente principale')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Tecniche di Suddivisione e Validazione dei Dati\n",
    "\n",
    "In questa terza parte, ci concentreremo su:\n",
    "- Train-test split\n",
    "- Validazione incrociata (cross-validation)\n",
    "- Stratificazione\n",
    "- Metriche di valutazione\n",
    "- Gestione dello sbilanciamento delle classi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train-Test Split\n",
    "\n",
    "Applichiamo la suddivisione base in training e test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preparare i dati\n",
    "# Utilizziamo le feature selezionate dal Random Forest\n",
    "X_selected = X[top_rf_features]\n",
    "y = df_onehot['Survived']\n",
    "\n",
    "# Suddivisione base (70% training, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Dimensioni del training set: {X_train.shape}\")\n",
    "print(f\"Dimensioni del test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificare la distribuzione della variabile target nei subset\n",
    "print(\"Distribuzione della variabile target nel dataset completo:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "print(\"\\nDistribuzione della variabile target nel training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nDistribuzione della variabile target nel test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Addestrare un modello sul training set e valutarlo sul test set\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fare predizioni sul test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Valutare le performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Matrice di confusione')\n",
    "plt.xlabel('Predetto')\n",
    "plt.ylabel('Reale')\n",
    "plt.xticks([0.5, 1.5], ['Non sopravvissuto (0)', 'Sopravvissuto (1)'])\n",
    "plt.yticks([0.5, 1.5], ['Non sopravvissuto (0)', 'Sopravvissuto (1)'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Validazione Incrociata (Cross-Validation)\n",
    "\n",
    "Applichiamo la validazione incrociata per ottenere una stima più robusta delle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calcolare i punteggi\n",
    "cv_scores = cross_val_score(model, X_selected, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(f\"Punteggi per fold: {cv_scores}\")\n",
    "print(f\"Punteggio medio: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcolare diverse metriche con cross-validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results = cross_validate(model, X_selected, y, cv=kf, scoring=scoring)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "for metric in scoring:\n",
    "    scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric.capitalize()}: {scores.mean():.4f} (±{scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare i risultati della cross-validation\n",
    "cv_results_df = pd.DataFrame({\n",
    "    'Fold': range(1, kf.n_splits + 1),\n",
    "    'Accuracy': cv_results['test_accuracy'],\n",
    "    'Precision': cv_results['test_precision'],\n",
    "    'Recall': cv_results['test_recall'],\n",
    "    'F1': cv_results['test_f1']\n",
    "})\n",
    "\n",
    "cv_results_melted = pd.melt(cv_results_df, id_vars=['Fold'], var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Fold', y='Score', hue='Metric', data=cv_results_melted)\n",
    "plt.title('Risultati della Cross-Validation per fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Punteggio')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Metrica')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Stratificazione\n",
    "\n",
    "Applichiamo la stratificazione per mantenere la stessa distribuzione delle classi in tutti i subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stratified K-Fold Cross-Validation\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calcolare i punteggi\n",
    "stratified_cv_scores = cross_val_score(model, X_selected, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(f\"Punteggi per fold (stratificati): {stratified_cv_scores}\")\n",
    "print(f\"Punteggio medio (stratificato): {stratified_cv_scores.mean():.4f} (±{stratified_cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare K-Fold standard con Stratified K-Fold\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Fold': range(1, kf.n_splits + 1),\n",
    "    'K-Fold standard': cv_scores,\n",
    "    'Stratified K-Fold': stratified_cv_scores\n",
    "})\n",
    "\n",
    "comparison_melted = pd.melt(comparison_df, id_vars=['Fold'], var_name='Metodo', value_name='Accuracy')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Fold', y='Accuracy', hue='Metodo', data=comparison_melted)\n",
    "plt.title('Confronto tra K-Fold standard e Stratified K-Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.7, 0.9)  # Adattare in base ai risultati\n",
    "plt.legend(title='Metodo')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stratified Train-Test Split\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X_selected, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Distribuzione della variabile target nel dataset completo:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "print(\"\\nDistribuzione della variabile target nel training set stratificato:\")\n",
    "print(y_train_strat.value_counts(normalize=True))\n",
    "print(\"\\nDistribuzione della variabile target nel test set stratificato:\")\n",
    "print(y_test_strat.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Metriche di Valutazione\n",
    "\n",
    "Approfondiamo le diverse metriche di valutazione per problemi di classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Addestrare un modello sul training set stratificato\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_strat, y_train_strat)\n",
    "\n",
    "# Fare predizioni sul test set\n",
    "y_pred = model.predict(X_test_strat)\n",
    "y_pred_proba = model.predict_proba(X_test_strat)[:, 1]  # Probabilità della classe positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcolare diverse metriche\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                           roc_auc_score, log_loss, confusion_matrix, classification_report)\n",
    "\n",
    "accuracy = accuracy_score(y_test_strat, y_pred)\n",
    "precision = precision_score(y_test_strat, y_pred)\n",
    "recall = recall_score(y_test_strat, y_pred)\n",
    "f1 = f1_score(y_test_strat, y_pred)\n",
    "auc = roc_auc_score(y_test_strat, y_pred_proba)\n",
    "loss = log_loss(y_test_strat, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"Log Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_strat, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la curva ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_strat, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la curva Precision-Recall\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test_strat, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test_strat, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, \n",
    "         label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Gestione dello Sbilanciamento delle Classi\n",
    "\n",
    "Applichiamo tecniche per gestire lo sbilanciamento delle classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificare lo sbilanciamento delle classi\n",
    "class_counts = y.value_counts()\n",
    "print(\"Distribuzione delle classi:\")\n",
    "print(class_counts)\n",
    "print(f\"Rapporto di sbilanciamento: {class_counts[0] / class_counts[1]:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Distribuzione delle classi')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.xticks([0, 1], ['Non sopravvissuto (0)', 'Sopravvissuto (1)'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Creare un dataset artificialmente sbilanciato per dimostrare le tecniche\n",
    "# (Il dataset Titanic è già leggermente sbilanciato, ma creiamo uno sbilanciamento più marcato)\n",
    "\n",
    "# Selezionare casualmente un sottoinsieme della classe minoritaria (sopravvissuti)\n",
    "minority_indices = y[y == 1].index\n",
    "np.random.seed(42)\n",
    "remove_indices = np.random.choice(minority_indices, size=int(len(minority_indices) * 0.7), replace=False)\n",
    "\n",
    "# Creare il dataset sbilanciato\n",
    "X_imbalanced = X_selected.drop(remove_indices)\n",
    "y_imbalanced = y.drop(remove_indices)\n",
    "\n",
    "# Verificare lo sbilanciamento\n",
    "imbalanced_counts = y_imbalanced.value_counts()\n",
    "print(\"Distribuzione delle classi nel dataset sbilanciato:\")\n",
    "print(imbalanced_counts)\n",
    "print(f\"Rapporto di sbilanciamento: {imbalanced_counts[0] / imbalanced_counts[1]:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y_imbalanced)\n",
    "plt.title('Distribuzione delle classi nel dataset sbilanciato')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Conteggio')\n",
    "plt.xticks([0, 1], ['Non sopravvissuto (0)', 'Sopravvissuto (1)'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Suddividere il dataset sbilanciato\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imbalanced, y_imbalanced, test_size=0.3, stratify=y_imbalanced, random_state=42)\n",
    "\n",
    "# Addestrare un modello sul dataset sbilanciato\n",
    "model_imb = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_imb.fit(X_train_imb, y_train_imb)\n",
    "\n",
    "# Fare predizioni\n",
    "y_pred_imb = model_imb.predict(X_test_imb)\n",
    "\n",
    "# Valutare le performance\n",
    "print(\"Performance sul dataset sbilanciato:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_imb):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_imb, y_pred_imb):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_imb, y_pred_imb):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_imb, y_pred_imb):.4f}\")\n",
    "\n",
    "# Visualizzare la matrice di confusione\n",
    "cm_imb = confusion_matrix(y_test_imb, y_pred_imb)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_imb, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Matrice di confusione (dataset sbilanciato)')\n",
    "plt.xlabel('Predetto')\n",
    "plt.ylabel('Reale')\n",
    "plt.xticks([0.5, 1.5], ['Non sopravvissuto (0)', 'Sopravvissuto (1)'])\n",
    "plt.yticks([0.5, 1.5], ['Non sopravvissuto (0)', 'Sopravvissuto (1)'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train_imb, y_train_imb)\n",
    "\n",
    "print(\"Distribuzione delle classi dopo Random Undersampling:\")\n",
    "print(pd.Series(y_train_rus).value_counts())\n",
    "\n",
    "# Addestrare un modello\n",
    "model_rus = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_rus.fit(X_train_rus, y_train_rus)\n",
    "\n",
    "# Fare predizioni\n",
    "y_pred_rus = model_rus.predict(X_test_imb)\n",
    "\n",
    "# Valutare le performance\n",
    "print(\"\\nPerformance dopo Random Undersampling:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_rus):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_imb, y_pred_rus):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_imb, y_pred_rus):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_imb, y_pred_rus):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_imb, y_train_imb)\n",
    "\n",
    "print(\"Distribuzione delle classi dopo SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Addestrare un modello\n",
    "model_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Fare predizioni\n",
    "y_pred_smote = model_smote.predict(X_test_imb)\n",
    "\n",
    "# Valutare le performance\n",
    "print(\"\\nPerformance dopo SMOTE:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_smote):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_imb, y_pred_smote):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_imb, y_pred_smote):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_imb, y_pred_smote):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Pesi delle classi\n",
    "# Calcolare i pesi inversamente proporzionali alla frequenza delle classi\n",
    "class_weights = {0: 1, 1: imbalanced_counts[0] / imbalanced_counts[1]}\n",
    "print(f\"Pesi delle classi: {class_weights}\")\n",
    "\n",
    "# Addestrare un modello con pesi delle classi\n",
    "model_weighted = RandomForestClassifier(n_estimators=100, class_weight=class_weights, random_state=42)\n",
    "model_weighted.fit(X_train_imb, y_train_imb)\n",
    "\n",
    "# Fare predizioni\n",
    "y_pred_weighted = model_weighted.predict(X_test_imb)\n",
    "\n",
    "# Valutare le performance\n",
    "print(\"\\nPerformance con pesi delle classi:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_weighted):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test_imb, y_pred_weighted):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test_imb, y_pred_weighted):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test_imb, y_pred_weighted):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare le diverse tecniche\n",
    "techniques = ['Senza trattamento', 'Random Undersampling', 'SMOTE', 'Pesi delle classi']\n",
    "accuracies = [\n",
    "    accuracy_score(y_test_imb, y_pred_imb),\n",
    "    accuracy_score(y_test_imb, y_pred_rus),\n",
    "    accuracy_score(y_test_imb, y_pred_smote),\n",
    "    accuracy_score(y_test_imb, y_pred_weighted)\n",
    "]\n",
    "precisions = [\n",
    "    precision_score(y_test_imb, y_pred_imb),\n",
    "    precision_score(y_test_imb, y_pred_rus),\n",
    "    precision_score(y_test_imb, y_pred_smote),\n",
    "    precision_score(y_test_imb, y_pred_weighted)\n",
    "]\n",
    "recalls = [\n",
    "    recall_score(y_test_imb, y_pred_imb),\n",
    "    recall_score(y_test_imb, y_pred_rus),\n",
    "    recall_score(y_test_imb, y_pred_smote),\n",
    "    recall_score(y_test_imb, y_pred_weighted)\n",
    "]\n",
    "f1_scores = [\n",
    "    f1_score(y_test_imb, y_pred_imb),\n",
    "    f1_score(y_test_imb, y_pred_rus),\n",
    "    f1_score(y_test_imb, y_pred_smote),\n",
    "    f1_score(y_test_imb, y_pred_weighted)\n",
    "]\n",
    "\n",
    "# Creare un dataframe con i risultati\n",
    "results_df = pd.DataFrame({\n",
    "    'Tecnica': techniques,\n",
    "    'Accuracy': accuracies,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls,\n",
    "    'F1-score': f1_scores\n",
    "})\n",
    "\n",
    "# Visualizzare i risultati\n",
    "results_melted = pd.melt(results_df, id_vars=['Tecnica'], var_name='Metrica', value_name='Valore')\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Tecnica', y='Valore', hue='Metrica', data=results_melted)\n",
    "plt.title('Confronto delle tecniche per gestire lo sbilanciamento delle classi')\n",
    "plt.xlabel('Tecnica')\n",
    "plt.ylabel('Valore')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Metrica')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusioni\n",
    "\n",
    "In questo notebook abbiamo esplorato le principali tecniche di preparazione e pre-elaborazione dei dati, coprendo i tre temi principali del corso:\n",
    "\n",
    "1. **Raccolta e pulizia dei dati**:\n",
    "   - Caricamento e esplorazione dei dati\n",
    "   - Identificazione e gestione dei valori mancanti\n",
    "   - Rilevamento e trattamento degli outlier\n",
    "   - Normalizzazione e standardizzazione\n",
    "   - Gestione dei dati duplicati\n",
    "\n",
    "2. **Feature engineering**:\n",
    "   - Trasformazione delle variabili\n",
    "   - Creazione di nuove feature\n",
    "   - Encoding di variabili categoriche\n",
    "   - Selezione delle feature\n",
    "   - Riduzione della dimensionalità\n",
    "\n",
    "3. **Tecniche di suddivisione e validazione dei dati**:\n",
    "   - Train-test split\n",
    "   - Validazione incrociata (cross-validation)\n",
    "   - Stratificazione\n",
    "   - Metriche di valutazione\n",
    "   - Gestione dello sbilanciamento delle classi\n",
    "\n",
    "Queste tecniche sono fondamentali per qualsiasi progetto di machine learning e data science, e spesso hanno un impatto maggiore sulle performance finali rispetto alla scelta dell'algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizi Proposti\n",
    "\n",
    "1. Carica un altro dataset a tua scelta (ad esempio da Kaggle o UCI Machine Learning Repository) e applica le tecniche di pulizia dei dati viste in questo notebook.\n",
    "\n",
    "2. Crea nuove feature basate sul dominio specifico del dataset scelto e valuta il loro impatto sulle performance di un modello.\n",
    "\n",
    "3. Confronta diverse tecniche di encoding per le variabili categoriche e identifica quale funziona meglio per il tuo dataset.\n",
    "\n",
    "4. Implementa una pipeline completa di pre-elaborazione dei dati utilizzando `sklearn.pipeline.Pipeline` e `sklearn.compose.ColumnTransformer`.\n",
    "\n",
    "5. Sperimenta con diverse tecniche di gestione dello sbilanciamento delle classi e analizza il loro impatto sulle diverse metriche di valutazione."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
