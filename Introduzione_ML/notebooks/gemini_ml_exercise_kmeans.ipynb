{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione: Utilizzo di Gemini in Google Colab per il Machine Learning con K-means\n",
    "\n",
    "Questo notebook guida gli studenti nell'utilizzo di Gemini, il modello di AI di Google, all'interno di Google Colab per sviluppare un progetto di Machine Learning con focus sulla clusterizzazione K-means. Seguiremo le principali fasi di un progetto ML:\n",
    "\n",
    "1. Caricamento e analisi del dataset\n",
    "2. Pulizia dei dati\n",
    "3. Feature engineering semplice\n",
    "4. Realizzazione di un modello di classificazione\n",
    "5. Implementazione della clusterizzazione K-means\n",
    "\n",
    "## Come utilizzare questo notebook\n",
    "\n",
    "- Le caselle di testo contengono i prompt da dare a Gemini\n",
    "- Le caselle di codice sono dove incollare il codice generato da Gemini\n",
    "- Segui le istruzioni passo dopo passo per completare l'esercitazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione iniziale\n",
    "\n",
    "Prima di iniziare, dobbiamo attivare Gemini in Google Colab. Nella barra laterale di Colab, cerca l'icona di Gemini (un rombo colorato) e assicurati che sia attivato.\n",
    "\n",
    "Installiamo anche le librerie necessarie per il nostro progetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa le librerie necessarie\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: Caricamento e analisi del dataset\n",
    "\n",
    "### Prompt per Gemini:\n",
    "\n",
    "```\n",
    "Voglio caricare e analizzare il dataset Wine di scikit-learn. Puoi fornirmi il codice per:\n",
    "1. Caricare il dataset\n",
    "2. Visualizzare le prime righe\n",
    "3. Ottenere statistiche descrittive\n",
    "4. Creare alcuni grafici per esplorare i dati (istogrammi, box plot e matrice di correlazione)\n",
    "```\n",
    "\n",
    "Copia il prompt sopra e incollalo nella chat di Gemini. Poi copia il codice generato nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incolla qui il codice generato da Gemini per caricare e analizzare il dataset Wine\n",
    "\n",
    "# Esempio di codice che potresti ottenere:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# 1. Caricare il dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "feature_names = wine.feature_names\n",
    "target_names = wine.target_names\n",
    "\n",
    "# Creiamo un DataFrame per una migliore manipolazione\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['wine_type'] = [target_names[i] for i in y]\n",
    "\n",
    "# 2. Visualizzare le prime righe\n",
    "print(\"Informazioni sul dataset Wine:\")\n",
    "print(f\"Numero di campioni: {X.shape[0]}\")\n",
    "print(f\"Numero di feature: {X.shape[1]}\")\n",
    "print(f\"Classi: {target_names}\")\n",
    "print(\"\\nPrime 5 righe del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Ottenere statistiche descrittive\n",
    "print(\"\\nStatistiche descrittive:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Distribuzione delle classi\n",
    "print(\"\\nDistribuzione delle classi:\")\n",
    "print(df['wine_type'].value_counts())\n",
    "\n",
    "# 4. Creare alcuni grafici per esplorare i dati\n",
    "# Configurazione del plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Istogrammi per alcune feature\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(data=df, x='alcohol', hue='wine_type', kde=True)\n",
    "plt.title('Distribuzione di alcohol per tipo di vino')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(data=df, x='malic_acid', hue='wine_type', kde=True)\n",
    "plt.title('Distribuzione di malic_acid per tipo di vino')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(data=df, x='color_intensity', hue='wine_type', kde=True)\n",
    "plt.title('Distribuzione di color_intensity per tipo di vino')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(data=df, x='flavanoids', hue='wine_type', kde=True)\n",
    "plt.title('Distribuzione di flavanoids per tipo di vino')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot per alcune feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='wine_type', y='alcohol', data=df)\n",
    "plt.title('Box plot di alcohol per tipo di vino')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(x='wine_type', y='malic_acid', data=df)\n",
    "plt.title('Box plot di malic_acid per tipo di vino')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='wine_type', y='color_intensity', data=df)\n",
    "plt.title('Box plot di color_intensity per tipo di vino')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(x='wine_type', y='flavanoids', data=df)\n",
    "plt.title('Box plot di flavanoids per tipo di vino')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Matrice di correlazione\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.drop('wine_type', axis=1).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title('Matrice di correlazione delle feature')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi dei risultati\n",
    "\n",
    "Osserva i risultati ottenuti. Quali sono le caratteristiche principali del dataset Wine? Quali feature sembrano più utili per distinguere i diversi tipi di vino?\n",
    "\n",
    "Puoi chiedere a Gemini di aiutarti a interpretare i risultati con il seguente prompt:\n",
    "\n",
    "```\n",
    "Puoi aiutarmi a interpretare i risultati dell'analisi del dataset Wine? Quali sono le feature più importanti per distinguere i diversi tipi di vino in base ai grafici e alla matrice di correlazione?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2: Pulizia dei dati\n",
    "\n",
    "### Prompt per Gemini:\n",
    "\n",
    "```\n",
    "Voglio pulire il dataset Wine. Puoi fornirmi il codice per:\n",
    "1. Verificare se ci sono valori mancanti\n",
    "2. Identificare e gestire eventuali outlier\n",
    "3. Normalizzare le feature (utilizzando StandardScaler)\n",
    "4. Visualizzare la distribuzione dei dati prima e dopo la normalizzazione\n",
    "```\n",
    "\n",
    "Copia il prompt sopra e incollalo nella chat di Gemini. Poi copia il codice generato nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incolla qui il codice generato da Gemini per la pulizia dei dati\n",
    "\n",
    "# Esempio di codice che potresti ottenere:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Verificare se ci sono valori mancanti\n",
    "print(\"Valori mancanti nel dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Identificare e gestire eventuali outlier\n",
    "# Funzione per identificare gli outlier con il metodo IQR\n",
    "def identify_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Verifichiamo gli outlier per ogni feature numerica\n",
    "numeric_features = df.drop('wine_type', axis=1).columns\n",
    "outliers_summary = {}\n",
    "\n",
    "for feature in numeric_features:\n",
    "    outliers, lower, upper = identify_outliers(df, feature)\n",
    "    if len(outliers) > 0:\n",
    "        outliers_summary[feature] = {\n",
    "            'count': len(outliers),\n",
    "            'lower_bound': lower,\n",
    "            'upper_bound': upper\n",
    "        }\n",
    "        print(f\"Outlier trovati in {feature}: {len(outliers)}\")\n",
    "        print(f\"  Limite inferiore: {lower:.2f}, Limite superiore: {upper:.2f}\")\n",
    "    else:\n",
    "        print(f\"Nessun outlier trovato in {feature}\")\n",
    "\n",
    "# Visualizziamo la distribuzione di alcune feature con outlier\n",
    "features_with_outliers = [feature for feature in outliers_summary.keys() if outliers_summary[feature]['count'] > 0]\n",
    "if features_with_outliers:\n",
    "    plt.figure(figsize=(15, 5 * (len(features_with_outliers) + 1) // 2))\n",
    "    for i, feature in enumerate(features_with_outliers[:4]):  # Limitiamo a 4 feature per brevità\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        sns.boxplot(x='wine_type', y=feature, data=df)\n",
    "        plt.title(f'Box plot di {feature} con outlier')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Gestione degli outlier: in questo caso, scegliamo di mantenerli poiché il dataset è piccolo\n",
    "# e gli outlier potrebbero contenere informazioni importanti\n",
    "print(\"\\nDecisione: Mantenere gli outlier per questo dataset\")\n",
    "\n",
    "# 3. Normalizzare le feature\n",
    "# Separiamo feature e target\n",
    "X = df.drop('wine_type', axis=1)\n",
    "y = df['wine_type']\n",
    "\n",
    "# Visualizziamo la distribuzione di alcune feature prima della normalizzazione\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribuzione delle feature prima della normalizzazione', fontsize=16)\n",
    "\n",
    "for i, feature in enumerate(X.columns[:4]):  # Visualizziamo solo le prime 4 feature\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(data=X, x=feature, kde=True)\n",
    "    plt.title(f'Distribuzione di {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Inizializziamo lo scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Applichiamo la normalizzazione\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convertiamo in DataFrame per una migliore visualizzazione\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "# 4. Visualizzare la distribuzione dei dati dopo la normalizzazione\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribuzione delle feature dopo la normalizzazione', fontsize=16)\n",
    "\n",
    "for i, feature in enumerate(X_scaled_df.columns[:4]):  # Visualizziamo solo le prime 4 feature\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.histplot(data=X_scaled_df, x=feature, kde=True)\n",
    "    plt.title(f'Distribuzione di {feature} (normalizzata)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Verifichiamo la media e la deviazione standard delle feature normalizzate\n",
    "print(\"\\nStatistiche delle feature normalizzate:\")\n",
    "print(X_scaled_df.describe().loc[['mean', 'std']])\n",
    "\n",
    "# Aggiungiamo la colonna target al DataFrame normalizzato per usi futuri\n",
    "X_scaled_df['wine_type'] = y.values\n",
    "print(\"\\nPrime 5 righe del dataset normalizzato con target:\")\n",
    "print(X_scaled_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riflessione sulla pulizia dei dati\n",
    "\n",
    "Rifletti sui risultati della pulizia dei dati:\n",
    "- Ci sono valori mancanti nel dataset?\n",
    "- Quali feature presentano outlier significativi?\n",
    "- Come sono cambiate le distribuzioni dopo la normalizzazione?\n",
    "- Perché la normalizzazione è importante per algoritmi come K-means?\n",
    "\n",
    "Puoi chiedere a Gemini di spiegarti l'importanza della normalizzazione dei dati per K-means con il seguente prompt:\n",
    "\n",
    "```\n",
    "Perché la normalizzazione dei dati è particolarmente importante per l'algoritmo K-means? Cosa potrebbe succedere se applicassimo K-means a dati non normalizzati?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3: Feature engineering semplice\n",
    "\n",
    "### Prompt per Gemini:\n",
    "\n",
    "```\n",
    "Voglio fare un feature engineering semplice sul dataset Wine. Puoi fornirmi il codice per:\n",
    "1. Creare feature polinomiali di grado 2 per alcune feature selezionate (ad esempio, alcohol e flavanoids)\n",
    "2. Creare feature di interazione tra coppie di feature importanti\n",
    "3. Selezionare le feature più importanti utilizzando la correlazione con la variabile target\n",
    "4. Visualizzare le nuove feature create\n",
    "```\n",
    "\n",
    "Copia il prompt sopra e incollalo nella chat di Gemini. Poi copia il codice generato nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incolla qui il codice generato da Gemini per il feature engineering\n",
    "\n",
    "# Esempio di codice che potresti ottenere:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Utilizziamo il dataset originale (non normalizzato) per il feature engineering\n",
    "X_original = df.drop('wine_type', axis=1)\n",
    "y_original = df['wine_type']\n",
    "\n",
    "# Convertiamo le etichette categoriche in numeriche per calcolare la correlazione\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_original)\n",
    "\n",
    "# 1. Creare feature polinomiali di grado 2 per alcune feature selezionate\n",
    "# Selezioniamo alcune feature per cui creare termini quadratici\n",
    "selected_features = ['alcohol', 'flavanoids', 'color_intensity', 'proline']\n",
    "X_poly = X_original.copy()\n",
    "\n",
    "for feature in selected_features:\n",
    "    X_poly[f\"{feature}_squared\"] = X_original[feature] ** 2\n",
    "\n",
    "print(\"Feature polinomiali create:\")\n",
    "for feature in selected_features:\n",
    "    print(f\"- {feature}_squared\")\n",
    "\n",
    "# 2. Creare feature di interazione tra coppie di feature importanti\n",
    "# Definiamo alcune coppie di feature per cui creare interazioni\n",
    "interaction_pairs = [\n",
    "    ('alcohol', 'malic_acid'),\n",
    "    ('flavanoids', 'color_intensity'),\n",
    "    ('alcohol', 'proline'),\n",
    "    ('od280/od315_of_diluted_wines', 'proline')\n",
    "]\n",
    "\n",
    "for feat1, feat2 in interaction_pairs:\n",
    "    X_poly[f\"{feat1}_{feat2}_interaction\"] = X_original[feat1] * X_original[feat2]\n",
    "\n",
    "print(\"\\nFeature di interazione create:\")\n",
    "for feat1, feat2 in interaction_pairs:\n",
    "    print(f\"- {feat1}_{feat2}_interaction\")\n",
    "\n",
    "# 3. Selezionare le feature più importanti utilizzando la correlazione con la variabile target\n",
    "# Aggiungiamo la variabile target al DataFrame per calcolare la correlazione\n",
    "X_poly_with_target = X_poly.copy()\n",
    "X_poly_with_target['target'] = y_encoded\n",
    "\n",
    "# Calcoliamo la correlazione di ogni feature con la variabile target\n",
    "correlations = X_poly_with_target.corr()['target'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nCorrelazione delle feature con la variabile target:\")\n",
    "print(correlations.drop('target'))\n",
    "\n",
    "# Selezioniamo le top 10 feature più correlate con il target\n",
    "top_features = correlations.drop('target').abs().sort_values(ascending=False).head(10).index.tolist()\n",
    "\n",
    "print(\"\\nTop 10 feature più correlate con il target:\")\n",
    "for i, feature in enumerate(top_features):\n",
    "    print(f\"{i+1}. {feature} (correlazione: {correlations[feature]:.4f})\")\n",
    "\n",
    "# Creiamo un dataset con solo le feature più importanti\n",
    "X_selected = X_poly[top_features]\n",
    "\n",
    "# 4. Visualizzare le nuove feature create\n",
    "# Visualizziamo la distribuzione delle nuove feature polinomiali\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribuzione delle feature polinomiali', fontsize=16)\n",
    "\n",
    "squared_features = [f for f in X_poly.columns if '_squared' in f]\n",
    "for i, feature in enumerate(squared_features):\n",
    "    if i < 4:  # Limitiamo a 4 grafici\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        sns.histplot(data=X_poly, x=feature, hue=y_original, kde=True, bins=15)\n",
    "        plt.title(f'Distribuzione di {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Visualizziamo la distribuzione delle nuove feature di interazione\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.suptitle('Distribuzione delle feature di interazione', fontsize=16)\n",
    "\n",
    "interaction_features = [f for f in X_poly.columns if '_interaction' in f]\n",
    "for i, feature in enumerate(interaction_features):\n",
    "    if i < 4:  # Limitiamo a 4 grafici\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        sns.histplot(data=X_poly, x=feature, hue=y_original, kde=True, bins=15)\n",
    "        plt.title(f'Distribuzione di {feature}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()\n",
    "\n",
    "# Visualizziamo la matrice di correlazione delle feature selezionate\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = X_selected.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title('Matrice di correlazione delle feature selezionate')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepariamo il dataset finale per i modelli\n",
    "# Normalizziamo le feature selezionate\n",
    "scaler = StandardScaler()\n",
    "X_selected_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "print(\"\\nDimensioni del dataset finale:\")\n",
    "print(f\"Numero di campioni: {X_selected_scaled.shape[0]}\")\n",
    "print(f\"Numero di feature: {X_selected_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riflessione sul feature engineering\n",
    "\n",
    "Rifletti sui risultati del feature engineering:\n",
    "- Quali nuove feature sembrano più utili per distinguere i tipi di vino?\n",
    "- Come sono cambiate le correlazioni dopo la creazione delle nuove feature?\n",
    "- Quali feature originali e derivate sono state selezionate come più importanti?\n",
    "\n",
    "Puoi chiedere a Gemini di spiegarti l'importanza del feature engineering nel machine learning con il seguente prompt:\n",
    "\n",
    "```\n",
    "Perché il feature engineering è importante nel machine learning? Quali sono i vantaggi di creare feature polinomiali e di interazione? In quali casi potrebbe essere controproducente?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4: Realizzazione di un modello di classificazione\n",
    "\n",
    "### Prompt per Gemini:\n",
    "\n",
    "```\n",
    "Voglio creare un modello di classificazione per il dataset Wine utilizzando le feature selezionate. Puoi fornirmi il codice per:\n",
    "1. Dividere il dataset in training e test set (80% training, 20% test)\n",
    "2. Addestrare un modello di Random Forest\n",
    "3. Valutare le performance del modello (accuracy, matrice di confusione, report di classificazione)\n",
    "4. Visualizzare l'importanza delle feature nel modello\n",
    "```\n",
    "\n",
    "Copia il prompt sopra e incollalo nella chat di Gemini. Poi copia il codice generato nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incolla qui il codice generato da Gemini per la creazione del modello\n",
    "\n",
    "# Esempio di codice che potresti ottenere:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilizziamo le feature selezionate e normalizzate\n",
    "X = X_selected_scaled\n",
    "y = y_original.values  # Utilizziamo le etichette originali\n",
    "\n",
    "# 1. Dividere il dataset in training e test set (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Verifichiamo le dimensioni dei set\n",
    "print(f\"Dimensioni X_train: {X_train.shape}\")\n",
    "print(f\"Dimensioni X_test: {X_test.shape}\")\n",
    "print(f\"Dimensioni y_train: {len(y_train)}\")\n",
    "print(f\"Dimensioni y_test: {len(y_test)}\")\n",
    "\n",
    "# Verifichiamo la distribuzione delle classi nei set di training e test\n",
    "print(\"\\nDistribuzione delle classi nel set di training:\")\n",
    "train_class_counts = pd.Series(y_train).value_counts()\n",
    "for class_name, count in zip(train_class_counts.index, train_class_counts.values):\n",
    "    print(f\"- {class_name}: {count}\")\n",
    "\n",
    "print(\"\\nDistribuzione delle classi nel set di test:\")\n",
    "test_class_counts = pd.Series(y_test).value_counts()\n",
    "for class_name, count in zip(test_class_counts.index, test_class_counts.values):\n",
    "    print(f\"- {class_name}: {count}\")\n",
    "\n",
    "# 2. Addestrare un modello di Random Forest\n",
    "# Inizializziamo il modello\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Addestriamo il modello\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Facciamo previsioni sul set di test\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# 3. Valutare le performance del modello\n",
    "# Calcoliamo l'accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy del modello: {accuracy:.4f}\")\n",
    "\n",
    "# Calcoliamo la matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualizziamo la matrice di confusione\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=wine.target_names, yticklabels=wine.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matrice di Confusione')\n",
    "plt.show()\n",
    "\n",
    "# Generiamo il report di classificazione\n",
    "report = classification_report(y_test, y_pred, target_names=wine.target_names)\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(report)\n",
    "\n",
    "# 4. Visualizzare l'importanza delle feature nel modello\n",
    "# Otteniamo l'importanza delle feature\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Creiamo un DataFrame per visualizzare meglio\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': top_features,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nImportanza delle feature nel modello Random Forest:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Visualizziamo l'importanza delle feature\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Importanza delle Feature nel Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riflessione sul modello\n",
    "\n",
    "Rifletti sui risultati del modello:\n",
    "- Qual è l'accuracy del modello? È un buon risultato?\n",
    "- Ci sono classi che il modello fatica a distinguere?\n",
    "- Quali sono le feature più importanti per la classificazione?\n",
    "- Come si confrontano le feature originali con quelle create durante il feature engineering?\n",
    "\n",
    "Puoi chiedere a Gemini di spiegarti come funziona l'algoritmo Random Forest e come interpretare i risultati con il seguente prompt:\n",
    "\n",
    "```\n",
    "Puoi spiegarmi come funziona l'algoritmo Random Forest? Come dovrei interpretare la matrice di confusione e l'importanza delle feature? Quali sono i vantaggi di Random Forest rispetto ad altri algoritmi di classificazione?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 5: Implementazione della clusterizzazione K-means\n",
    "\n",
    "### Prompt per Gemini:\n",
    "\n",
    "```\n",
    "Voglio implementare l'algoritmo K-means sul dataset Wine. Puoi fornirmi il codice per:\n",
    "1. Applicare l'algoritmo K-means con k=3 (poiché sappiamo che ci sono 3 tipi di vino)\n",
    "2. Visualizzare i cluster utilizzando PCA per ridurre a 2 dimensioni\n",
    "3. Confrontare i cluster ottenuti con le etichette reali\n",
    "4. Determinare il numero ottimale di cluster usando il metodo del gomito e il silhouette score\n",
    "5. Analizzare le caratteristiche di ciascun cluster\n",
    "```\n",
    "\n",
    "Copia il prompt sopra e incollalo nella chat di Gemini. Poi copia il codice generato nella cella seguente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incolla qui il codice generato da Gemini per la clusterizzazione K-means\n",
    "\n",
    "# Esempio di codice che potresti ottenere:\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Utilizziamo le feature selezionate e normalizzate\n",
    "X = X_selected_scaled\n",
    "y = y_original.values\n",
    "\n",
    "# Convertiamo le etichette categoriche in numeriche per il confronto\n",
    "le = LabelEncoder()\n",
    "y_numeric = le.fit_transform(y)\n",
    "\n",
    "# 1. Applicare l'algoritmo K-means con k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# 2. Visualizzare i cluster utilizzando PCA per ridurre a 2 dimensioni\n",
    "# Applichiamo PCA per ridurre a 2 dimensioni\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['Cluster'] = cluster_labels\n",
    "df_pca['True_Label'] = y\n",
    "\n",
    "# Visualizziamo i cluster\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot dei cluster predetti\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=df_pca, palette='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Aggiungiamo i centroidi\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "plt.title('Cluster K-means (k=3)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot delle etichette reali\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='True_Label', data=df_pca, palette='Set1', s=100, alpha=0.7)\n",
    "plt.title('Etichette reali')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Confrontare i cluster ottenuti con le etichette reali\n",
    "# Calcoliamo l'Adjusted Rand Index (ARI)\n",
    "ari = adjusted_rand_score(y_numeric, cluster_labels)\n",
    "print(f\"Adjusted Rand Index: {ari:.4f}\")\n",
    "# L'ARI varia da -1 a 1, dove 1 indica un clustering perfetto\n",
    "\n",
    "# Calcoliamo il Silhouette Score\n",
    "silhouette = silhouette_score(X, cluster_labels)\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
    "# Il Silhouette Score varia da -1 a 1, dove valori più alti indicano cluster meglio definiti\n",
    "\n",
    "# Creiamo una tabella di contingenza per vedere come i cluster si mappano alle etichette reali\n",
    "contingency_table = pd.crosstab(y, pd.Series(cluster_labels, name='Cluster'))\n",
    "print(\"\\nTabella di contingenza (etichette reali vs cluster):\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Visualizziamo la tabella di contingenza come heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Tabella di contingenza: Etichette reali vs Cluster')\n",
    "plt.show()\n",
    "\n",
    "# 4. Determinare il numero ottimale di cluster usando il metodo del gomito e il silhouette score\n",
    "# Calcoliamo l'inerzia (somma dei quadrati delle distanze) per diversi valori di k\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "ari_scores = []\n",
    "k_range = range(2, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, cluster_labels))\n",
    "    ari_scores.append(adjusted_rand_score(y_numeric, cluster_labels))\n",
    "\n",
    "# Visualizziamo il grafico dell'inerzia (metodo del gomito)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(k_range, inertia, 'o-', markersize=8)\n",
    "plt.xlabel('Numero di cluster (k)')\n",
    "plt.ylabel('Inerzia')\n",
    "plt.title('Metodo del gomito')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Visualizziamo il grafico del silhouette score\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(k_range, silhouette_scores, 'o-', markersize=8)\n",
    "plt.xlabel('Numero di cluster (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Visualizziamo il grafico dell'ARI\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(k_range, ari_scores, 'o-', markersize=8)\n",
    "plt.xlabel('Numero di cluster (k)')\n",
    "plt.ylabel('Adjusted Rand Index')\n",
    "plt.title('Adjusted Rand Index')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Troviamo il k ottimale in base al silhouette score\n",
    "optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Numero ottimale di cluster in base al silhouette score: {optimal_k_silhouette}\")\n",
    "\n",
    "# Troviamo il k ottimale in base all'ARI\n",
    "optimal_k_ari = k_range[np.argmax(ari_scores)]\n",
    "print(f\"Numero ottimale di cluster in base all'ARI: {optimal_k_ari}\")\n",
    "\n",
    "# 5. Analizzare le caratteristiche di ciascun cluster\n",
    "# Aggiungiamo le etichette dei cluster al dataset originale\n",
    "df_with_clusters = df.copy()\n",
    "df_with_clusters['cluster'] = cluster_labels\n",
    "\n",
    "# Calcoliamo le medie delle feature per ciascun cluster\n",
    "cluster_means = df_with_clusters.groupby('cluster').mean()\n",
    "print(\"\\nMedia delle feature per ciascun cluster:\")\n",
    "print(cluster_means)\n",
    "\n",
    "# Visualizziamo le caratteristiche di ciascun cluster per alcune feature selezionate\n",
    "selected_features_for_viz = ['alcohol', 'malic_acid', 'flavanoids', 'color_intensity', 'proline']\n",
    "\n",
    "# Creiamo un radar chart per visualizzare le caratteristiche dei cluster\n",
    "# Normalizziamo i valori per il radar chart\n",
    "cluster_means_normalized = cluster_means.copy()\n",
    "for feature in cluster_means.columns:\n",
    "    min_val = cluster_means[feature].min()\n",
    "    max_val = cluster_means[feature].max()\n",
    "    if max_val > min_val:  # Evita divisione per zero\n",
    "        cluster_means_normalized[feature] = (cluster_means[feature] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Selezioniamo solo le feature che vogliamo visualizzare\n",
    "cluster_means_normalized = cluster_means_normalized[selected_features_for_viz]\n",
    "\n",
    "# Creiamo il radar chart\n",
    "categories = selected_features_for_viz\n",
    "N = len(categories)\n",
    "\n",
    "# Creiamo gli angoli per il radar chart\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Chiudiamo il cerchio\n",
    "\n",
    "# Inizializziamo il plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Aggiungiamo le etichette\n",
    "plt.xticks(angles[:-1], categories, size=12)\n",
    "\n",
    "# Disegniamo i limiti del grafico\n",
    "ax.set_rlabel_position(0)\n",
    "plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.5\", \"0.75\"], color=\"grey\", size=10)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plottiamo ciascun cluster\n",
    "for cluster_id in range(3):\n",
    "    values = cluster_means_normalized.loc[cluster_id].values.tolist()\n",
    "    values += values[:1]  # Chiudiamo il cerchio\n",
    "    ax.plot(angles, values, linewidth=2, linestyle='solid', label=f'Cluster {cluster_id}')\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "# Aggiungiamo la legenda\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "plt.title('Caratteristiche dei cluster', size=15)\n",
    "plt.show()\n",
    "\n",
    "# Visualizziamo anche i box plot per alcune feature selezionate\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(selected_features_for_viz):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(x='cluster', y=feature, data=df_with_clusters)\n",
    "    plt.title(f'Distribuzione di {feature} per cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confrontiamo la distribuzione dei tipi di vino in ciascun cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "wine_type_by_cluster = pd.crosstab(df_with_clusters['cluster'], df_with_clusters['wine_type'], normalize='index') * 100\n",
    "wine_type_by_cluster.plot(kind='bar', stacked=True)\n",
    "plt.title('Distribuzione dei tipi di vino in ciascun cluster (%)')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Percentuale')\n",
    "plt.legend(title='Tipo di vino')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribuzione dei tipi di vino in ciascun cluster (%):\\n\")\n",
    "print(wine_type_by_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riflessione sulla clusterizzazione K-means\n",
    "\n",
    "Rifletti sui risultati della clusterizzazione K-means:\n",
    "- I cluster identificati da K-means corrispondono ai tipi di vino reali?\n",
    "- Qual è il numero ottimale di cluster secondo il metodo del gomito e il silhouette score?\n",
    "- Quali sono le caratteristiche distintive di ciascun cluster?\n",
    "- Come si confronta la clusterizzazione non supervisionata con la classificazione supervisionata?\n",
    "\n",
    "Puoi chiedere a Gemini di spiegarti come funziona l'algoritmo K-means e come interpretare i risultati con il seguente prompt:\n",
    "\n",
    "```\n",
    "Puoi spiegarmi come funziona l'algoritmo K-means? Come dovrei interpretare l'Adjusted Rand Index, il Silhouette Score e il metodo del gomito? Quali sono i limiti di K-means e in quali situazioni potrebbe non funzionare bene?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni e sfide aggiuntive\n",
    "\n",
    "Congratulazioni! Hai completato l'esercitazione sull'utilizzo di Gemini in Google Colab per un progetto di Machine Learning con focus sulla clusterizzazione K-means. Hai seguito le principali fasi di un progetto ML:\n",
    "\n",
    "1. Caricamento e analisi del dataset\n",
    "2. Pulizia dei dati\n",
    "3. Feature engineering semplice\n",
    "4. Realizzazione di un modello di classificazione\n",
    "5. Implementazione della clusterizzazione K-means\n",
    "\n",
    "### Confronto tra classificazione supervisionata e clusterizzazione non supervisionata\n",
    "\n",
    "In questo notebook, hai avuto l'opportunità di confrontare due approcci fondamentali del machine learning:\n",
    "- **Classificazione supervisionata** (Random Forest): utilizza le etichette per addestrare un modello che può predire la classe di nuovi dati\n",
    "- **Clusterizzazione non supervisionata** (K-means): raggruppa i dati in base alla loro similarità senza utilizzare le etichette\n",
    "\n",
    "Hai potuto osservare come K-means sia in grado di identificare pattern nei dati senza conoscere le etichette reali, e come questi pattern possano corrispondere (o meno) alle classi reali.\n",
    "\n",
    "### Sfide aggiuntive\n",
    "\n",
    "Se vuoi approfondire ulteriormente, ecco alcune sfide che puoi provare:\n",
    "\n",
    "1. Prova a utilizzare un dataset diverso (ad esempio, Breast Cancer o Digits da scikit-learn)\n",
    "2. Implementa altri algoritmi di clustering (DBSCAN, Hierarchical Clustering, Gaussian Mixture Models)\n",
    "3. Esplora tecniche di riduzione della dimensionalità diverse da PCA (t-SNE, UMAP)\n",
    "4. Implementa una pipeline completa di ML con GridSearchCV per l'ottimizzazione degli iperparametri di K-means\n",
    "5. Crea una visualizzazione interattiva dei risultati con Plotly\n",
    "\n",
    "Per ciascuna di queste sfide, puoi chiedere aiuto a Gemini con prompt specifici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt per sfide aggiuntive\n",
    "\n",
    "Ecco alcuni prompt che puoi utilizzare per le sfide aggiuntive:\n",
    "\n",
    "```\n",
    "Puoi fornirmi il codice per implementare DBSCAN sul dataset Wine e confrontare i risultati con K-means?\n",
    "```\n",
    "\n",
    "```\n",
    "Puoi mostrarmi come implementare t-SNE per visualizzare il dataset Wine in 2D e confrontarlo con PCA?\n",
    "```\n",
    "\n",
    "```\n",
    "Puoi fornirmi il codice per implementare una pipeline di ML con GridSearchCV per ottimizzare gli iperparametri di K-means sul dataset Wine?\n",
    "```\n",
    "\n",
    "```\n",
    "Puoi mostrarmi come creare una visualizzazione interattiva dei risultati di clustering con Plotly?\n",
    "```\n",
    "\n",
    "```\n",
    "Puoi spiegarmi le differenze tra K-means, DBSCAN e Hierarchical Clustering e quando è meglio utilizzare ciascun algoritmo?\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
