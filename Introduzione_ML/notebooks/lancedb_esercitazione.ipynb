{"cells":[{"cell_type":"markdown","metadata":{"id":"67b4VqhI9bwb"},"source":["# Esercitazione: Vector Database con LanceDB\n","\n","Questo notebook contiene esercizi pratici sull'utilizzo di LanceDB come vector database per dati testuali e di immagini. Completa gli esercizi seguendo le istruzioni e i suggerimenti forniti.\n","\n","## Obiettivi dell'esercitazione\n","\n","Al termine di questa esercitazione sarai in grado di:\n","1. Configurare e utilizzare LanceDB come vector database\n","2. Generare embedding per testi e immagini\n","3. Eseguire query di ricerca semantica\n","4. Implementare applicazioni di ricerca multimodale\n","\n","## Prerequisiti\n","\n","Prima di iniziare questa esercitazione, assicurati di aver compreso i concetti base presentati nei notebook introduttivi su LanceDB per il testo e LanceDB per le immagini con CLIP."]},{"cell_type":"markdown","metadata":{"id":"Ii7fDcaZ9bwe"},"source":["## Parte 1: Installazione delle dipendenze\n","\n","Per prima cosa, installiamo le librerie necessarie:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3WPNYp29bwe"},"outputs":[],"source":["# Installa le dipendenze necessarie\n","!pip install lancedb sentence-transformers datasets tqdm torch torchvision pillow matplotlib\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"markdown","metadata":{"id":"ic_jkhzV9bwf"},"source":["## Parte 2: Importazione delle librerie\n","\n","Importiamo le librerie che utilizzeremo in questa esercitazione:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NaiRamBY9bwf"},"outputs":[],"source":["import os\n","import torch\n","import clip\n","import lancedb\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer\n","import requests\n","from io import BytesIO\n","import torchvision.transforms as transforms\n","from torchvision.datasets import CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"7rXectv59bwf"},"source":["# Esercizio 1: Vector Database per il Testo\n","\n","In questo esercizio, creerai un vector database per dati testuali utilizzando LanceDB."]},{"cell_type":"markdown","metadata":{"id":"mUIMpBH59bwg"},"source":["## 1.1 Caricamento di un dataset testuale\n","\n","Utilizzeremo un dataset di recensioni di film in italiano dalla libreria Hugging Face datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQwUUCVF9bwg"},"outputs":[],"source":["# Carica il dataset di recensioni di film\n","dataset = load_dataset(\"italian_movies\", split=\"train\")\n","\n","# ESERCIZIO: Seleziona solo le prime 100 recensioni per questo esempio\n","# Suggerimento: usa il metodo select() o slice()\n","reviews = dataset.select(range(100))\n","\n","# Visualizza la struttura di una recensione\n","print(\"Struttura di una recensione:\")\n","for key in reviews[0].keys():\n","    print(f\"- {key}\")\n","\n","# Visualizza un esempio di titolo e recensione\n","print(f\"\\nTitolo: {reviews[0]['title']}\")\n","print(f\"Recensione: {reviews[0]['review'][:200]}...\")"]},{"cell_type":"markdown","metadata":{"id":"K1NT-p879bwg"},"source":["## 1.2 Chunking del testo\n","\n","Implementa una funzione per suddividere il testo in chunk più piccoli."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"id-zf8om9bwg","executionInfo":{"status":"ok","timestamp":1743101838092,"user_tz":-60,"elapsed":5,"user":{"displayName":"Massimo Bozza","userId":"16426671533548073617"}}},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione chunk_text\n","def chunk_text(text, chunk_size=150, overlap=30):\n","    \"\"\"\n","    Suddivide un testo in chunk di dimensione specificata con sovrapposizione.\n","\n","    Args:\n","        text (str): Il testo da suddividere\n","        chunk_size (int): Numero di parole per chunk\n","        overlap (int): Numero di parole di sovrapposizione tra chunk consecutivi\n","\n","    Returns:\n","        list: Lista di chunk di testo\n","    \"\"\"\n","    words = text.split()\n","    chunks = []\n","    for i in range(0, len(words), chunk_size - overlap):\n","        chunk = words[i:i + chunk_size]\n","        chunks.append(\" \".join(chunk))\n","    return chunks"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pJsuPWw9bwh","executionInfo":{"status":"ok","timestamp":1743101841229,"user_tz":-60,"elapsed":17,"user":{"displayName":"Massimo Bozza","userId":"16426671533548073617"}},"outputId":"0fa31f43-af64-403d-d6bf-23de24a3904b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testo originale: Questo è un testo di esempio per testare la funzione di chunking. Dovrebbe essere suddiviso in chunk più piccoli con una certa sovrapposizione tra i chunk consecutivi. Questo ci permette di mantenere il contesto anche quando dividiamo testi lunghi.\n","Numero di chunk: 6\n","Chunk 1: Questo è un testo di esempio per testare la funzione\n","Chunk 2: testare la funzione di chunking. Dovrebbe essere suddiviso in chunk\n","Chunk 3: suddiviso in chunk più piccoli con una certa sovrapposizione tra\n","Chunk 4: certa sovrapposizione tra i chunk consecutivi. Questo ci permette di\n","Chunk 5: ci permette di mantenere il contesto anche quando dividiamo testi\n","Chunk 6: quando dividiamo testi lunghi.\n"]}],"source":["# Test della funzione chunk_text\n","test_text = \"Questo è un testo di esempio per testare la funzione di chunking. Dovrebbe essere suddiviso in chunk più piccoli con una certa sovrapposizione tra i chunk consecutivi. Questo ci permette di mantenere il contesto anche quando dividiamo testi lunghi.\"\n","chunks = chunk_text(test_text, chunk_size=10, overlap=3)\n","\n","print(f\"Testo originale: {test_text}\")\n","print(f\"Numero di chunk: {len(chunks)}\")\n","for i, chunk in enumerate(chunks):\n","    print(f\"Chunk {i+1}: {chunk}\")"]},{"cell_type":"markdown","metadata":{"id":"n-8OVfSI9bwh"},"source":["## 1.3 Preparazione dei dati\n","\n","Prepara i dati creando chunk per ogni recensione."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKIx3yUt9bwh"},"outputs":[],"source":["# ESERCIZIO: Prepara i dati creando chunk per ogni recensione\n","all_chunks = []\n","chunk_metadata = []\n","\n","# Il tuo codice qui\n","# Suggerimento: itera su ogni recensione, crea chunk e aggiungi metadati\n","\n","print(f\"Totale recensioni: {len(reviews)}\")\n","print(f\"Totale chunks: {len(all_chunks)}\")\n","if all_chunks:\n","    print(f\"\\nEsempio di chunk: {all_chunks[0][:100]}...\")"]},{"cell_type":"markdown","metadata":{"id":"sgiCpmHQ9bwh"},"source":["## 1.4 Generazione degli embedding\n","\n","Utilizza un modello di SentenceTransformers per generare gli embedding dei chunk di testo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"my_9WwPN9bwh"},"outputs":[],"source":["# ESERCIZIO: Carica un modello di embedding e genera gli embedding per i chunk\n","# Suggerimento: usa SentenceTransformer con un modello multilingue\n","\n","# Il tuo codice qui\n","# Carica il modello\n","model = None  # Sostituisci con il tuo codice\n","\n","# Genera gli embedding\n","embeddings = []  # Sostituisci con il tuo codice\n","\n","if embeddings:\n","    print(f\"Dimensione di un embedding: {len(embeddings[0])}\")"]},{"cell_type":"markdown","metadata":{"id":"5IwxYcFB9bwh"},"source":["## 1.5 Creazione del Vector Database con LanceDB\n","\n","Crea un database LanceDB e carica gli embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YXU5qVN9bwh"},"outputs":[],"source":["# ESERCIZIO: Crea un dataframe con i chunk, i metadati e gli embedding\n","# Suggerimento: crea un dizionario per ogni chunk e convertilo in dataframe\n","\n","# Il tuo codice qui\n","df = None  # Sostituisci con il tuo codice\n","\n","if df is not None:\n","    print(f\"Dataframe shape: {df.shape}\")\n","    display(df.head(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlDyxrKT9bwh"},"outputs":[],"source":["# ESERCIZIO: Crea un database LanceDB e una tabella per i dati\n","# Suggerimento: usa lancedb.connect() e db.create_table()\n","\n","# Il tuo codice qui\n","db_path = \"./lancedb_reviews\"\n","table_name = \"movie_reviews\"\n","\n","# Crea il database e la tabella\n","# ...\n","\n","print(f\"Tabella '{table_name}' creata con successo!\")"]},{"cell_type":"markdown","metadata":{"id":"vj55AoEe9bwi"},"source":["## 1.6 Esecuzione di query semantiche\n","\n","Implementa una funzione per eseguire query semantiche sul vector database."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfLos84a9bwi"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione semantic_search\n","def semantic_search(query_text, top_k=5):\n","    \"\"\"\n","    Esegue una ricerca semantica nel vector database.\n","\n","    Args:\n","        query_text (str): Il testo della query\n","        top_k (int): Numero di risultati da restituire\n","\n","    Returns:\n","        list: Lista dei risultati più rilevanti\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: genera l'embedding per la query e usa table.search()\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTINo66c9bwi"},"outputs":[],"source":["# ESERCIZIO: Testa la funzione semantic_search con diverse query\n","# Suggerimento: prova query come \"film d'azione emozionante\" o \"commedia divertente\"\n","\n","# Il tuo codice qui\n","query = \"film d'azione emozionante\"\n","# Esegui la ricerca e visualizza i risultati\n","# ..."]},{"cell_type":"markdown","metadata":{"id":"ZtZSkrEq9bwi"},"source":["# Esercizio 2: Vector Database per le Immagini con CLIP\n","\n","In questo esercizio, creerai un vector database per dati di immagini utilizzando LanceDB e CLIP."]},{"cell_type":"markdown","metadata":{"id":"04f_sFqH9bwi"},"source":["## 2.1 Caricamento del modello CLIP\n","\n","Carica il modello CLIP pre-addestrato."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm0yDhyN9bwi"},"outputs":[],"source":["# ESERCIZIO: Carica il modello CLIP\n","# Suggerimento: usa clip.load() con il modello \"ViT-B/32\"\n","\n","# Il tuo codice qui\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# Carica il modello CLIP\n","# ...\n","\n","print(f\"Modello CLIP caricato su {device}\")"]},{"cell_type":"markdown","metadata":{"id":"nsjaHjUS9bwi"},"source":["## 2.2 Caricamento di un dataset di immagini\n","\n","Carica il dataset CIFAR-10 per questo esercizio."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6p3iJ9S9bwi"},"outputs":[],"source":["# ESERCIZIO: Carica il dataset CIFAR-10 e seleziona un sottoinsieme di immagini\n","# Suggerimento: usa CIFAR10 da torchvision.datasets\n","\n","# Il tuo codice qui\n","# Carica il dataset\n","# ...\n","\n","# Seleziona un sottoinsieme di immagini\n","num_images = 200\n","# ...\n","\n","print(f\"Caricate {len(images)} immagini da {len(set(labels))} classi diverse\")\n","print(f\"Alcune classi presenti: {', '.join(set(labels)[:5])}...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vU02FN469bwi"},"outputs":[],"source":["# ESERCIZIO: Visualizza alcune immagini di esempio\n","# Suggerimento: usa matplotlib per visualizzare le immagini\n","\n","# Il tuo codice qui\n","# ..."]},{"cell_type":"markdown","metadata":{"id":"uUW3uIEL9bwi"},"source":["## 2.3 Generazione degli embedding con CLIP\n","\n","Implementa una funzione per generare embedding di immagini con CLIP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h03bHm7O9bwi"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione get_image_embedding\n","def get_image_embedding(image):\n","    \"\"\"\n","    Genera l'embedding di un'immagine utilizzando CLIP.\n","\n","    Args:\n","        image (PIL.Image): L'immagine di input\n","\n","    Returns:\n","        numpy.ndarray: L'embedding dell'immagine\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: preprocessa l'immagine, genera l'embedding e normalizzalo\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RBPN8VqW9bwi"},"outputs":[],"source":["# ESERCIZIO: Genera gli embedding per tutte le immagini\n","# Suggerimento: usa un ciclo con tqdm per mostrare una barra di progresso\n","\n","# Il tuo codice qui\n","embeddings = []\n","# ...\n","\n","if embeddings:\n","    print(f\"Dimensione di un embedding: {len(embeddings[0])}\")"]},{"cell_type":"markdown","metadata":{"id":"7i0g974h9bwi"},"source":["## 2.4 Creazione del Vector Database con LanceDB\n","\n","Crea un database LanceDB e carica gli embedding delle immagini."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fLa9YCH9bwj"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione image_to_bytes\n","def image_to_bytes(image):\n","    \"\"\"\n","    Converte un'immagine PIL in array di byte.\n","\n","    Args:\n","        image (PIL.Image): L'immagine di input\n","\n","    Returns:\n","        bytes: L'immagine convertita in bytes\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: usa BytesIO e il metodo save di PIL\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sZQ7qRN9bwj"},"outputs":[],"source":["# ESERCIZIO: Crea un dataframe con le immagini, le etichette e gli embedding\n","# Suggerimento: crea un dizionario per ogni immagine e convertilo in dataframe\n","\n","# Il tuo codice qui\n","df = None  # Sostituisci con il tuo codice\n","\n","if df is not None:\n","    print(f\"Dataframe shape: {df.shape}\")\n","    display(df[['id', 'label']].head(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaXhWwyw9bwj"},"outputs":[],"source":["# ESERCIZIO: Crea un database LanceDB e una tabella per i dati\n","# Suggerimento: usa lancedb.connect() e db.create_table()\n","\n","# Il tuo codice qui\n","db_path = \"./lancedb_cifar\"\n","table_name = \"cifar_images\"\n","\n","# Crea il database e la tabella\n","# ...\n","\n","print(f\"Tabella '{table_name}' creata con successo!\")"]},{"cell_type":"markdown","metadata":{"id":"lmd5VGHU9bwj"},"source":["## 2.5 Ricerca semantica multimodale (testo-immagine)\n","\n","Implementa funzioni per eseguire query semantiche utilizzando descrizioni testuali."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSOutxM59bwj"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione get_text_embedding\n","def get_text_embedding(text):\n","    \"\"\"\n","    Genera l'embedding di un testo utilizzando CLIP.\n","\n","    Args:\n","        text (str): Il testo di input\n","\n","    Returns:\n","        numpy.ndarray: L'embedding del testo\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: tokenizza il testo, genera l'embedding e normalizzalo\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JppK1xp79bwj"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione display_results\n","def display_results(results, query=None):\n","    \"\"\"\n","    Visualizza i risultati della ricerca.\n","\n","    Args:\n","        results (pandas.DataFrame): I risultati della ricerca\n","        query (str, optional): La query di ricerca\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: usa matplotlib per visualizzare le immagini\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZc6MXfX9bwj"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione semantic_search\n","def semantic_search(query_text, top_k=10):\n","    \"\"\"\n","    Esegue una ricerca semantica nel vector database utilizzando una query testuale.\n","\n","    Args:\n","        query_text (str): Il testo della query\n","        top_k (int): Numero di risultati da restituire\n","\n","    Returns:\n","        pandas.DataFrame: Dataframe con i risultati più rilevanti\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: genera l'embedding per la query e usa table.search()\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBztkoEy9bwj"},"outputs":[],"source":["# ESERCIZIO: Testa la funzione semantic_search con diverse query\n","# Suggerimento: prova query come \"un animale\" o \"un veicolo\"\n","\n","# Il tuo codice qui\n","query = \"un animale\"\n","# Esegui la ricerca e visualizza i risultati\n","# ..."]},{"cell_type":"markdown","metadata":{"id":"X-FDrrKj9bwo"},"source":["## 2.6 Ricerca per similarità di immagine\n","\n","Implementa una funzione per cercare immagini simili a un'immagine di riferimento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnhCFyGI9bwo"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione image_search\n","def image_search(query_image, top_k=10):\n","    \"\"\"\n","    Esegue una ricerca semantica nel vector database utilizzando un'immagine come query.\n","\n","    Args:\n","        query_image (PIL.Image): L'immagine di query\n","        top_k (int): Numero di risultati da restituire\n","\n","    Returns:\n","        pandas.DataFrame: Dataframe con i risultati più rilevanti\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: genera l'embedding per l'immagine e usa table.search()\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LahC6HXg9bwo"},"outputs":[],"source":["# ESERCIZIO: Testa la funzione image_search con un'immagine casuale\n","# Suggerimento: seleziona un'immagine casuale dal dataset e cercane di simili\n","\n","# Il tuo codice qui\n","# Seleziona un'immagine casuale\n","# ...\n","\n","# Esegui la ricerca e visualizza i risultati\n","# ..."]},{"cell_type":"markdown","metadata":{"id":"jEwx7f8X9bwo"},"source":["# Esercizio 3: Applicazione pratica - Motore di ricerca multimodale\n","\n","In questo esercizio finale, creerai un'applicazione pratica che combina la ricerca testuale e di immagini."]},{"cell_type":"markdown","metadata":{"id":"yCgcpWUu9bwo"},"source":["## 3.1 Caricamento di immagini esterne\n","\n","Implementa una funzione per caricare immagini da URL."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"auYwjbGd9bwp"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione load_image_from_url\n","def load_image_from_url(url):\n","    \"\"\"\n","    Carica un'immagine da un URL.\n","\n","    Args:\n","        url (str): URL dell'immagine\n","\n","    Returns:\n","        PIL.Image: Immagine caricata\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: usa requests per scaricare l'immagine e PIL per aprirla\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"VXX3D6M79bwp"},"source":["## 3.2 Ricerca combinata testo-immagine\n","\n","Implementa una funzione che combina la ricerca testuale e di immagini."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeUwPCEF9bwp"},"outputs":[],"source":["# ESERCIZIO: Implementa la funzione combined_search\n","def combined_search(text_query=None, image_query=None, weights=(0.5, 0.5), top_k=10):\n","    \"\"\"\n","    Esegue una ricerca combinata testo-immagine nel vector database.\n","\n","    Args:\n","        text_query (str, optional): Il testo della query\n","        image_query (PIL.Image, optional): L'immagine di query\n","        weights (tuple): Pesi per la combinazione (testo, immagine)\n","        top_k (int): Numero di risultati da restituire\n","\n","    Returns:\n","        pandas.DataFrame: Dataframe con i risultati più rilevanti\n","    \"\"\"\n","    # Il tuo codice qui\n","    # Suggerimento: genera gli embedding per testo e immagine, combinali con i pesi\n","    # e usa table.search()\n","\n","    # Rimuovi questa riga e implementa la tua soluzione\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6tt2gtL9bwp"},"outputs":[],"source":["# ESERCIZIO: Testa la funzione combined_search\n","# Suggerimento: prova diverse combinazioni di query testuali e di immagini\n","\n","# Il tuo codice qui\n","text_query = \"un animale carino\"\n","# Seleziona un'immagine o caricala da URL\n","# ...\n","\n","# Esegui la ricerca combinata e visualizza i risultati\n","# ..."]},{"cell_type":"markdown","metadata":{"id":"JVMD2P4Q9bwp"},"source":["## 3.3 Creazione di un'interfaccia utente semplice\n","\n","Crea un'interfaccia utente semplice per il tuo motore di ricerca multimodale."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXmVv-nl9bwp"},"outputs":[],"source":["# ESERCIZIO BONUS: Crea un'interfaccia utente semplice con ipywidgets\n","# Suggerimento: installa ipywidgets e crea widget per input di testo, caricamento di immagini e visualizzazione dei risultati\n","\n","# Il tuo codice qui\n","# !pip install ipywidgets\n","# import ipywidgets as widgets\n","# from IPython.display import display\n","# ...\n","\n","# Crea l'interfaccia utente\n","# ..."]},{"cell_type":"markdown","metadata":{"id":"yDz8OOTJ9bwp"},"source":["# Domande di comprensione\n","\n","Rispondi alle seguenti domande per verificare la tua comprensione dei concetti presentati in questa esercitazione:\n","\n","1. Cosa sono gli embedding e perché sono utili per la ricerca semantica?\n","\n","2. Quali sono i vantaggi di utilizzare un vector database come LanceDB rispetto a un database tradizionale?\n","\n","3. Perché è importante suddividere i testi lunghi in chunk più piccoli quando si lavora con vector database?\n","\n","4. Come funziona CLIP e perché è particolarmente utile per la ricerca multimodale?\n","\n","5. Quali sono alcune applicazioni pratiche dei vector database nel campo dell'intelligenza artificiale e del machine learning?"]},{"cell_type":"markdown","metadata":{"id":"Lv8efUg99bwp"},"source":["# Soluzioni\n","\n","Le soluzioni agli esercizi sono disponibili nel notebook separato \"lancedb_esercitazione_soluzioni.ipynb\"."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}