{"cells":[{"cell_type":"markdown","metadata":{"id":"aGBhbdf09E2I"},"source":["# Vector Database per il Testo con LanceDB\n","\n","Questo notebook dimostra come utilizzare LanceDB come vector database per dati testuali. Vedremo come:\n","\n","1. Installare le dipendenze necessarie\n","2. Caricare e preparare un dataset testuale\n","3. Suddividere il testo in chunk\n","4. Generare embedding per i chunk di testo\n","5. Caricare gli embedding in LanceDB\n","6. Eseguire query di ricerca semantica\n","\n","## Cos'è un Vector Database?\n","\n","Un vector database è un tipo di database ottimizzato per memorizzare e cercare vettori di embedding. Gli embedding sono rappresentazioni numeriche di dati (come testo o immagini) che catturano il significato semantico. I vector database consentono di eseguire ricerche semantiche efficienti, trovando elementi simili in base alla loro vicinanza nello spazio vettoriale."]},{"cell_type":"markdown","metadata":{"id":"jloynni_9E2L"},"source":["## 1. Installazione delle dipendenze\n","\n","Per prima cosa, installiamo le librerie necessarie:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILBJYC6m9E2L","executionInfo":{"status":"ok","timestamp":1743101787519,"user_tz":-60,"elapsed":132383,"user":{"displayName":"Massimo Bozza","userId":"16426671533548073617"}},"outputId":"c7931ad7-f0da-4069-d613-5f1b86551c55"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lancedb\n","  Downloading lancedb-0.21.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Collecting deprecation (from lancedb)\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: pyarrow>=14 in /usr/local/lib/python3.11/dist-packages (from lancedb) (18.1.0)\n","Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.11/dist-packages (from lancedb) (2.10.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lancedb) (24.2)\n","Collecting overrides>=0.7 (from lancedb)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.0)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.29.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10->lancedb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10->lancedb) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Downloading lancedb-0.21.2-cp39-abi3-manylinux_2_28_x86_64.whl (32.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, overrides, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, deprecation, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, lancedb, datasets\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.0\n","    Uninstalling fsspec-2025.3.0:\n","      Successfully uninstalled fsspec-2025.3.0\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 deprecation-2.1.0 dill-0.3.8 fsspec-2024.12.0 lancedb-0.21.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 overrides-7.7.0 xxhash-3.5.0\n"]}],"source":["!pip install lancedb sentence-transformers datasets tqdm"]},{"cell_type":"markdown","metadata":{"id":"8DUZq6CE9E2M"},"source":["## 2. Importazione delle librerie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQt0BqVv9E2M"},"outputs":[],"source":["import os\n","import lancedb\n","import pandas as pd\n","import numpy as np\n","from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"Eb_oBTXQ9E2N"},"source":["## 3. Caricamento di un dataset testuale\n","\n","Utilizzeremo un dataset di articoli di Wikipedia in italiano dalla libreria Hugging Face datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44dEOigS9E2N"},"outputs":[],"source":["# Carichiamo un dataset di esempio (articoli di Wikipedia in italiano)\n","dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.it\", split=\"train\", streaming=True)\n","# Prendiamo solo i primi 50 articoli per questo esempio\n","articles = list(dataset.take(50))\n","\n","# Visualizziamo la struttura di un articolo\n","print(\"Struttura di un articolo:\")\n","for key in articles[0].keys():\n","    print(f\"- {key}\")\n","\n","# Visualizziamo un esempio di titolo e l'inizio del testo\n","print(f\"\\nTitolo: {articles[0]['title']}\")\n","print(f\"Inizio del testo: {articles[0]['text'][:300]}...\")"]},{"cell_type":"markdown","metadata":{"id":"_5gvDymk9E2N"},"source":["## 4. Chunking del testo\n","\n","Per gestire testi lunghi, è necessario suddividerli in chunk più piccoli. Questo processo è chiamato \"chunking\" ed è fondamentale per l'elaborazione efficiente del testo nei vector database."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8u31tQSI9E2N"},"outputs":[],"source":["def chunk_text(text, chunk_size=200, overlap=50):\n","    \"\"\"\n","    Suddivide un testo in chunk di dimensione specificata con sovrapposizione.\n","\n","    Args:\n","        text (str): Il testo da suddividere\n","        chunk_size (int): Numero di parole per chunk\n","        overlap (int): Numero di parole di sovrapposizione tra chunk consecutivi\n","\n","    Returns:\n","        list: Lista di chunk di testo\n","    \"\"\"\n","    words = text.split()\n","    chunks = []\n","\n","    if len(words) <= chunk_size:\n","        return [text]\n","\n","    i = 0\n","    while i < len(words):\n","        # Prendiamo chunk_size parole o fino alla fine del testo\n","        chunk_words = words[i:i + chunk_size]\n","        chunk = \" \".join(chunk_words)\n","        chunks.append(chunk)\n","\n","        # Avanziamo di (chunk_size - overlap) parole\n","        i += (chunk_size - overlap)\n","\n","    return chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5jZInbT9E2O"},"outputs":[],"source":["# Prepariamo i dati: creiamo chunk per ogni articolo\n","all_chunks = []\n","chunk_metadata = []\n","\n","for i, article in enumerate(tqdm(articles, desc=\"Chunking articles\")):\n","    title = article['title']\n","    text = article['text']\n","\n","    # Suddividiamo il testo in chunk\n","    chunks = chunk_text(text)\n","\n","    # Aggiungiamo ogni chunk alla lista con i relativi metadati\n","    for j, chunk in enumerate(chunks):\n","        all_chunks.append(chunk)\n","        chunk_metadata.append({\n","            'article_id': i,\n","            'title': title,\n","            'chunk_id': j,\n","            'total_chunks': len(chunks)\n","        })\n","\n","print(f\"Totale articoli: {len(articles)}\")\n","print(f\"Totale chunks: {len(all_chunks)}\")\n","print(f\"\\nEsempio di chunk: {all_chunks[10][:150]}...\")"]},{"cell_type":"markdown","metadata":{"id":"joYojm_I9E2O"},"source":["## 5. Generazione degli embedding\n","\n","Utilizzeremo un modello di SentenceTransformers pre-addestrato per generare gli embedding dei nostri chunk di testo. Per il testo in italiano, utilizziamo un modello multilingue."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7bFfkVrb9E2O"},"outputs":[],"source":["# Carichiamo un modello di embedding multilingue\n","model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n","\n","# Generiamo gli embedding per tutti i chunk\n","embeddings = []\n","\n","# Processiamo i chunk in batch per efficienza\n","batch_size = 32\n","for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"Generating embeddings\"):\n","    batch = all_chunks[i:i+batch_size]\n","    batch_embeddings = model.encode(batch)\n","    embeddings.extend(batch_embeddings)\n","\n","print(f\"Dimensione di un embedding: {len(embeddings[0])}\")"]},{"cell_type":"markdown","metadata":{"id":"pojgWCfb9E2O"},"source":["## 6. Creazione del Vector Database con LanceDB\n","\n","Ora creiamo un database LanceDB e carichiamo i nostri embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzqGHkyl9E2O"},"outputs":[],"source":["# Creiamo un dataframe con i chunk, i metadati e gli embedding\n","data = []\n","for i in range(len(all_chunks)):\n","    data.append({\n","        'text': all_chunks[i],\n","        'article_id': chunk_metadata[i]['article_id'],\n","        'title': chunk_metadata[i]['title'],\n","        'chunk_id': chunk_metadata[i]['chunk_id'],\n","        'total_chunks': chunk_metadata[i]['total_chunks'],\n","        'vector': embeddings[i]\n","    })\n","\n","df = pd.DataFrame(data)\n","print(f\"Dataframe shape: {df.shape}\")\n","df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64mdJuh79E2P"},"outputs":[],"source":["# Creiamo un database LanceDB\n","db_path = \"./lancedb_wiki\"\n","db = lancedb.connect(db_path)\n","\n","# Creiamo una tabella per i nostri dati\n","table_name = \"wiki_articles\"\n","\n","# Se la tabella esiste già, la eliminiamo\n","if table_name in db.table_names():\n","    db.drop_table(table_name)\n","\n","# Creiamo la tabella con i nostri dati\n","table = db.create_table(table_name, data=df, mode=\"overwrite\")\n","\n","print(f\"Tabella '{table_name}' creata con successo!\")"]},{"cell_type":"markdown","metadata":{"id":"wlBI9xCl9E2P"},"source":["## 7. Esecuzione di query semantiche\n","\n","Ora possiamo eseguire query semantiche sul nostro vector database."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uI9lS2oq9E2P"},"outputs":[],"source":["def semantic_search(query_text, top_k=5):\n","    \"\"\"\n","    Esegue una ricerca semantica nel vector database.\n","\n","    Args:\n","        query_text (str): Il testo della query\n","        top_k (int): Numero di risultati da restituire\n","\n","    Returns:\n","        list: Lista dei risultati più rilevanti\n","    \"\"\"\n","    # Generiamo l'embedding per la query\n","    query_embedding = model.encode(query_text)\n","\n","    # Eseguiamo la ricerca vettoriale\n","    results = table.search(query_embedding).limit(top_k).to_pandas()\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ITKoZ3q39E2P"},"outputs":[],"source":["# Esempio di query semantica\n","query = \"storia dell'Italia antica\"\n","results = semantic_search(query)\n","\n","print(f\"Query: '{query}'\\n\")\n","print(\"Risultati più rilevanti:\")\n","for i, row in results.iterrows():\n","    print(f\"\\n--- Risultato {i+1} ---\")\n","    print(f\"Titolo: {row['title']}\")\n","    print(f\"Chunk: {row['chunk_id']+1}/{row['total_chunks']}\")\n","    print(f\"Testo: {row['text'][:200]}...\")\n","    print(f\"Distanza: {row['_distance']:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsDFuuMC9E2P"},"outputs":[],"source":["# Proviamo un'altra query\n","query = \"scienza e tecnologia moderna\"\n","results = semantic_search(query)\n","\n","print(f\"Query: '{query}'\\n\")\n","print(\"Risultati più rilevanti:\")\n","for i, row in results.iterrows():\n","    print(f\"\\n--- Risultato {i+1} ---\")\n","    print(f\"Titolo: {row['title']}\")\n","    print(f\"Chunk: {row['chunk_id']+1}/{row['total_chunks']}\")\n","    print(f\"Testo: {row['text'][:200]}...\")\n","    print(f\"Distanza: {row['_distance']:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"LkXa3nZN9E2P"},"source":["## 8. Filtraggio dei risultati\n","\n","LanceDB supporta anche il filtraggio dei risultati in base ai metadati."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wihNwsK39E2P"},"outputs":[],"source":["def filtered_search(query_text, title_filter=None, top_k=5):\n","    \"\"\"\n","    Esegue una ricerca semantica con filtro sul titolo.\n","\n","    Args:\n","        query_text (str): Il testo della query\n","        title_filter (str): Filtro sul titolo (opzionale)\n","        top_k (int): Numero di risultati da restituire\n","\n","    Returns:\n","        list: Lista dei risultati più rilevanti\n","    \"\"\"\n","    # Generiamo l'embedding per la query\n","    query_embedding = model.encode(query_text)\n","\n","    # Prepariamo la query\n","    search_query = table.search(query_embedding)\n","\n","    # Applichiamo il filtro se specificato\n","    if title_filter:\n","        search_query = search_query.where(f\"title LIKE '%{title_filter}%'\")\n","\n","    # Eseguiamo la ricerca\n","    results = search_query.limit(top_k).to_pandas()\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7ItqHcB9E2P"},"outputs":[],"source":["# Esempio di ricerca con filtro\n","query = \"eventi importanti\"\n","title_filter = \"storia\"\n","results = filtered_search(query, title_filter)\n","\n","print(f\"Query: '{query}' (filtro titolo: '{title_filter}')\\n\")\n","print(\"Risultati più rilevanti:\")\n","for i, row in results.iterrows():\n","    print(f\"\\n--- Risultato {i+1} ---\")\n","    print(f\"Titolo: {row['title']}\")\n","    print(f\"Chunk: {row['chunk_id']+1}/{row['total_chunks']}\")\n","    print(f\"Testo: {row['text'][:200]}...\")\n","    print(f\"Distanza: {row['_distance']:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"ZayOO1Ue9E2P"},"source":["## 9. Aggiornamento del Vector Database\n","\n","LanceDB supporta anche l'aggiornamento incrementale dei dati."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhWWcwtp9E2P"},"outputs":[],"source":["# Creiamo un nuovo chunk da aggiungere\n","new_text = \"L'intelligenza artificiale è un campo dell'informatica che si occupa di creare sistemi in grado di svolgere compiti che normalmente richiederebbero l'intelligenza umana. Questi compiti includono il riconoscimento vocale, il processo decisionale, la traduzione tra lingue e la percezione visiva.\"\n","new_title = \"Intelligenza Artificiale\"\n","\n","# Generiamo l'embedding\n","new_embedding = model.encode(new_text)\n","\n","# Creiamo un dataframe con il nuovo dato\n","new_data = pd.DataFrame([\n","    {\n","        'text': new_text,\n","        'article_id': len(articles),  # Nuovo ID\n","        'title': new_title,\n","        'chunk_id': 0,\n","        'total_chunks': 1,\n","        'vector': new_embedding\n","    }\n","])\n","\n","# Aggiungiamo il nuovo dato alla tabella\n","table.add(new_data)\n","\n","print(f\"Nuovo chunk aggiunto al database!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7zoz9lx9E2Q"},"outputs":[],"source":["# Verifichiamo che il nuovo dato sia stato aggiunto\n","query = \"intelligenza artificiale e machine learning\"\n","results = semantic_search(query)\n","\n","print(f\"Query: '{query}'\\n\")\n","print(\"Risultati più rilevanti:\")\n","for i, row in results.iterrows():\n","    print(f\"\\n--- Risultato {i+1} ---\")\n","    print(f\"Titolo: {row['title']}\")\n","    print(f\"Chunk: {row['chunk_id']+1}/{row['total_chunks']}\")\n","    print(f\"Testo: {row['text'][:200]}...\")\n","    print(f\"Distanza: {row['_distance']:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"htIBp-__9E2Q"},"source":["## 10. Conclusioni\n","\n","In questo notebook abbiamo visto come:\n","\n","1. Caricare e preparare un dataset testuale\n","2. Suddividere il testo in chunk\n","3. Generare embedding per i chunk di testo\n","4. Creare un vector database con LanceDB\n","5. Eseguire query semantiche\n","6. Filtrare i risultati in base ai metadati\n","7. Aggiornare il database con nuovi dati\n","\n","I vector database come LanceDB sono strumenti potenti per la ricerca semantica e possono essere utilizzati in molte applicazioni, come motori di ricerca, sistemi di raccomandazione, chatbot e molto altro."]},{"cell_type":"markdown","metadata":{"id":"VPZHb-Q_9E2Q"},"source":["## Esercizi aggiuntivi\n","\n","1. Prova a utilizzare un modello di embedding diverso (ad esempio, un modello specifico per l'italiano)\n","2. Sperimenta con diverse strategie di chunking (dimensione dei chunk, sovrapposizione)\n","3. Implementa una funzione per recuperare l'intero articolo dato un chunk rilevante\n","4. Crea un'interfaccia utente semplice per la ricerca semantica"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}