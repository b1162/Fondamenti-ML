{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprendimento Supervisionato: Esercitazioni Pratiche\n",
    "\n",
    "Questo notebook contiene esercitazioni pratiche sui tre temi principali dell'apprendimento supervisionato:\n",
    "1. Algoritmi di regressione e classificazione\n",
    "2. Addestramento e valutazione del modello\n",
    "3. Overfitting e regolarizzazione\n",
    "\n",
    "Attraverso esempi concreti e dataset reali, metteremo in pratica i concetti teorici visti nelle slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup iniziale\n",
    "\n",
    "Importiamo le librerie necessarie e configuriamo l'ambiente di lavoro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie di base\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Configurazione visualizzazioni\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Impostazioni di visualizzazione per pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "\n",
    "# Impostazione seed per riproducibilità\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Algoritmi di Regressione e Classificazione\n",
    "\n",
    "In questa prima parte, esploreremo diversi algoritmi di regressione e classificazione, applicandoli a dataset reali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regressione: Previsione dei prezzi delle case\n",
    "\n",
    "Utilizzeremo il dataset California Housing per prevedere i prezzi delle case in base a diverse caratteristiche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Caricamento del dataset\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = housing.target\n",
    "\n",
    "# Visualizziamo le prime righe del dataset\n",
    "print(\"Dimensioni del dataset:\", X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiche descrittive\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la distribuzione della variabile target\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y, bins=50, alpha=0.7)\n",
    "plt.axvline(y.mean(), color='red', linestyle='--', label=f'Media: {y.mean():.2f}')\n",
    "plt.axvline(np.median(y), color='green', linestyle='--', label=f'Mediana: {np.median(y):.2f}')\n",
    "plt.xlabel('Prezzo mediano delle case (in $100,000)')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.title('Distribuzione dei prezzi delle case')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizziamo le correlazioni tra le feature e il target\n",
    "correlations = pd.DataFrame(X.corrwith(pd.Series(y)), columns=['Correlazione con il prezzo'])\n",
    "correlations = correlations.sort_values('Correlazione con il prezzo', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=correlations.index, y=correlations['Correlazione con il prezzo'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Correlazione delle feature con il prezzo delle case')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la matrice di correlazione tra le feature\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = X.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Matrice di correlazione delle feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparazione dei dati per la regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suddivisione in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizzazione delle feature\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dimensioni X_train: {X_train.shape}\")\n",
    "print(f\"Dimensioni X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Regressione Lineare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = linear_model.predict(X_train_scaled)\n",
    "y_pred_test = linear_model.predict(X_test_scaled)\n",
    "\n",
    "# Valutazione\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Regressione Lineare:\")\n",
    "print(f\"  MSE Train: {train_mse:.4f}\")\n",
    "print(f\"  MSE Test: {test_mse:.4f}\")\n",
    "print(f\"  R² Train: {train_r2:.4f}\")\n",
    "print(f\"  R² Test: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i coefficienti\n",
    "coef = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficiente': linear_model.coef_\n",
    "})\n",
    "coef = coef.sort_values('Coefficiente', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficiente', y='Feature', data=coef)\n",
    "plt.title('Coefficienti della Regressione Lineare')\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo le predizioni vs valori reali\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Valori reali')\n",
    "plt.ylabel('Predizioni')\n",
    "plt.title('Regressione Lineare: Predizioni vs Valori reali')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Regressione Ridge (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = ridge_model.predict(X_train_scaled)\n",
    "y_pred_test = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Valutazione\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Regressione Ridge:\")\n",
    "print(f\"  MSE Train: {train_mse:.4f}\")\n",
    "print(f\"  MSE Test: {test_mse:.4f}\")\n",
    "print(f\"  R² Train: {train_r2:.4f}\")\n",
    "print(f\"  R² Test: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confrontiamo i coefficienti di Ridge con quelli della regressione lineare\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Linear': linear_model.coef_,\n",
    "    'Ridge': ridge_model.coef_\n",
    "})\n",
    "\n",
    "# Calcoliamo la differenza percentuale\n",
    "coef_comparison['Diff %'] = 100 * (coef_comparison['Ridge'] - coef_comparison['Linear']) / coef_comparison['Linear']\n",
    "coef_comparison = coef_comparison.sort_values('Diff %', key=abs, ascending=False)\n",
    "\n",
    "coef_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Regressione Lasso (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = lasso_model.predict(X_train_scaled)\n",
    "y_pred_test = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Valutazione\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Regressione Lasso:\")\n",
    "print(f\"  MSE Train: {train_mse:.4f}\")\n",
    "print(f\"  MSE Test: {test_mse:.4f}\")\n",
    "print(f\"  R² Train: {train_r2:.4f}\")\n",
    "print(f\"  R² Test: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i coefficienti di Lasso\n",
    "coef_lasso = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficiente': lasso_model.coef_\n",
    "})\n",
    "coef_lasso = coef_lasso.sort_values('Coefficiente', key=abs, ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficiente', y='Feature', data=coef_lasso)\n",
    "plt.title('Coefficienti della Regressione Lasso')\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Contiamo quanti coefficienti sono esattamente zero\n",
    "zero_coefs = (lasso_model.coef_ == 0).sum()\n",
    "print(f\"Numero di coefficienti esattamente zero: {zero_coefs} su {len(lasso_model.coef_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)  # Nota: non è necessario scalare i dati per Random Forest\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "# Valutazione\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Random Forest Regressor:\")\n",
    "print(f\"  MSE Train: {train_mse:.4f}\")\n",
    "print(f\"  MSE Test: {test_mse:.4f}\")\n",
    "print(f\"  R² Train: {train_r2:.4f}\")\n",
    "print(f\"  R² Test: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo l'importanza delle feature\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Random Forest: Importanza delle Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo le predizioni vs valori reali\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Valori reali')\n",
    "plt.ylabel('Predizioni')\n",
    "plt.title('Random Forest: Predizioni vs Valori reali')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Confronto tra modelli di regressione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confrontiamo le performance dei diversi modelli\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Per Random Forest non è necessario scalare i dati\n",
    "    if name == 'Random Forest':\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train MSE': train_mse,\n",
    "        'Test MSE': test_mse,\n",
    "        'Train R²': train_r2,\n",
    "        'Test R²': test_r2\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# MSE\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df['Model'], results_df['Train MSE'], alpha=0.7, label='Train')\n",
    "plt.bar(results_df['Model'], results_df['Test MSE'], alpha=0.7, label='Test')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE per modello')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# R²\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(results_df['Model'], results_df['Train R²'], alpha=0.7, label='Train')\n",
    "plt.bar(results_df['Model'], results_df['Test R²'], alpha=0.7, label='Test')\n",
    "plt.ylabel('R²')\n",
    "plt.title('R² per modello')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classificazione: Previsione del cancro al seno\n",
    "\n",
    "Utilizzeremo il dataset Breast Cancer di scikit-learn per classificare i tumori come maligni o benigni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Caricamento del dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target\n",
    "\n",
    "# Visualizziamo le prime righe del dataset\n",
    "print(\"Dimensioni del dataset:\", X.shape)\n",
    "print(\"Classi:\", cancer.target_names)\n",
    "print(\"Distribuzione delle classi:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiche descrittive\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la distribuzione delle feature per classe\n",
    "# Selezioniamo solo alcune feature per chiarezza\n",
    "selected_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(selected_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.histplot(X[feature][y == 0], color='red', alpha=0.5, label='Maligno')\n",
    "    sns.histplot(X[feature][y == 1], color='blue', alpha=0.5, label='Benigno')\n",
    "    plt.xlabel(feature)\n",
    "    plt.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparazione dei dati per la classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suddivisione in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardizzazione delle feature\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dimensioni X_train: {X_train.shape}\")\n",
    "print(f\"Dimensioni X_test: {X_test.shape}\")\n",
    "print(f\"Distribuzione delle classi in y_train: {pd.Series(y_train).value_counts()}\")\n",
    "print(f\"Distribuzione delle classi in y_test: {pd.Series(y_test).value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Regressione Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = log_reg.predict(X_train_scaled)\n",
    "y_pred_test = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Valutazione\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Regressione Logistica:\")\n",
    "print(f\"  Accuracy Train: {train_acc:.4f}\")\n",
    "print(f\"  Accuracy Test: {test_acc:.4f}\")\n",
    "print(f\"  Precision Test: {test_precision:.4f}\")\n",
    "print(f\"  Recall Test: {test_recall:.4f}\")\n",
    "print(f\"  F1 Score Test: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer.target_names, \n",
    "            yticklabels=cancer.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matrice di Confusione - Regressione Logistica')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i coefficienti\n",
    "coef = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficiente': log_reg.coef_[0]\n",
    "})\n",
    "coef = coef.sort_values('Coefficiente', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Coefficiente', y='Feature', data=coef)\n",
    "plt.title('Coefficienti della Regressione Logistica')\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = svm_model.predict(X_train_scaled)\n",
    "y_pred_test = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Valutazione\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Support Vector Machine:\")\n",
    "print(f\"  Accuracy Train: {train_acc:.4f}\")\n",
    "print(f\"  Accuracy Test: {test_acc:.4f}\")\n",
    "print(f\"  Precision Test: {test_precision:.4f}\")\n",
    "print(f\"  Recall Test: {test_recall:.4f}\")\n",
    "print(f\"  F1 Score Test: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer.target_names, \n",
    "            yticklabels=cancer.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matrice di Confusione - SVM')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creazione e addestramento del modello\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)  # Nota: non è necessario scalare i dati per Random Forest\n",
    "\n",
    "# Predizioni\n",
    "y_pred_train = rf_classifier.predict(X_train)\n",
    "y_pred_test = rf_classifier.predict(X_test)\n",
    "\n",
    "# Valutazione\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test)\n",
    "test_recall = recall_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Random Forest Classifier:\")\n",
    "print(f\"  Accuracy Train: {train_acc:.4f}\")\n",
    "print(f\"  Accuracy Test: {test_acc:.4f}\")\n",
    "print(f\"  Precision Test: {test_precision:.4f}\")\n",
    "print(f\"  Recall Test: {test_recall:.4f}\")\n",
    "print(f\"  F1 Score Test: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la matrice di confusione\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer.target_names, \n",
    "            yticklabels=cancer.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Matrice di Confusione - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo l'importanza delle feature\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_classifier.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Random Forest: Top 15 Feature per Importanza')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Confronto tra modelli di classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confrontiamo le performance dei diversi modelli\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Per Random Forest non è necessario scalare i dati\n",
    "    if name == 'Random Forest':\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_precision = precision_score(y_test, y_pred_test)\n",
    "    test_recall = recall_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Precision': test_precision,\n",
    "        'Recall': test_recall,\n",
    "        'F1 Score': test_f1\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(results_df['Model'], results_df['Train Accuracy'], alpha=0.7, label='Train')\n",
    "plt.bar(results_df['Model'], results_df['Test Accuracy'], alpha=0.7, label='Test')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per modello')\n",
    "plt.ylim(0.9, 1.0)  # Zoom per vedere meglio le differenze\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Precision, Recall, F1\n",
    "plt.subplot(1, 2, 2)\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, model in enumerate(results_df['Model']):\n",
    "    values = [results_df.loc[i, metric] for metric in metrics]\n",
    "    plt.bar(np.arange(len(metrics)) + i*0.25, values, width=0.25, label=model)\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metriche di performance per modello')\n",
    "plt.xticks(np.arange(len(metrics)) + 0.25, metrics)\n",
    "plt.ylim(0.9, 1.0)  # Zoom per vedere meglio le differenze\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo le curve ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'Random Forest':\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_scores = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: Addestramento e Valutazione del Modello\n",
    "\n",
    "In questa seconda parte, esploreremo diverse tecniche di addestramento e valutazione dei modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cross-Validation\n",
    "\n",
    "La cross-validation è una tecnica per valutare i modelli utilizzando diversi sottoinsiemi dei dati di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "# Definiamo i modelli da valutare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Utilizziamo StratifiedKFold per mantenere la distribuzione delle classi\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Valutiamo i modelli con cross-validation\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'Random Forest':\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    else:\n",
    "        # Creiamo una pipeline che include lo scaling\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name}: {scores.mean():.4f} (±{scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati della cross-validation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([cv_results[name] for name in models.keys()], labels=models.keys())\n",
    "plt.title('Cross-Validation Results')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Curve di Apprendimento\n",
    "\n",
    "Le curve di apprendimento mostrano come variano le performance del modello al variare della dimensione del training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Definiamo una funzione per visualizzare le curve di apprendimento\n",
    "def plot_learning_curve(estimator, X, y, title, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo le curve di apprendimento per i diversi modelli\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == 'Random Forest':\n",
    "        plot_learning_curve(model, X, y, f'Learning Curve - {name}', ylim=(0.8, 1.01), cv=cv)\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        plot_learning_curve(pipeline, X, y, f'Learning Curve - {name}', ylim=(0.8, 1.01), cv=cv)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Ottimizzazione degli Iperparametri\n",
    "\n",
    "L'ottimizzazione degli iperparametri è il processo di ricerca dei valori ottimali per gli iperparametri di un modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definiamo gli spazi di ricerca per gli iperparametri\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "        'kernel': ['rbf']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Eseguiamo la grid search per il modello Random Forest\n",
    "model_name = 'Random Forest'\n",
    "model = models[model_name]\n",
    "param_grid = param_grids[model_name]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati della grid search\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Estraiamo i parametri e i punteggi\n",
    "params = results['params']\n",
    "scores = results['mean_test_score']\n",
    "\n",
    "# Creiamo un DataFrame per visualizzare i risultati\n",
    "results_df = pd.DataFrame({\n",
    "    'n_estimators': [p['n_estimators'] for p in params],\n",
    "    'max_depth': [str(p['max_depth']) for p in params],  # Convertiamo None in stringa\n",
    "    'min_samples_split': [p['min_samples_split'] for p in params],\n",
    "    'score': scores\n",
    "})\n",
    "\n",
    "# Visualizziamo i risultati per n_estimators e max_depth\n",
    "pivot_table = results_df.pivot_table(\n",
    "    values='score', \n",
    "    index='n_estimators', \n",
    "    columns='max_depth',\n",
    "    aggfunc=np.mean\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='viridis', fmt='.4f')\n",
    "plt.title('Grid Search Results: n_estimators vs max_depth')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpretabilità del Modello\n",
    "\n",
    "L'interpretabilità del modello è importante per comprendere come il modello prende le sue decisioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizziamo il modello Random Forest ottimizzato\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Visualizziamo l'importanza delle feature\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_rf.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('Random Forest: Top 15 Feature per Importanza')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo le predizioni per alcuni esempi\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calcoliamo l'importanza basata su permutazione\n",
    "result = permutation_importance(best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "perm_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result.importances_mean,\n",
    "    'Std': result.importances_std\n",
    "})\n",
    "perm_importance = perm_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=perm_importance.head(15), \n",
    "            xerr=perm_importance['Std'].head(15))\n",
    "plt.title('Permutation Importance: Top 15 Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Overfitting e Regolarizzazione\n",
    "\n",
    "In questa terza parte, esploreremo il problema dell'overfitting e le tecniche di regolarizzazione per prevenirlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dimostrazione dell'Overfitting\n",
    "\n",
    "Creiamo un esempio per dimostrare l'overfitting utilizzando un modello di regressione polinomiale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generiamo dati sintetici\n",
    "np.random.seed(42)\n",
    "X_synth = np.sort(np.random.uniform(0, 1, 30))[:, np.newaxis]\n",
    "y_synth = np.sin(2 * np.pi * X_synth).ravel() + np.random.normal(0, 0.1, X_synth.shape[0])\n",
    "\n",
    "# Suddividiamo in training e test\n",
    "X_train_synth, X_test_synth, y_train_synth, y_test_synth = train_test_split(\n",
    "    X_synth, y_synth, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creiamo modelli polinomiali di diversi gradi\n",
    "degrees = [1, 3, 5, 9, 15]\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Creiamo il modello polinomiale\n",
    "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "        (\"polynomial_features\", polynomial_features),\n",
    "        (\"linear_regression\", linear_regression)\n",
    "    ])\n",
    "    \n",
    "    # Addestriamo il modello\n",
    "    pipeline.fit(X_train_synth, y_train_synth)\n",
    "    \n",
    "    # Predizioni\n",
    "    X_test_synth_sorted = np.sort(X_test_synth, axis=0)\n",
    "    y_pred = pipeline.predict(X_test_synth_sorted)\n",
    "    \n",
    "    # Calcoliamo gli errori\n",
    "    train_error = mean_squared_error(y_train_synth, pipeline.predict(X_train_synth))\n",
    "    test_error = mean_squared_error(y_test_synth, pipeline.predict(X_test_synth))\n",
    "    \n",
    "    # Visualizziamo i risultati\n",
    "    plt.scatter(X_train_synth, y_train_synth, color='red', s=20, label='Training points')\n",
    "    plt.scatter(X_test_synth, y_test_synth, color='green', s=20, label='Test points')\n",
    "    plt.plot(X_test_synth_sorted, y_pred, color='blue', label='Prediction')\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.title(f\"Degree {degree}\\nTrain MSE: {train_error:.4f}, Test MSE: {test_error:.4f}\")\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Regolarizzazione L1 e L2\n",
    "\n",
    "Esploriamo l'effetto della regolarizzazione L1 (Lasso) e L2 (Ridge) sui coefficienti del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Creiamo feature polinomiali di grado 9\n",
    "poly = PolynomialFeatures(degree=9, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_synth)\n",
    "X_test_poly = poly.transform(X_test_synth)\n",
    "\n",
    "# Range di valori per alpha\n",
    "alphas = [0, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "# Visualizziamo l'effetto di alpha sui coefficienti e sulle predizioni\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    # Ridge\n",
    "    ax1 = plt.subplot(2, len(alphas), i+1)\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_poly, y_train_synth)\n",
    "    \n",
    "    # Predizioni\n",
    "    X_plot = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = ridge.predict(X_plot_poly)\n",
    "    \n",
    "    # Errori\n",
    "    train_error = mean_squared_error(y_train_synth, ridge.predict(X_train_poly))\n",
    "    test_error = mean_squared_error(y_test_synth, ridge.predict(X_test_poly))\n",
    "    \n",
    "    # Visualizziamo i risultati\n",
    "    plt.scatter(X_train_synth, y_train_synth, color='red', s=20)\n",
    "    plt.scatter(X_test_synth, y_test_synth, color='green', s=20)\n",
    "    plt.plot(X_plot, y_plot, color='blue')\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.title(f\"Ridge, alpha={alpha}\\nTrain MSE: {train_error:.4f}, Test MSE: {test_error:.4f}\")\n",
    "    \n",
    "    # Lasso\n",
    "    ax2 = plt.subplot(2, len(alphas), i+1+len(alphas))\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000, tol=0.001)\n",
    "    lasso.fit(X_train_poly, y_train_synth)\n",
    "    \n",
    "    # Predizioni\n",
    "    y_plot = lasso.predict(X_plot_poly)\n",
    "    \n",
    "    # Errori\n",
    "    train_error = mean_squared_error(y_train_synth, lasso.predict(X_train_poly))\n",
    "    test_error = mean_squared_error(y_test_synth, lasso.predict(X_test_poly))\n",
    "    \n",
    "    # Visualizziamo i risultati\n",
    "    plt.scatter(X_train_synth, y_train_synth, color='red', s=20)\n",
    "    plt.scatter(X_test_synth, y_test_synth, color='green', s=20)\n",
    "    plt.plot(X_plot, y_plot, color='blue')\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.title(f\"Lasso, alpha={alpha}\\nTrain MSE: {train_error:.4f}, Test MSE: {test_error:.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i coefficienti per diversi valori di alpha\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Ridge\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "coefs = []\n",
    "alphas_to_plot = np.logspace(-3, 3, 7)\n",
    "\n",
    "for alpha in alphas_to_plot:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_poly, y_train_synth)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "ax1.plot(np.log10(alphas_to_plot), coefs)\n",
    "ax1.set_xlabel('log(alpha)')\n",
    "ax1.set_ylabel('Coefficients')\n",
    "ax1.set_title('Ridge coefficients as a function of regularization')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Lasso\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas_to_plot:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000, tol=0.001)\n",
    "    lasso.fit(X_train_poly, y_train_synth)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "ax2.plot(np.log10(alphas_to_plot), coefs)\n",
    "ax2.set_xlabel('log(alpha)')\n",
    "ax2.set_ylabel('Coefficients')\n",
    "ax2.set_title('Lasso coefficients as a function of regularization')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Regolarizzazione nei Modelli di Classificazione\n",
    "\n",
    "Esploriamo l'effetto della regolarizzazione nei modelli di classificazione, utilizzando il dataset Breast Cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizziamo il dataset Breast Cancer\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Caricamento del dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Suddivisione in training e test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardizzazione delle feature\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Valori di C da testare (C è l'inverso della regolarizzazione)\n",
    "C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "# Testiamo diversi valori di C per la regressione logistica\n",
    "results = []\n",
    "\n",
    "for C in C_values:\n",
    "    # L1 regularization\n",
    "    log_reg_l1 = LogisticRegression(C=C, penalty='l1', solver='liblinear', random_state=42)\n",
    "    log_reg_l1.fit(X_train_scaled, y_train)\n",
    "    y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "    accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "    n_nonzero_l1 = np.sum(log_reg_l1.coef_ != 0)\n",
    "    \n",
    "    # L2 regularization\n",
    "    log_reg_l2 = LogisticRegression(C=C, penalty='l2', solver='liblinear', random_state=42)\n",
    "    log_reg_l2.fit(X_train_scaled, y_train)\n",
    "    y_pred_l2 = log_reg_l2.predict(X_test_scaled)\n",
    "    accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "    n_nonzero_l2 = np.sum(log_reg_l2.coef_ != 0)\n",
    "    \n",
    "    results.append({\n",
    "        'C': C,\n",
    "        'L1 Accuracy': accuracy_l1,\n",
    "        'L2 Accuracy': accuracy_l2,\n",
    "        'L1 Non-zero Coefficients': n_nonzero_l1,\n",
    "        'L2 Non-zero Coefficients': n_nonzero_l2\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.log10(results_df['C']), results_df['L1 Accuracy'], 'o-', label='L1')\n",
    "plt.plot(np.log10(results_df['C']), results_df['L2 Accuracy'], 'o-', label='L2')\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Regularization Strength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Non-zero coefficients\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.log10(results_df['C']), results_df['L1 Non-zero Coefficients'], 'o-', label='L1')\n",
    "plt.plot(np.log10(results_df['C']), results_df['L2 Non-zero Coefficients'], 'o-', label='L2')\n",
    "plt.xlabel('log10(C)')\n",
    "plt.ylabel('Number of non-zero coefficients')\n",
    "plt.title('Sparsity vs Regularization Strength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i coefficienti per diversi valori di C\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Selezioniamo alcuni valori di C\n",
    "C_to_plot = [0.001, 0.1, 10.0]\n",
    "\n",
    "for i, C in enumerate(C_to_plot):\n",
    "    # L1\n",
    "    ax1 = plt.subplot(2, len(C_to_plot), i+1)\n",
    "    log_reg_l1 = LogisticRegression(C=C, penalty='l1', solver='liblinear', random_state=42)\n",
    "    log_reg_l1.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    coef = pd.DataFrame({\n",
    "        'Feature': cancer.feature_names,\n",
    "        'Coefficient': log_reg_l1.coef_[0]\n",
    "    })\n",
    "    coef = coef.sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef.head(10))\n",
    "    plt.title(f'L1, C={C}\\nTop 10 Coefficients')\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "    \n",
    "    # L2\n",
    "    ax2 = plt.subplot(2, len(C_to_plot), i+1+len(C_to_plot))\n",
    "    log_reg_l2 = LogisticRegression(C=C, penalty='l2', solver='liblinear', random_state=42)\n",
    "    log_reg_l2.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    coef = pd.DataFrame({\n",
    "        'Feature': cancer.feature_names,\n",
    "        'Coefficient': log_reg_l2.coef_[0]\n",
    "    })\n",
    "    coef = coef.sort_values('Coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coef.head(10))\n",
    "    plt.title(f'L2, C={C}\\nTop 10 Coefficients')\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Regolarizzazione nei Modelli ad Albero\n",
    "\n",
    "Esploriamo l'effetto della regolarizzazione nei modelli ad albero, come Random Forest e Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Utilizziamo il dataset Breast Cancer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Testiamo diversi valori di max_depth per l'albero di decisione\n",
    "max_depths = [1, 2, 3, 5, 10, None]\n",
    "results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred_train = dt.predict(X_train)\n",
    "    y_pred_test = dt.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'max_depth': str(max_depth),  # Convertiamo None in stringa\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Gap': train_acc - test_acc\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(results_df['max_depth'], results_df['Train Accuracy'], 'o-', label='Train')\n",
    "plt.plot(results_df['max_depth'], results_df['Test Accuracy'], 'o-', label='Test')\n",
    "plt.fill_between(results_df['max_depth'], \n",
    "                 results_df['Train Accuracy'], \n",
    "                 results_df['Test Accuracy'], \n",
    "                 alpha=0.2, color='red', label='Gap')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Decision Tree: Accuracy vs max_depth')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confrontiamo Random Forest con diversi valori di n_estimators\n",
    "n_estimators_values = [1, 5, 10, 50, 100, 200]\n",
    "results = []\n",
    "\n",
    "for n_estimators in n_estimators_values:\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_train = rf.predict(X_train)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'n_estimators': n_estimators,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Gap': train_acc - test_acc\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(results_df['n_estimators'], results_df['Train Accuracy'], 'o-', label='Train')\n",
    "plt.plot(results_df['n_estimators'], results_df['Test Accuracy'], 'o-', label='Test')\n",
    "plt.fill_between(results_df['n_estimators'], \n",
    "                 results_df['Train Accuracy'], \n",
    "                 results_df['Test Accuracy'], \n",
    "                 alpha=0.2, color='red', label='Gap')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Random Forest: Accuracy vs n_estimators')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confrontiamo Gradient Boosting con diversi valori di learning_rate\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Gradient Boosting\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=lr, random_state=42)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred_train = gb.predict(X_train)\n",
    "    y_pred_test = gb.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Gap': train_acc - test_acc\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(results_df['learning_rate'], results_df['Train Accuracy'], 'o-', label='Train')\n",
    "plt.plot(results_df['learning_rate'], results_df['Test Accuracy'], 'o-', label='Test')\n",
    "plt.fill_between(results_df['learning_rate'], \n",
    "                 results_df['Train Accuracy'], \n",
    "                 results_df['Test Accuracy'], \n",
    "                 alpha=0.2, color='red', label='Gap')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Gradient Boosting: Accuracy vs learning_rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Altre Tecniche per Prevenire l'Overfitting\n",
    "\n",
    "Esploriamo altre tecniche per prevenire l'overfitting, come la feature selection e la dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Utilizziamo il dataset Breast Cancer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Standardizziamo le feature\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature Selection con SelectKBest\n",
    "k_values = [5, 10, 15, 20, 25, 30]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Selezioniamo le k feature migliori\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "    \n",
    "    # Addestriamo un modello\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Valutiamo il modello\n",
    "    y_pred_train = model.predict(X_train_selected)\n",
    "    y_pred_test = model.predict(X_test_selected)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'Method': 'SelectKBest',\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc\n",
    "    })\n",
    "\n",
    "# PCA\n",
    "for k in k_values:\n",
    "    # Riduciamo la dimensionalità con PCA\n",
    "    pca = PCA(n_components=k)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # Addestriamo un modello\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Valutiamo il modello\n",
    "    y_pred_train = model.predict(X_train_pca)\n",
    "    y_pred_test = model.predict(X_test_pca)\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'Method': 'PCA',\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc\n",
    "    })\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# SelectKBest\n",
    "plt.subplot(1, 2, 1)\n",
    "select_results = results_df[results_df['Method'] == 'SelectKBest']\n",
    "plt.plot(select_results['k'], select_results['Train Accuracy'], 'o-', label='Train')\n",
    "plt.plot(select_results['k'], select_results['Test Accuracy'], 'o-', label='Test')\n",
    "plt.xlabel('Number of features (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('SelectKBest: Accuracy vs Number of Features')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# PCA\n",
    "plt.subplot(1, 2, 2)\n",
    "pca_results = results_df[results_df['Method'] == 'PCA']\n",
    "plt.plot(pca_results['k'], pca_results['Train Accuracy'], 'o-', label='Train')\n",
    "plt.plot(pca_results['k'], pca_results['Test Accuracy'], 'o-', label='Test')\n",
    "plt.xlabel('Number of components (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('PCA: Accuracy vs Number of Components')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusioni\n",
    "\n",
    "In questo notebook, abbiamo esplorato i tre temi principali dell'apprendimento supervisionato:\n",
    "\n",
    "1. **Algoritmi di regressione e classificazione**:\n",
    "   - Abbiamo implementato e confrontato diversi algoritmi di regressione (Linear, Ridge, Lasso, Random Forest) sul dataset California Housing.\n",
    "   - Abbiamo implementato e confrontato diversi algoritmi di classificazione (Logistic Regression, SVM, Random Forest) sul dataset Breast Cancer.\n",
    "\n",
    "2. **Addestramento e valutazione del modello**:\n",
    "   - Abbiamo utilizzato tecniche di cross-validation per valutare i modelli in modo robusto.\n",
    "   - Abbiamo visualizzato curve di apprendimento per comprendere come le performance dei modelli variano al variare della dimensione del training set.\n",
    "   - Abbiamo ottimizzato gli iperparametri dei modelli utilizzando grid search.\n",
    "   - Abbiamo esplorato tecniche di interpretabilità del modello, come l'importanza delle feature.\n",
    "\n",
    "3. **Overfitting e regolarizzazione**:\n",
    "   - Abbiamo dimostrato l'overfitting utilizzando un modello di regressione polinomiale.\n",
    "   - Abbiamo esplorato l'effetto della regolarizzazione L1 (Lasso) e L2 (Ridge) sui coefficienti del modello.\n",
    "   - Abbiamo applicato tecniche di regolarizzazione nei modelli di classificazione.\n",
    "   - Abbiamo esplorato l'effetto della regolarizzazione nei modelli ad albero.\n",
    "   - Abbiamo utilizzato altre tecniche per prevenire l'overfitting, come la feature selection e la dimensionality reduction.\n",
    "\n",
    "Questi concetti e tecniche sono fondamentali per sviluppare modelli di machine learning efficaci e robusti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizi Proposti\n",
    "\n",
    "1. **Regressione**:\n",
    "   - Implementa un modello di Support Vector Regression (SVR) sul dataset California Housing e confrontalo con gli altri modelli di regressione.\n",
    "   - Prova a utilizzare la regressione polinomiale con diversi gradi e confronta i risultati.\n",
    "\n",
    "2. **Classificazione**:\n",
    "   - Implementa un modello di Naive Bayes sul dataset Breast Cancer e confrontalo con gli altri modelli di classificazione.\n",
    "   - Prova a utilizzare un modello di Gradient Boosting e ottimizza i suoi iperparametri.\n",
    "\n",
    "3. **Addestramento e valutazione**:\n",
    "   - Implementa la nested cross-validation per ottenere una stima più robusta delle performance del modello.\n",
    "   - Utilizza diverse metriche di valutazione (precision, recall, F1-score) e confronta i risultati.\n",
    "\n",
    "4. **Overfitting e regolarizzazione**:\n",
    "   - Implementa l'Elastic Net e confrontalo con Ridge e Lasso.\n",
    "   - Prova a utilizzare la tecnica di early stopping per prevenire l'overfitting.\n",
    "\n",
    "5. **Dataset personalizzato**:\n",
    "   - Applica le tecniche apprese in questo notebook a un dataset di tua scelta.\n",
    "   - Confronta diversi algoritmi e tecniche di regolarizzazione sul tuo dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
