{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione Pratica: Algoritmi di Clustering\n",
    "\n",
    "## Corso di Machine Learning: Apprendimento Non Supervisionato\n",
    "\n",
    "Questa esercitazione pratica ti guiderà attraverso l'implementazione e l'applicazione dei principali algoritmi di clustering discussi nelle lezioni teoriche. Esploreremo diversi dataset e vedremo come applicare e valutare vari metodi di clustering.\n",
    "\n",
    "### Obiettivi dell'esercitazione:\n",
    "- Implementare e applicare K-means, Clustering Gerarchico e DBSCAN\n",
    "- Visualizzare i risultati del clustering\n",
    "- Valutare la qualità dei cluster ottenuti\n",
    "- Confrontare le prestazioni dei diversi algoritmi\n",
    "- Applicare il clustering a un caso di studio reale: segmentazione dei clienti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione dell'ambiente\n",
    "\n",
    "Iniziamo importando le librerie necessarie per questa esercitazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importazione delle librerie necessarie\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Impostazioni di visualizzazione\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Generazione e Visualizzazione dei Dataset\n",
    "\n",
    "Prima di applicare gli algoritmi di clustering, generiamo alcuni dataset sintetici con diverse strutture per testare i vari algoritmi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Funzione per visualizzare i dataset\n",
    "def plot_dataset(X, y=None, title=\"Dataset\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if y is not None:\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', s=50, alpha=0.8)\n",
    "    else:\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=50, alpha=0.8)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.colorbar(label='Cluster' if y is not None else None)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Dataset con cluster ben separati (ideale per K-means)\n",
    "X_blobs, y_blobs = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "plot_dataset(X_blobs, y_blobs, \"Dataset 1: Cluster ben separati\")\n",
    "\n",
    "# 2. Dataset con cluster non sferici (sfida per K-means)\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.08, random_state=42)\n",
    "plot_dataset(X_moons, y_moons, \"Dataset 2: Cluster a forma di mezzaluna\")\n",
    "\n",
    "# 3. Dataset con cluster concentrici (sfida per molti algoritmi)\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
    "plot_dataset(X_circles, y_circles, \"Dataset 3: Cluster concentrici\")\n",
    "\n",
    "# 4. Dataset con densità variabile (ideale per DBSCAN)\n",
    "X_varied = np.vstack([\n",
    "    np.random.normal(0, 0.5, (100, 2)),  # Cluster denso\n",
    "    np.random.normal(5, 2, (200, 2))     # Cluster sparso\n",
    "])\n",
    "y_varied = np.hstack([np.zeros(100), np.ones(200)])\n",
    "plot_dataset(X_varied, y_varied, \"Dataset 4: Cluster con densità variabile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: K-means Clustering\n",
    "\n",
    "Implementiamo e applichiamo l'algoritmo K-means ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Determinare il numero ottimale di cluster con il metodo del gomito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_elbow_method(X, max_k=10):\n",
    "    distortions = []\n",
    "    silhouette_scores = []\n",
    "    K = range(2, max_k+1)\n",
    "    \n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calcolo del silhouette score\n",
    "        if k > 1:  # Silhouette score richiede almeno 2 cluster\n",
    "            labels = kmeans.labels_\n",
    "            silhouette_scores.append(silhouette_score(X, labels))\n",
    "    \n",
    "    # Plot del metodo del gomito\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(K, distortions, 'bo-')\n",
    "    plt.xlabel('Numero di cluster (k)')\n",
    "    plt.ylabel('Distorsione (Inertia)')\n",
    "    plt.title('Metodo del gomito')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(K[1:], silhouette_scores, 'ro-')\n",
    "    plt.xlabel('Numero di cluster (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score per diversi valori di k')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return distortions, silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare il metodo del gomito al dataset con cluster ben separati\n",
    "distortions_blobs, silhouette_blobs = plot_elbow_method(X_blobs, max_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implementazione di K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_kmeans(X, n_clusters=4, title=\"K-means Clustering\"):\n",
    "    # Applicare K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    y_pred = kmeans.fit_predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.8)\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, alpha=1, label='Centroidi')\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcolare metriche di valutazione\n",
    "    if n_clusters > 1:  # Silhouette score richiede almeno 2 cluster\n",
    "        sil_score = silhouette_score(X, y_pred)\n",
    "        ch_score = calinski_harabasz_score(X, y_pred)\n",
    "        db_score = davies_bouldin_score(X, y_pred)\n",
    "        \n",
    "        print(f\"Silhouette Score: {sil_score:.3f} (più alto è meglio)\")\n",
    "        print(f\"Calinski-Harabasz Index: {ch_score:.3f} (più alto è meglio)\")\n",
    "        print(f\"Davies-Bouldin Index: {db_score:.3f} (più basso è meglio)\")\n",
    "    \n",
    "    return y_pred, centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare K-means al dataset con cluster ben separati\n",
    "y_pred_blobs, centers_blobs = apply_kmeans(X_blobs, n_clusters=4, title=\"K-means su cluster ben separati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare K-means al dataset con cluster a forma di mezzaluna\n",
    "y_pred_moons, centers_moons = apply_kmeans(X_moons, n_clusters=2, title=\"K-means su cluster a forma di mezzaluna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare K-means al dataset con cluster concentrici\n",
    "y_pred_circles, centers_circles = apply_kmeans(X_circles, n_clusters=2, title=\"K-means su cluster concentrici\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 K-means++: Miglioramento dell'inizializzazione\n",
    "\n",
    "K-means++ è l'algoritmo di inizializzazione predefinito in scikit-learn. Vediamo come funziona e confrontiamolo con l'inizializzazione casuale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_kmeans_init(X, n_clusters=4):\n",
    "    # K-means con inizializzazione casuale\n",
    "    kmeans_random = KMeans(n_clusters=n_clusters, init='random', random_state=42, n_init=10)\n",
    "    y_pred_random = kmeans_random.fit_predict(X)\n",
    "    inertia_random = kmeans_random.inertia_\n",
    "    \n",
    "    # K-means con inizializzazione k-means++\n",
    "    kmeans_plus = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42, n_init=10)\n",
    "    y_pred_plus = kmeans_plus.fit_predict(X)\n",
    "    inertia_plus = kmeans_plus.inertia_\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred_random, cmap='viridis', s=50, alpha=0.8)\n",
    "    plt.scatter(kmeans_random.cluster_centers_[:, 0], kmeans_random.cluster_centers_[:, 1], \n",
    "                c='red', marker='X', s=200, alpha=1, label='Centroidi')\n",
    "    plt.title(f\"K-means con inizializzazione casuale\\nInertia: {inertia_random:.2f}\", fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred_plus, cmap='viridis', s=50, alpha=0.8)\n",
    "    plt.scatter(kmeans_plus.cluster_centers_[:, 0], kmeans_plus.cluster_centers_[:, 1], \n",
    "                c='red', marker='X', s=200, alpha=1, label='Centroidi')\n",
    "    plt.title(f\"K-means con inizializzazione k-means++\\nInertia: {inertia_plus:.2f}\", fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Confrontare le metriche\n",
    "    sil_random = silhouette_score(X, y_pred_random)\n",
    "    sil_plus = silhouette_score(X, y_pred_plus)\n",
    "    \n",
    "    print(f\"Inizializzazione casuale - Inertia: {inertia_random:.2f}, Silhouette: {sil_random:.3f}\")\n",
    "    print(f\"Inizializzazione k-means++ - Inertia: {inertia_plus:.2f}, Silhouette: {sil_plus:.3f}\")\n",
    "    \n",
    "    return kmeans_random, kmeans_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare le inizializzazioni sul dataset con cluster ben separati\n",
    "kmeans_random, kmeans_plus = compare_kmeans_init(X_blobs, n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Clustering Gerarchico\n",
    "\n",
    "Implementiamo e applichiamo il clustering gerarchico ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualizzazione del dendrogramma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_dendrogram(X, max_samples=100, title=\"Dendrogramma del Clustering Gerarchico\"):\n",
    "    # Se il dataset è troppo grande, prendiamo un campione\n",
    "    if X.shape[0] > max_samples:\n",
    "        indices = np.random.choice(X.shape[0], max_samples, replace=False)\n",
    "        X_sample = X[indices]\n",
    "    else:\n",
    "        X_sample = X\n",
    "    \n",
    "    # Calcolare la matrice di linkage\n",
    "    linked = linkage(X_sample, method='ward')\n",
    "    \n",
    "    # Visualizzare il dendrogramma\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Campioni', fontsize=12)\n",
    "    plt.ylabel('Distanza', fontsize=12)\n",
    "    plt.axhline(y=15, c='k', linestyle='--', alpha=0.5)\n",
    "    plt.text(X_sample.shape[0]/2, 16, 'Soglia di taglio suggerita', ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare il dendrogramma per il dataset con cluster ben separati\n",
    "plot_dendrogram(X_blobs, title=\"Dendrogramma per cluster ben separati\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementazione del Clustering Gerarchico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_hierarchical(X, n_clusters=4, linkage='ward', title=\"Clustering Gerarchico\"):\n",
    "    # Applicare il clustering gerarchico\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
    "    y_pred = hierarchical.fit_predict(X)\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.8)\n",
    "    plt.title(f\"{title} (linkage={linkage})\", fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcolare metriche di valutazione\n",
    "    if n_clusters > 1:  # Silhouette score richiede almeno 2 cluster\n",
    "        sil_score = silhouette_score(X, y_pred)\n",
    "        ch_score = calinski_harabasz_score(X, y_pred)\n",
    "        db_score = davies_bouldin_score(X, y_pred)\n",
    "        \n",
    "        print(f\"Silhouette Score: {sil_score:.3f} (più alto è meglio)\")\n",
    "        print(f\"Calinski-Harabasz Index: {ch_score:.3f} (più alto è meglio)\")\n",
    "        print(f\"Davies-Bouldin Index: {db_score:.3f} (più basso è meglio)\")\n",
    "    \n",
    "    return y_pred, hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare il clustering gerarchico al dataset con cluster ben separati\n",
    "y_pred_hier_blobs, hier_blobs = apply_hierarchical(X_blobs, n_clusters=4, title=\"Clustering Gerarchico su cluster ben separati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare il clustering gerarchico al dataset con cluster a forma di mezzaluna\n",
    "y_pred_hier_moons, hier_moons = apply_hierarchical(X_moons, n_clusters=2, title=\"Clustering Gerarchico su cluster a forma di mezzaluna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Confronto tra diversi metodi di linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_linkage_methods(X, n_clusters=4):\n",
    "    linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, method in enumerate(linkage_methods):\n",
    "        # Applicare il clustering gerarchico\n",
    "        hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage=method)\n",
    "        y_pred = hierarchical.fit_predict(X)\n",
    "        \n",
    "        # Calcolare silhouette score\n",
    "        sil_score = silhouette_score(X, y_pred)\n",
    "        \n",
    "        # Visualizzare i risultati\n",
    "        axes[i].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.8)\n",
    "        axes[i].set_title(f\"Linkage: {method}\\nSilhouette: {sil_score:.3f}\", fontsize=14)\n",
    "        axes[i].set_xlabel('Feature 1', fontsize=12)\n",
    "        axes[i].set_ylabel('Feature 2', fontsize=12)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare i metodi di linkage sul dataset con cluster ben separati\n",
    "compare_linkage_methods(X_blobs, n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare i metodi di linkage sul dataset con cluster a forma di mezzaluna\n",
    "compare_linkage_methods(X_moons, n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "Implementiamo e applichiamo DBSCAN ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Determinare i parametri ottimali per DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_optimal_eps(X, min_samples=5, k=5):\n",
    "    # Calcolare le distanze ai k vicini più prossimi\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    distances, indices = neigh.kneighbors(X)\n",
    "    \n",
    "    # Ordinare le distanze in ordine crescente\n",
    "    distances = np.sort(distances[:, k-1])\n",
    "    \n",
    "    # Visualizzare il grafico delle distanze\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(distances)), distances, 'b-')\n",
    "    plt.xlabel('Punti ordinati per distanza')\n",
    "    plt.ylabel(f'Distanza al {k}-esimo vicino più prossimo')\n",
    "    plt.title('Grafico delle distanze per determinare eps ottimale')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calcolare la derivata per trovare il punto di massima curvatura\n",
    "    derivative = np.gradient(distances)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(derivative)), derivative, 'r-')\n",
    "    plt.xlabel('Punti ordinati per distanza')\n",
    "    plt.ylabel('Derivata della distanza')\n",
    "    plt.title('Derivata delle distanze per identificare il \"gomito\"')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trovare il punto di massima curvatura (approssimazione)\n",
    "    knee_point = np.argmax(derivative) if len(derivative) > 0 else 0\n",
    "    suggested_eps = distances[knee_point]\n",
    "    \n",
    "    print(f\"Valore di eps suggerito: {suggested_eps:.3f}\")\n",
    "    print(f\"Valore di min_samples suggerito: {min_samples}\")\n",
    "    \n",
    "    return suggested_eps, min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trovare i parametri ottimali per DBSCAN sul dataset con cluster a forma di mezzaluna\n",
    "eps_moons, min_samples_moons = find_optimal_eps(X_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implementazione di DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_dbscan(X, eps=0.5, min_samples=5, title=\"DBSCAN Clustering\"):\n",
    "    # Applicare DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    y_pred = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Contare il numero di cluster e punti di rumore\n",
    "    n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "    n_noise = list(y_pred).count(-1)\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Visualizzare i cluster\n",
    "    unique_labels = set(y_pred)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Punti di rumore in nero\n",
    "            col = 'k'\n",
    "        \n",
    "        class_member_mask = (y_pred == k)\n",
    "        xy = X[class_member_mask]\n",
    "        plt.scatter(xy[:, 0], xy[:, 1], s=50, c=[col], alpha=0.8, label=f\"Cluster {k}\" if k != -1 else \"Rumore\")\n",
    "    \n",
    "    plt.title(f\"{title}\\neps={eps:.3f}, min_samples={min_samples}\\nCluster trovati: {n_clusters}, Punti di rumore: {n_noise}\", fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcolare metriche di valutazione (solo se ci sono almeno 2 cluster e non tutti i punti sono rumore)\n",
    "    if n_clusters >= 2 and n_noise < len(X):\n",
    "        # Filtrare i punti di rumore per il calcolo delle metriche\n",
    "        X_filtered = X[y_pred != -1]\n",
    "        y_filtered = y_pred[y_pred != -1]\n",
    "        \n",
    "        if len(set(y_filtered)) >= 2:  # Almeno 2 cluster dopo il filtraggio\n",
    "            sil_score = silhouette_score(X_filtered, y_filtered)\n",
    "            ch_score = calinski_harabasz_score(X_filtered, y_filtered)\n",
    "            db_score = davies_bouldin_score(X_filtered, y_filtered)\n",
    "            \n",
    "            print(f\"Silhouette Score (esclusi punti di rumore): {sil_score:.3f} (più alto è meglio)\")\n",
    "            print(f\"Calinski-Harabasz Index (esclusi punti di rumore): {ch_score:.3f} (più alto è meglio)\")\n",
    "            print(f\"Davies-Bouldin Index (esclusi punti di rumore): {db_score:.3f} (più basso è meglio)\")\n",
    "    \n",
    "    return y_pred, dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare DBSCAN al dataset con cluster a forma di mezzaluna\n",
    "y_pred_dbscan_moons, dbscan_moons = apply_dbscan(X_moons, eps=eps_moons, min_samples=min_samples_moons, \n",
    "                                                title=\"DBSCAN su cluster a forma di mezzaluna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trovare i parametri ottimali per DBSCAN sul dataset con cluster concentrici\n",
    "eps_circles, min_samples_circles = find_optimal_eps(X_circles)\n",
    "\n",
    "# Applicare DBSCAN al dataset con cluster concentrici\n",
    "y_pred_dbscan_circles, dbscan_circles = apply_dbscan(X_circles, eps=eps_circles, min_samples=min_samples_circles, \n",
    "                                                    title=\"DBSCAN su cluster concentrici\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Effetto dei parametri di DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def explore_dbscan_parameters(X, eps_values=[0.1, 0.3, 0.5], min_samples_values=[2, 5, 10]):\n",
    "    fig, axes = plt.subplots(len(eps_values), len(min_samples_values), figsize=(15, 12))\n",
    "    \n",
    "    for i, eps in enumerate(eps_values):\n",
    "        for j, min_samples in enumerate(min_samples_values):\n",
    "            # Applicare DBSCAN\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            y_pred = dbscan.fit_predict(X)\n",
    "            \n",
    "            # Contare il numero di cluster e punti di rumore\n",
    "            n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "            n_noise = list(y_pred).count(-1)\n",
    "            \n",
    "            # Visualizzare i risultati\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Visualizzare i cluster\n",
    "            unique_labels = set(y_pred)\n",
    "            colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
    "            \n",
    "            for k, col in zip(unique_labels, colors):\n",
    "                if k == -1:\n",
    "                    # Punti di rumore in nero\n",
    "                    col = 'k'\n",
    "                \n",
    "                class_member_mask = (y_pred == k)\n",
    "                xy = X[class_member_mask]\n",
    "                ax.scatter(xy[:, 0], xy[:, 1], s=30, c=[col], alpha=0.8)\n",
    "            \n",
    "            ax.set_title(f\"eps={eps:.2f}, min_samples={min_samples}\\nCluster: {n_clusters}, Rumore: {n_noise}\", fontsize=10)\n",
    "            ax.set_xlabel('Feature 1', fontsize=8)\n",
    "            ax.set_ylabel('Feature 2', fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare l'effetto dei parametri di DBSCAN sul dataset con cluster a forma di mezzaluna\n",
    "explore_dbscan_parameters(X_moons, \n",
    "                          eps_values=[0.1, 0.2, 0.3], \n",
    "                          min_samples_values=[3, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Gaussian Mixture Models (GMM)\n",
    "\n",
    "Implementiamo e applichiamo i Gaussian Mixture Models ai nostri dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_gmm(X, n_components=4, title=\"Gaussian Mixture Model\"):\n",
    "    # Applicare GMM\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(X)\n",
    "    y_pred = gmm.predict(X)\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.8)\n",
    "    \n",
    "    # Visualizzare le ellissi di confidenza\n",
    "    for i in range(n_components):\n",
    "        if not np.any(y_pred == i):\n",
    "            continue\n",
    "            \n",
    "        mean = gmm.means_[i]\n",
    "        covar = gmm.covariances_[i]\n",
    "        v, w = np.linalg.eigh(covar)\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180. * angle / np.pi  # Convertire in gradi\n",
    "        \n",
    "        # Disegnare l'ellisse\n",
    "        ell = plt.matplotlib.patches.Ellipse(mean, v[0], v[1], 180. + angle, \n",
    "                                             edgecolor='black', facecolor='none', linewidth=2)\n",
    "        plt.gca().add_patch(ell)\n",
    "        plt.scatter(mean[0], mean[1], c='red', marker='X', s=200, alpha=1)\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcolare metriche di valutazione\n",
    "    if n_components > 1:  # Silhouette score richiede almeno 2 cluster\n",
    "        sil_score = silhouette_score(X, y_pred)\n",
    "        ch_score = calinski_harabasz_score(X, y_pred)\n",
    "        db_score = davies_bouldin_score(X, y_pred)\n",
    "        \n",
    "        print(f\"Silhouette Score: {sil_score:.3f} (più alto è meglio)\")\n",
    "        print(f\"Calinski-Harabasz Index: {ch_score:.3f} (più alto è meglio)\")\n",
    "        print(f\"Davies-Bouldin Index: {db_score:.3f} (più basso è meglio)\")\n",
    "        print(f\"Log-Likelihood: {gmm.score(X):.3f} (più alto è meglio)\")\n",
    "        print(f\"BIC: {gmm.bic(X):.3f} (più basso è meglio)\")\n",
    "        print(f\"AIC: {gmm.aic(X):.3f} (più basso è meglio)\")\n",
    "    \n",
    "    return y_pred, gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare GMM al dataset con cluster ben separati\n",
    "y_pred_gmm_blobs, gmm_blobs = apply_gmm(X_blobs, n_components=4, title=\"GMM su cluster ben separati\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare GMM al dataset con cluster a forma di mezzaluna\n",
    "y_pred_gmm_moons, gmm_moons = apply_gmm(X_moons, n_components=2, title=\"GMM su cluster a forma di mezzaluna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Determinare il numero ottimale di componenti per GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_optimal_components(X, max_components=10):\n",
    "    n_components_range = range(1, max_components + 1)\n",
    "    bic = []\n",
    "    aic = []\n",
    "    silhouette = []\n",
    "    \n",
    "    for n_components in n_components_range:\n",
    "        # Addestrare il GMM\n",
    "        gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "        gmm.fit(X)\n",
    "        \n",
    "        # Calcolare BIC e AIC\n",
    "        bic.append(gmm.bic(X))\n",
    "        aic.append(gmm.aic(X))\n",
    "        \n",
    "        # Calcolare silhouette score (solo per n_components >= 2)\n",
    "        if n_components >= 2:\n",
    "            y_pred = gmm.predict(X)\n",
    "            silhouette.append(silhouette_score(X, y_pred))\n",
    "        else:\n",
    "            silhouette.append(0)  # Placeholder per n_components=1\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_components_range, bic, 'o-', label='BIC')\n",
    "    plt.plot(n_components_range, aic, 's-', label='AIC')\n",
    "    plt.xlabel('Numero di componenti')\n",
    "    plt.ylabel('Punteggio')\n",
    "    plt.title('BIC e AIC per diversi numeri di componenti')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_components_range[1:], silhouette[1:], 'o-')\n",
    "    plt.xlabel('Numero di componenti')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score per diversi numeri di componenti')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Trovare il numero ottimale di componenti\n",
    "    optimal_bic = n_components_range[np.argmin(bic)]\n",
    "    optimal_aic = n_components_range[np.argmin(aic)]\n",
    "    optimal_silhouette = n_components_range[1:][np.argmax(silhouette[1:])]\n",
    "    \n",
    "    print(f\"Numero ottimale di componenti secondo BIC: {optimal_bic}\")\n",
    "    print(f\"Numero ottimale di componenti secondo AIC: {optimal_aic}\")\n",
    "    print(f\"Numero ottimale di componenti secondo Silhouette: {optimal_silhouette}\")\n",
    "    \n",
    "    return optimal_bic, optimal_aic, optimal_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trovare il numero ottimale di componenti per il dataset con cluster ben separati\n",
    "optimal_bic, optimal_aic, optimal_silhouette = find_optimal_components(X_blobs, max_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Confronto tra Algoritmi di Clustering\n",
    "\n",
    "Confrontiamo le prestazioni dei diversi algoritmi di clustering sui nostri dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_clustering_algorithms(X, true_labels=None, n_clusters=4):\n",
    "    # Applicare i diversi algoritmi\n",
    "    algorithms = {\n",
    "        'K-means': KMeans(n_clusters=n_clusters, random_state=42, n_init=10),\n",
    "        'Hierarchical (Ward)': AgglomerativeClustering(n_clusters=n_clusters, linkage='ward'),\n",
    "        'DBSCAN': DBSCAN(eps=0.3, min_samples=5),\n",
    "        'GMM': GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, (name, algorithm) in enumerate(algorithms.items()):\n",
    "        # Addestrare l'algoritmo\n",
    "        if name == 'GMM':\n",
    "            algorithm.fit(X)\n",
    "            y_pred = algorithm.predict(X)\n",
    "        else:\n",
    "            y_pred = algorithm.fit_predict(X)\n",
    "        \n",
    "        # Gestire il caso di DBSCAN che può avere etichette -1 (rumore)\n",
    "        if name == 'DBSCAN':\n",
    "            n_clusters_found = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "            n_noise = list(y_pred).count(-1)\n",
    "            title = f\"{name}\\nCluster trovati: {n_clusters_found}, Rumore: {n_noise}\"\n",
    "        else:\n",
    "            title = name\n",
    "        \n",
    "        # Visualizzare i risultati\n",
    "        axes[i].scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.8)\n",
    "        axes[i].set_title(title, fontsize=14)\n",
    "        axes[i].set_xlabel('Feature 1', fontsize=12)\n",
    "        axes[i].set_ylabel('Feature 2', fontsize=12)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calcolare metriche di valutazione\n",
    "        metrics = {}\n",
    "        \n",
    "        # Per DBSCAN, calcolare le metriche solo sui punti non di rumore\n",
    "        if name == 'DBSCAN' and -1 in y_pred:\n",
    "            X_filtered = X[y_pred != -1]\n",
    "            y_filtered = y_pred[y_pred != -1]\n",
    "            \n",
    "            if len(set(y_filtered)) >= 2 and len(y_filtered) > 0:  # Almeno 2 cluster dopo il filtraggio\n",
    "                metrics['silhouette'] = silhouette_score(X_filtered, y_filtered)\n",
    "                metrics['calinski_harabasz'] = calinski_harabasz_score(X_filtered, y_filtered)\n",
    "                metrics['davies_bouldin'] = davies_bouldin_score(X_filtered, y_filtered)\n",
    "        elif len(set(y_pred)) >= 2:  # Almeno 2 cluster\n",
    "            metrics['silhouette'] = silhouette_score(X, y_pred)\n",
    "            metrics['calinski_harabasz'] = calinski_harabasz_score(X, y_pred)\n",
    "            metrics['davies_bouldin'] = davies_bouldin_score(X, y_pred)\n",
    "        \n",
    "        results[name] = {'labels': y_pred, 'metrics': metrics}\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizzare le metriche di valutazione\n",
    "    metrics_df = pd.DataFrame()\n",
    "    for name, result in results.items():\n",
    "        if result['metrics']:\n",
    "            metrics_df[name] = pd.Series(result['metrics'])\n",
    "    \n",
    "    if not metrics_df.empty:\n",
    "        print(\"Metriche di valutazione:\")\n",
    "        print(metrics_df.T)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con cluster ben separati\n",
    "results_blobs = compare_clustering_algorithms(X_blobs, true_labels=y_blobs, n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con cluster a forma di mezzaluna\n",
    "results_moons = compare_clustering_algorithms(X_moons, true_labels=y_moons, n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con cluster concentrici\n",
    "results_circles = compare_clustering_algorithms(X_circles, true_labels=y_circles, n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Caso di Studio - Segmentazione dei Clienti\n",
    "\n",
    "Applichiamo gli algoritmi di clustering a un caso di studio reale: la segmentazione dei clienti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Caricare il dataset Mall Customer Segmentation\n",
    "url = \"https://raw.githubusercontent.com/SteffiPeTaffy/machineLearningAZ/master/Machine%20Learning%20A-Z%20Template%20Folder/Part%204%20-%20Clustering/Section%2025%20-%20Hierarchical%20Clustering/Mall_Customers.csv\"\n",
    "customers = pd.read_csv(url)\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare il dataset\n",
    "print(f\"Dimensioni del dataset: {customers.shape}\")\n",
    "print(\"\\nInformazioni sul dataset:\")\n",
    "customers.info()\n",
    "print(\"\\nStatistiche descrittive:\")\n",
    "customers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la distribuzione delle variabili\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(customers['Age'], kde=True)\n",
    "plt.title('Distribuzione dell\\'età')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(x='Gender', data=customers)\n",
    "plt.title('Distribuzione del genere')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(customers['Annual Income (k$)'], kde=True)\n",
    "plt.title('Distribuzione del reddito annuale')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(customers['Spending Score (1-100)'], kde=True)\n",
    "plt.title('Distribuzione del punteggio di spesa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preparare i dati per il clustering\n",
    "# Utilizziamo solo reddito annuale e punteggio di spesa per la segmentazione\n",
    "X_customers = customers[['Annual Income (k$)', 'Spending Score (1-100)']].values\n",
    "\n",
    "# Visualizzare i dati\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_customers[:, 0], X_customers[:, 1], s=50, alpha=0.8)\n",
    "plt.title('Reddito annuale vs Punteggio di spesa', fontsize=14)\n",
    "plt.xlabel('Reddito annuale (k$)', fontsize=12)\n",
    "plt.ylabel('Punteggio di spesa (1-100)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determinare il numero ottimale di cluster con il metodo del gomito\n",
    "distortions_customers, silhouette_customers = plot_elbow_method(X_customers, max_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare K-means con il numero ottimale di cluster\n",
    "optimal_k = 5  # Basato sul metodo del gomito e silhouette score\n",
    "y_pred_customers, centers_customers = apply_kmeans(X_customers, n_clusters=optimal_k, \n",
    "                                                  title=\"Segmentazione dei clienti con K-means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Interpretare i cluster\n",
    "customers_clustered = customers.copy()\n",
    "customers_clustered['Cluster'] = y_pred_customers\n",
    "\n",
    "# Statistiche per cluster\n",
    "cluster_stats = customers_clustered.groupby('Cluster').agg({\n",
    "    'Age': ['mean', 'min', 'max'],\n",
    "    'Annual Income (k$)': ['mean', 'min', 'max'],\n",
    "    'Spending Score (1-100)': ['mean', 'min', 'max'],\n",
    "    'CustomerID': 'count'\n",
    "}).rename(columns={'CustomerID': 'Count'})\n",
    "\n",
    "print(\"Statistiche per cluster:\")\n",
    "display(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare i cluster con etichette interpretative\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red', 'blue', 'green', 'cyan', 'magenta']\n",
    "cluster_labels = [\n",
    "    'Reddito basso, Spesa bassa',\n",
    "    'Reddito alto, Spesa bassa',\n",
    "    'Reddito basso, Spesa alta',\n",
    "    'Reddito alto, Spesa alta',\n",
    "    'Reddito medio, Spesa media'\n",
    "]\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    plt.scatter(X_customers[y_pred_customers == i, 0], X_customers[y_pred_customers == i, 1], \n",
    "                s=100, c=colors[i], label=f'Cluster {i+1}: {cluster_labels[i]}')\n",
    "\n",
    "plt.scatter(centers_customers[:, 0], centers_customers[:, 1], s=300, c='yellow', marker='*', label='Centroidi')\n",
    "plt.title('Segmentazione dei clienti', fontsize=16)\n",
    "plt.xlabel('Reddito annuale (k$)', fontsize=14)\n",
    "plt.ylabel('Punteggio di spesa (1-100)', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la distribuzione dell'età per cluster\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Cluster', y='Age', data=customers_clustered)\n",
    "plt.title('Distribuzione dell\\'età per cluster', fontsize=14)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Età', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la distribuzione del genere per cluster\n",
    "gender_cluster = pd.crosstab(customers_clustered['Cluster'], customers_clustered['Gender'])\n",
    "gender_cluster.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.title('Distribuzione del genere per cluster', fontsize=14)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Conteggio', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Genere')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "In questa esercitazione pratica, abbiamo esplorato diversi algoritmi di clustering e li abbiamo applicati a vari dataset, incluso un caso di studio reale sulla segmentazione dei clienti. Abbiamo visto come:\n",
    "\n",
    "1. **K-means** è efficace per cluster ben separati e di forma sferica, ma ha difficoltà con forme complesse.\n",
    "2. **Clustering Gerarchico** offre una rappresentazione gerarchica dei dati e può essere utile per esplorare la struttura dei dati.\n",
    "3. **DBSCAN** eccelle nell'identificare cluster di forma arbitraria e nel rilevare outlier.\n",
    "4. **Gaussian Mixture Models** forniscono un approccio probabilistico al clustering e sono flessibili nella forma dei cluster.\n",
    "\n",
    "Abbiamo anche imparato l'importanza di:\n",
    "- Scegliere il numero appropriato di cluster\n",
    "- Selezionare l'algoritmo giusto in base alla natura dei dati\n",
    "- Valutare la qualità dei cluster con metriche appropriate\n",
    "- Interpretare i risultati nel contesto del problema specifico\n",
    "\n",
    "Nel caso di studio sulla segmentazione dei clienti, abbiamo identificato cinque segmenti distinti di clienti in base al loro reddito e comportamento di spesa, fornendo informazioni preziose per strategie di marketing mirate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizi Aggiuntivi\n",
    "\n",
    "1. Prova a segmentare i clienti utilizzando tutte e tre le variabili numeriche (età, reddito, punteggio di spesa). Come cambiano i cluster?\n",
    "2. Applica DBSCAN al dataset di segmentazione dei clienti. Quali parametri sceglieresti e perché?\n",
    "3. Implementa il clustering gerarchico sul dataset di segmentazione dei clienti e confronta i risultati con K-means.\n",
    "4. Crea un nuovo dataset sintetico con cluster di forme complesse e confronta le prestazioni dei diversi algoritmi.\n",
    "5. Implementa una versione semplificata di K-means da zero (senza utilizzare scikit-learn) e confronta i risultati con l'implementazione di scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
