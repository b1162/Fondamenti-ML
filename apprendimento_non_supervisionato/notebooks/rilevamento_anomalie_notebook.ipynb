{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione Pratica: Rilevamento di Anomalie\n",
    "\n",
    "## Corso di Machine Learning: Apprendimento Non Supervisionato\n",
    "\n",
    "Questa esercitazione pratica ti guiderà attraverso l'implementazione e l'applicazione delle principali tecniche di rilevamento di anomalie discusse nelle lezioni teoriche. Esploreremo diversi dataset e vedremo come applicare e valutare vari metodi per identificare osservazioni anomale.\n",
    "\n",
    "### Obiettivi dell'esercitazione:\n",
    "- Implementare e applicare approcci statistici, metodi basati sulla densità, Isolation Forest, One-Class SVM e altri algoritmi di rilevamento anomalie\n",
    "- Visualizzare e interpretare i risultati del rilevamento di anomalie\n",
    "- Valutare le prestazioni dei diversi algoritmi\n",
    "- Confrontare le diverse tecniche su vari tipi di dataset\n",
    "- Applicare il rilevamento di anomalie a casi di studio reali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione dell'ambiente\n",
    "\n",
    "Iniziamo importando le librerie necessarie per questa esercitazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importazione delle librerie necessarie\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Impostazioni di visualizzazione\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Generazione e Preparazione dei Dataset\n",
    "\n",
    "Creiamo diversi dataset sintetici con anomalie per testare i vari algoritmi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Funzione per visualizzare i dataset\n",
    "def plot_dataset(X, y=None, title=\"Dataset\", anomaly_label=1, alpha=0.7):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if X.shape[1] > 2:\n",
    "        # Se il dataset ha più di 2 dimensioni, utilizziamo PCA per visualizzarlo\n",
    "        pca = PCA(n_components=2)\n",
    "        X_2d = pca.fit_transform(X)\n",
    "        plt.title(f\"{title} (PCA 2D)\\nVarianza spiegata: {pca.explained_variance_ratio_.sum():.2f}\", fontsize=14)\n",
    "    else:\n",
    "        X_2d = X\n",
    "        plt.title(title, fontsize=14)\n",
    "    \n",
    "    if y is not None:\n",
    "        # Visualizzare punti normali e anomalie con colori diversi\n",
    "        normal_mask = (y != anomaly_label)\n",
    "        anomaly_mask = (y == anomaly_label)\n",
    "        \n",
    "        plt.scatter(X_2d[normal_mask, 0], X_2d[normal_mask, 1], \n",
    "                   c='blue', label='Normale', s=50, alpha=alpha)\n",
    "        plt.scatter(X_2d[anomaly_mask, 0], X_2d[anomaly_mask, 1], \n",
    "                   c='red', label='Anomalia', s=80, alpha=alpha, marker='X')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.scatter(X_2d[:, 0], X_2d[:, 1], s=50, alpha=alpha)\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Dataset con anomalie globali (outlier distanti dalla massa principale dei dati)\n",
    "def generate_global_outliers(n_samples=300, n_outliers=10, n_features=2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generare dati normali\n",
    "    X = np.random.randn(n_samples - n_outliers, n_features)\n",
    "    \n",
    "    # Generare outlier\n",
    "    outliers = np.random.uniform(low=-4, high=4, size=(n_outliers, n_features))\n",
    "    # Assicurarsi che gli outlier siano sufficientemente distanti\n",
    "    for i in range(n_outliers):\n",
    "        # Scegliere una direzione casuale\n",
    "        direction = np.random.randn(n_features)\n",
    "        direction = direction / np.linalg.norm(direction)\n",
    "        # Posizionare l'outlier a una distanza casuale (ma grande) nella direzione scelta\n",
    "        outliers[i] = direction * np.random.uniform(4, 6)\n",
    "    \n",
    "    # Combinare dati normali e outlier\n",
    "    X_combined = np.vstack([X, outliers])\n",
    "    \n",
    "    # Creare le etichette (0: normale, 1: anomalia)\n",
    "    y = np.zeros(n_samples)\n",
    "    y[n_samples - n_outliers:] = 1\n",
    "    \n",
    "    return X_combined, y\n",
    "\n",
    "# Generare e visualizzare il dataset con anomalie globali\n",
    "X_global, y_global = generate_global_outliers(n_samples=300, n_outliers=15)\n",
    "plot_dataset(X_global, y_global, \"Dataset con Anomalie Globali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Dataset con anomalie locali (outlier vicini alla massa principale ma in regioni a bassa densità)\n",
    "def generate_local_outliers(n_samples=300, n_outliers=10, n_features=2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generare dati normali da due cluster\n",
    "    X1 = np.random.randn(n_samples // 2, n_features) + np.array([3, 3])\n",
    "    X2 = np.random.randn(n_samples // 2 - n_outliers, n_features) + np.array([-3, -3])\n",
    "    \n",
    "    # Generare outlier nella regione tra i due cluster\n",
    "    outliers = np.random.uniform(low=-1, high=1, size=(n_outliers, n_features))\n",
    "    \n",
    "    # Combinare dati normali e outlier\n",
    "    X_combined = np.vstack([X1, X2, outliers])\n",
    "    \n",
    "    # Creare le etichette (0: normale, 1: anomalia)\n",
    "    y = np.zeros(n_samples)\n",
    "    y[n_samples - n_outliers:] = 1\n",
    "    \n",
    "    return X_combined, y\n",
    "\n",
    "# Generare e visualizzare il dataset con anomalie locali\n",
    "X_local, y_local = generate_local_outliers(n_samples=300, n_outliers=15)\n",
    "plot_dataset(X_local, y_local, \"Dataset con Anomalie Locali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Dataset con cluster di anomalie\n",
    "def generate_clustered_outliers(n_samples=300, n_outliers=20, n_features=2, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generare dati normali\n",
    "    X = np.random.randn(n_samples - n_outliers, n_features) * 1.5\n",
    "    \n",
    "    # Generare un piccolo cluster di outlier\n",
    "    outliers = np.random.randn(n_outliers, n_features) * 0.3 + np.array([5, 5])\n",
    "    \n",
    "    # Combinare dati normali e outlier\n",
    "    X_combined = np.vstack([X, outliers])\n",
    "    \n",
    "    # Creare le etichette (0: normale, 1: anomalia)\n",
    "    y = np.zeros(n_samples)\n",
    "    y[n_samples - n_outliers:] = 1\n",
    "    \n",
    "    return X_combined, y\n",
    "\n",
    "# Generare e visualizzare il dataset con cluster di anomalie\n",
    "X_clustered, y_clustered = generate_clustered_outliers(n_samples=300, n_outliers=20)\n",
    "plot_dataset(X_clustered, y_clustered, \"Dataset con Cluster di Anomalie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4. Dataset con anomalie in alta dimensionalità\n",
    "def generate_high_dim_outliers(n_samples=300, n_outliers=15, n_features=10, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generare dati normali\n",
    "    X = np.random.randn(n_samples - n_outliers, n_features)\n",
    "    \n",
    "    # Generare outlier con valori estremi in alcune dimensioni\n",
    "    outliers = np.random.randn(n_outliers, n_features)\n",
    "    for i in range(n_outliers):\n",
    "        # Selezionare casualmente alcune dimensioni (1-3) per avere valori estremi\n",
    "        n_extreme_dims = np.random.randint(1, 4)\n",
    "        extreme_dims = np.random.choice(n_features, n_extreme_dims, replace=False)\n",
    "        for dim in extreme_dims:\n",
    "            # Assegnare un valore estremo (positivo o negativo)\n",
    "            outliers[i, dim] = np.random.choice([-1, 1]) * np.random.uniform(5, 8)\n",
    "    \n",
    "    # Combinare dati normali e outlier\n",
    "    X_combined = np.vstack([X, outliers])\n",
    "    \n",
    "    # Creare le etichette (0: normale, 1: anomalia)\n",
    "    y = np.zeros(n_samples)\n",
    "    y[n_samples - n_outliers:] = 1\n",
    "    \n",
    "    return X_combined, y\n",
    "\n",
    "# Generare e visualizzare il dataset con anomalie in alta dimensionalità\n",
    "X_high_dim, y_high_dim = generate_high_dim_outliers(n_samples=300, n_outliers=15, n_features=10)\n",
    "plot_dataset(X_high_dim, y_high_dim, \"Dataset con Anomalie in Alta Dimensionalità\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Approcci Statistici al Rilevamento di Anomalie\n",
    "\n",
    "Implementiamo e applichiamo approcci statistici per il rilevamento di anomalie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_zscore(X, threshold=3.0):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Calcolare il z-score per ogni feature\n",
    "    z_scores = np.abs(X_scaled)\n",
    "    \n",
    "    # Considerare il massimo z-score tra tutte le feature per ogni punto\n",
    "    max_z_scores = np.max(z_scores, axis=1)\n",
    "    \n",
    "    # Identificare le anomalie\n",
    "    anomalies = max_z_scores > threshold\n",
    "    \n",
    "    return anomalies, max_z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare Z-score al dataset con anomalie globali\n",
    "anomalies_zscore, scores_zscore = detect_anomalies_zscore(X_global)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=anomalies_zscore, cmap='coolwarm', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Anomalia')\n",
    "plt.title('Rilevamento Anomalie con Z-score')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=scores_zscore, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Max Z-score')\n",
    "plt.title('Punteggi Z-score')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_global, anomalies_zscore))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_global, anomalies_zscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Elliptic Envelope (Mahalanobis Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_elliptic(X, contamination=0.05):\n",
    "    # Applicare Elliptic Envelope\n",
    "    detector = EllipticEnvelope(contamination=contamination, random_state=42)\n",
    "    detector.fit(X)\n",
    "    \n",
    "    # Predire le anomalie (-1 per anomalie, 1 per normali)\n",
    "    y_pred = detector.predict(X)\n",
    "    anomalies = y_pred == -1\n",
    "    \n",
    "    # Calcolare i punteggi di anomalia\n",
    "    scores = -detector.decision_function(X)  # Negativo per avere punteggi più alti per le anomalie\n",
    "    \n",
    "    return anomalies, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare Elliptic Envelope al dataset con anomalie globali\n",
    "contamination = len(y_global[y_global == 1]) / len(y_global)  # Proporzione reale di anomalie\n",
    "anomalies_elliptic, scores_elliptic = detect_anomalies_elliptic(X_global, contamination=contamination)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=anomalies_elliptic, cmap='coolwarm', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Anomalia')\n",
    "plt.title('Rilevamento Anomalie con Elliptic Envelope')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=scores_elliptic, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Punteggio Anomalia')\n",
    "plt.title('Punteggi Elliptic Envelope')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_global, anomalies_elliptic))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_global, anomalies_elliptic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizzazione delle regioni di decisione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_decision_boundary(X, detector, title=\"Decision Boundary\", method_name=\"\"):\n",
    "    # Creare una griglia di punti\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Calcolare i punteggi di anomalia per ogni punto della griglia\n",
    "    if method_name == \"Z-score\":\n",
    "        # Per Z-score, dobbiamo implementare manualmente\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        grid_points_scaled = scaler.transform(grid_points)\n",
    "        Z = np.max(np.abs(grid_points_scaled), axis=1)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        levels = [3.0]  # Soglia Z-score\n",
    "    else:\n",
    "        # Per altri metodi, utilizziamo decision_function\n",
    "        Z = detector.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        if method_name == \"Elliptic Envelope\":\n",
    "            levels = [0]  # Soglia Elliptic Envelope\n",
    "        else:\n",
    "            levels = [0]  # Soglia generica\n",
    "    \n",
    "    # Visualizzare la regione di decisione\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=10, cmap='viridis', alpha=0.3)\n",
    "    plt.colorbar(label='Punteggio Anomalia')\n",
    "    plt.contour(xx, yy, Z, levels=levels, colors='red', linestyles='dashed')\n",
    "    \n",
    "    # Visualizzare i punti\n",
    "    if hasattr(detector, 'predict'):\n",
    "        y_pred = detector.predict(X)\n",
    "        if method_name == \"Elliptic Envelope\":\n",
    "            anomalies = y_pred == -1\n",
    "        else:\n",
    "            anomalies = y_pred == -1\n",
    "    else:\n",
    "        # Per Z-score\n",
    "        anomalies, _ = detect_anomalies_zscore(X)\n",
    "    \n",
    "    plt.scatter(X[~anomalies, 0], X[~anomalies, 1], c='blue', label='Normale', s=50, alpha=0.7)\n",
    "    plt.scatter(X[anomalies, 0], X[anomalies, 1], c='red', label='Anomalia', s=80, alpha=0.7, marker='X')\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la regione di decisione per Elliptic Envelope\n",
    "detector_elliptic = EllipticEnvelope(contamination=contamination, random_state=42)\n",
    "detector_elliptic.fit(X_global)\n",
    "plot_decision_boundary(X_global, detector_elliptic, \"Regione di Decisione - Elliptic Envelope\", \"Elliptic Envelope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Metodi Basati sulla Densità\n",
    "\n",
    "Implementiamo e applichiamo metodi basati sulla densità per il rilevamento di anomalie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_lof(X, n_neighbors=20, contamination=0.05):\n",
    "    # Applicare Local Outlier Factor\n",
    "    detector = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=False)\n",
    "    \n",
    "    # Predire le anomalie (-1 per anomalie, 1 per normali)\n",
    "    y_pred = detector.fit_predict(X)\n",
    "    anomalies = y_pred == -1\n",
    "    \n",
    "    # Calcolare i punteggi LOF\n",
    "    scores = -detector.negative_outlier_factor_  # Negativo per avere punteggi più alti per le anomalie\n",
    "    \n",
    "    return anomalies, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare LOF al dataset con anomalie locali\n",
    "contamination_local = len(y_local[y_local == 1]) / len(y_local)  # Proporzione reale di anomalie\n",
    "anomalies_lof, scores_lof = detect_anomalies_lof(X_local, contamination=contamination_local)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_local[:, 0], X_local[:, 1], c=anomalies_lof, cmap='coolwarm', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Anomalia')\n",
    "plt.title('Rilevamento Anomalie con LOF')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_local[:, 0], X_local[:, 1], c=scores_lof, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Punteggio LOF')\n",
    "plt.title('Punteggi LOF')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_local, anomalies_lof))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_local, anomalies_lof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DBSCAN per il Rilevamento di Anomalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_dbscan(X, eps=0.5, min_samples=5):\n",
    "    # Applicare DBSCAN\n",
    "    detector = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    detector.fit(X)\n",
    "    \n",
    "    # Predire le anomalie (-1 per rumore/anomalie)\n",
    "    labels = detector.labels_\n",
    "    anomalies = labels == -1\n",
    "    \n",
    "    # Calcolare una sorta di punteggio di anomalia basato sulla distanza dal cluster più vicino\n",
    "    # Questo è un approccio semplificato, non è un punteggio standard di DBSCAN\n",
    "    scores = np.zeros(X.shape[0])\n",
    "    \n",
    "    # Per ogni punto, calcolare la distanza minima dai core points\n",
    "    core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
    "    core_samples_mask[detector.core_sample_indices_] = True\n",
    "    \n",
    "    if np.any(core_samples_mask):\n",
    "        # Se ci sono core points, calcolare la distanza da essi\n",
    "        core_points = X[core_samples_mask]\n",
    "        for i, x in enumerate(X):\n",
    "            min_dist = np.min(np.linalg.norm(x - core_points, axis=1))\n",
    "            scores[i] = min_dist\n",
    "    \n",
    "    return anomalies, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determinare i parametri ottimali per DBSCAN\n",
    "def find_optimal_eps(X, min_samples=5, k=5):\n",
    "    # Calcolare le distanze ai k vicini più prossimi\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    distances, indices = neigh.kneighbors(X)\n",
    "    \n",
    "    # Ordinare le distanze in ordine crescente\n",
    "    distances = np.sort(distances[:, k-1])\n",
    "    \n",
    "    # Visualizzare il grafico delle distanze\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(distances)), distances, 'b-')\n",
    "    plt.xlabel('Punti ordinati per distanza')\n",
    "    plt.ylabel(f'Distanza al {k}-esimo vicino più prossimo')\n",
    "    plt.title('Grafico delle distanze per determinare eps ottimale')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calcolare la derivata per trovare il punto di massima curvatura\n",
    "    derivative = np.gradient(distances)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(derivative)), derivative, 'r-')\n",
    "    plt.xlabel('Punti ordinati per distanza')\n",
    "    plt.ylabel('Derivata della distanza')\n",
    "    plt.title('Derivata delle distanze per identificare il \"gomito\"')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Trovare il punto di massima curvatura (approssimazione)\n",
    "    knee_point = np.argmax(derivative) if len(derivative) > 0 else 0\n",
    "    suggested_eps = distances[knee_point]\n",
    "    \n",
    "    print(f\"Valore di eps suggerito: {suggested_eps:.3f}\")\n",
    "    print(f\"Valore di min_samples suggerito: {min_samples}\")\n",
    "    \n",
    "    return suggested_eps, min_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trovare i parametri ottimali per DBSCAN sul dataset con anomalie locali\n",
    "eps_local, min_samples_local = find_optimal_eps(X_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare DBSCAN al dataset con anomalie locali\n",
    "anomalies_dbscan, scores_dbscan = detect_anomalies_dbscan(X_local, eps=eps_local, min_samples=min_samples_local)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_local[:, 0], X_local[:, 1], c=anomalies_dbscan, cmap='coolwarm', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Anomalia')\n",
    "plt.title('Rilevamento Anomalie con DBSCAN')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_local[:, 0], X_local[:, 1], c=scores_dbscan, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Distanza dal cluster più vicino')\n",
    "plt.title('Punteggi DBSCAN')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_local, anomalies_dbscan))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_local, anomalies_dbscan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Isolation Forest\n",
    "\n",
    "Implementiamo e applichiamo Isolation Forest per il rilevamento di anomalie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_iforest(X, n_estimators=100, contamination=0.05, max_samples='auto'):\n",
    "    # Applicare Isolation Forest\n",
    "    detector = IsolationForest(n_estimators=n_estimators, contamination=contamination, \n",
    "                              max_samples=max_samples, random_state=42)\n",
    "    detector.fit(X)\n",
    "    \n",
    "    # Predire le anomalie (-1 per anomalie, 1 per normali)\n",
    "    y_pred = detector.predict(X)\n",
    "    anomalies = y_pred == -1\n",
    "    \n",
    "    # Calcolare i punteggi di anomalia\n",
    "    scores = -detector.decision_function(X)  # Negativo per avere punteggi più alti per le anomalie\n",
    "    \n",
    "    return anomalies, scores, detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare Isolation Forest al dataset con anomalie globali\n",
    "anomalies_iforest, scores_iforest, detector_iforest = detect_anomalies_iforest(X_global, contamination=contamination)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=anomalies_iforest, cmap='coolwarm', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Anomalia')\n",
    "plt.title('Rilevamento Anomalie con Isolation Forest')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=scores_iforest, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Punteggio Anomalia')\n",
    "plt.title('Punteggi Isolation Forest')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_global, anomalies_iforest))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_global, anomalies_iforest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la regione di decisione per Isolation Forest\n",
    "plot_decision_boundary(X_global, detector_iforest, \"Regione di Decisione - Isolation Forest\", \"Isolation Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Effetto dei parametri di Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def explore_iforest_parameters(X, y, n_estimators_list=[50, 100, 200], max_samples_list=['auto', 64, 128]):\n",
    "    # Calcolare la contaminazione reale\n",
    "    contamination = np.sum(y == 1) / len(y)\n",
    "    \n",
    "    # Preparare la figura\n",
    "    fig, axes = plt.subplots(len(n_estimators_list), len(max_samples_list), figsize=(15, 12))\n",
    "    \n",
    "    # Per ogni combinazione di parametri\n",
    "    for i, n_estimators in enumerate(n_estimators_list):\n",
    "        for j, max_samples in enumerate(max_samples_list):\n",
    "            # Applicare Isolation Forest\n",
    "            detector = IsolationForest(n_estimators=n_estimators, contamination=contamination, \n",
    "                                      max_samples=max_samples, random_state=42)\n",
    "            detector.fit(X)\n",
    "            \n",
    "            # Predire le anomalie\n",
    "            y_pred = detector.predict(X)\n",
    "            anomalies = y_pred == -1\n",
    "            \n",
    "            # Calcolare le metriche\n",
    "            precision = precision_score(y, anomalies)\n",
    "            recall = recall_score(y, anomalies)\n",
    "            f1 = f1_score(y, anomalies)\n",
    "            \n",
    "            # Visualizzare i risultati\n",
    "            axes[i, j].scatter(X[:, 0], X[:, 1], c=anomalies, cmap='coolwarm', s=30, alpha=0.7)\n",
    "            axes[i, j].set_title(f\"n_est={n_estimators}, max_samp={max_samples}\\nP={precision:.2f}, R={recall:.2f}, F1={f1:.2f}\")\n",
    "            axes[i, j].set_xlabel('Feature 1')\n",
    "            axes[i, j].set_ylabel('Feature 2')\n",
    "            axes[i, j].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"Effetto dei parametri in Isolation Forest\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare l'effetto dei parametri di Isolation Forest sul dataset con anomalie globali\n",
    "explore_iforest_parameters(X_global, y_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: One-Class SVM\n",
    "\n",
    "Implementiamo e applichiamo One-Class SVM per il rilevamento di anomalie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_ocsvm(X, nu=0.05, kernel='rbf', gamma='scale'):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Applicare One-Class SVM\n",
    "    detector = OneClassSVM(nu=nu, kernel=kernel, gamma=gamma)\n",
    "    detector.fit(X_scaled)\n",
    "    \n",
    "    # Predire le anomalie (-1 per anomalie, 1 per normali)\n",
    "    y_pred = detector.predict(X_scaled)\n",
    "    anomalies = y_pred == -1\n",
    "    \n",
    "    # Calcolare i punteggi di anomalia\n",
    "    scores = -detector.decision_function(X_scaled)  # Negativo per avere punteggi più alti per le anomalie\n",
    "    \n",
    "    return anomalies, scores, detector, X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare One-Class SVM al dataset con anomalie globali\n",
    "anomalies_ocsvm, scores_ocsvm, detector_ocsvm, X_global_scaled = detect_anomalies_ocsvm(X_global, nu=contamination)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=anomalies_ocsvm, cmap='coolwarm', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Anomalia')\n",
    "plt.title('Rilevamento Anomalie con One-Class SVM')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_global[:, 0], X_global[:, 1], c=scores_ocsvm, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Punteggio Anomalia')\n",
    "plt.title('Punteggi One-Class SVM')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_global, anomalies_ocsvm))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_global, anomalies_ocsvm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la regione di decisione per One-Class SVM\n",
    "plot_decision_boundary(X_global_scaled, detector_ocsvm, \"Regione di Decisione - One-Class SVM\", \"One-Class SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Effetto dei parametri di One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def explore_ocsvm_parameters(X, y, nu_list=[0.01, 0.05, 0.1], gamma_list=['scale', 0.1, 1.0]):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Preparare la figura\n",
    "    fig, axes = plt.subplots(len(nu_list), len(gamma_list), figsize=(15, 12))\n",
    "    \n",
    "    # Per ogni combinazione di parametri\n",
    "    for i, nu in enumerate(nu_list):\n",
    "        for j, gamma in enumerate(gamma_list):\n",
    "            # Applicare One-Class SVM\n",
    "            detector = OneClassSVM(nu=nu, kernel='rbf', gamma=gamma)\n",
    "            detector.fit(X_scaled)\n",
    "            \n",
    "            # Predire le anomalie\n",
    "            y_pred = detector.predict(X_scaled)\n",
    "            anomalies = y_pred == -1\n",
    "            \n",
    "            # Calcolare le metriche\n",
    "            precision = precision_score(y, anomalies)\n",
    "            recall = recall_score(y, anomalies)\n",
    "            f1 = f1_score(y, anomalies)\n",
    "            \n",
    "            # Visualizzare i risultati\n",
    "            axes[i, j].scatter(X[:, 0], X[:, 1], c=anomalies, cmap='coolwarm', s=30, alpha=0.7)\n",
    "            axes[i, j].set_title(f\"nu={nu}, gamma={gamma}\\nP={precision:.2f}, R={recall:.2f}, F1={f1:.2f}\")\n",
    "            axes[i, j].set_xlabel('Feature 1')\n",
    "            axes[i, j].set_ylabel('Feature 2')\n",
    "            axes[i, j].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"Effetto dei parametri in One-Class SVM\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare l'effetto dei parametri di One-Class SVM sul dataset con anomalie globali\n",
    "explore_ocsvm_parameters(X_global, y_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Confronto tra Algoritmi di Rilevamento di Anomalie\n",
    "\n",
    "Confrontiamo le prestazioni dei diversi algoritmi di rilevamento di anomalie sui nostri dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def compare_anomaly_detection_algorithms(X, y, contamination=None):\n",
    "    if contamination is None:\n",
    "        contamination = np.sum(y == 1) / len(y)\n",
    "    \n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Definire gli algoritmi da confrontare\n",
    "    algorithms = {\n",
    "        'Z-score': None,  # Implementazione manuale\n",
    "        'Elliptic Envelope': EllipticEnvelope(contamination=contamination, random_state=42),\n",
    "        'Local Outlier Factor': LocalOutlierFactor(n_neighbors=20, contamination=contamination, novelty=False),\n",
    "        'Isolation Forest': IsolationForest(n_estimators=100, contamination=contamination, random_state=42),\n",
    "        'One-Class SVM': OneClassSVM(nu=contamination, kernel='rbf', gamma='scale')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Applicare ogni algoritmo\n",
    "    for name, algorithm in algorithms.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if name == 'Z-score':\n",
    "            # Implementazione manuale di Z-score\n",
    "            anomalies, scores = detect_anomalies_zscore(X, threshold=3.0)\n",
    "        elif name == 'Local Outlier Factor':\n",
    "            # LOF in modalità non-novelty\n",
    "            y_pred = algorithm.fit_predict(X_scaled)\n",
    "            anomalies = y_pred == -1\n",
    "            scores = -algorithm.negative_outlier_factor_\n",
    "        else:\n",
    "            # Altri algoritmi\n",
    "            if name == 'One-Class SVM':\n",
    "                algorithm.fit(X_scaled)\n",
    "                y_pred = algorithm.predict(X_scaled)\n",
    "                scores = -algorithm.decision_function(X_scaled)\n",
    "            else:\n",
    "                algorithm.fit(X_scaled)\n",
    "                y_pred = algorithm.predict(X_scaled)\n",
    "                scores = -algorithm.decision_function(X_scaled) if hasattr(algorithm, 'decision_function') else np.zeros(len(X))\n",
    "            anomalies = y_pred == -1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        \n",
    "        # Calcolare le metriche\n",
    "        precision = precision_score(y, anomalies)\n",
    "        recall = recall_score(y, anomalies)\n",
    "        f1 = f1_score(y, anomalies)\n",
    "        \n",
    "        # Calcolare AUC se possibile\n",
    "        try:\n",
    "            auc = roc_auc_score(y, scores)\n",
    "        except:\n",
    "            auc = np.nan\n",
    "        \n",
    "        results[name] = {\n",
    "            'anomalies': anomalies,\n",
    "            'scores': scores,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc': auc,\n",
    "            'time': execution_time\n",
    "        }\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Visualizzare il dataset originale con le anomalie vere\n",
    "    axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, alpha=0.7)\n",
    "    axes[0].set_title('Anomalie Vere')\n",
    "    axes[0].set_xlabel('Feature 1')\n",
    "    axes[0].set_ylabel('Feature 2')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Visualizzare i risultati di ogni algoritmo\n",
    "    for i, (name, result) in enumerate(results.items(), 1):\n",
    "        if i < len(axes):\n",
    "            axes[i].scatter(X[:, 0], X[:, 1], c=result['anomalies'], cmap='coolwarm', s=50, alpha=0.7)\n",
    "            axes[i].set_title(f\"{name}\\nP={result['precision']:.2f}, R={result['recall']:.2f}, F1={result['f1']:.2f}\")\n",
    "            axes[i].set_xlabel('Feature 1')\n",
    "            axes[i].set_ylabel('Feature 2')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizzare le metriche di valutazione\n",
    "    metrics = ['precision', 'recall', 'f1', 'auc', 'time']\n",
    "    metrics_df = pd.DataFrame({name: [result[metric] for metric in metrics] for name, result in results.items()}, index=metrics)\n",
    "    \n",
    "    # Visualizzare le metriche come grafico a barre\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Visualizzare precision, recall, f1, auc\n",
    "    plt.subplot(2, 1, 1)\n",
    "    metrics_df.loc[['precision', 'recall', 'f1', 'auc']].plot(kind='bar', ax=plt.gca())\n",
    "    plt.title('Metriche di Valutazione')\n",
    "    plt.ylabel('Valore')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Visualizzare il tempo di esecuzione\n",
    "    plt.subplot(2, 1, 2)\n",
    "    metrics_df.loc['time'].plot(kind='bar', ax=plt.gca(), color='green')\n",
    "    plt.title('Tempo di Esecuzione')\n",
    "    plt.ylabel('Secondi')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con anomalie globali\n",
    "results_global, metrics_global = compare_anomaly_detection_algorithms(X_global, y_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con anomalie locali\n",
    "results_local, metrics_local = compare_anomaly_detection_algorithms(X_local, y_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con cluster di anomalie\n",
    "results_clustered, metrics_clustered = compare_anomaly_detection_algorithms(X_clustered, y_clustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Curve ROC e Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_roc_pr_curves(results, y_true):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Curva ROC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, result in results.items():\n",
    "        if not np.isnan(result['auc']):\n",
    "            fpr, tpr, _ = roc_curve(y_true, result['scores'])\n",
    "            plt.plot(fpr, tpr, label=f\"{name} (AUC = {result['auc']:.3f})\")\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Curve ROC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Curva Precision-Recall\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, result in results.items():\n",
    "        if not np.isnan(result['auc']):\n",
    "            precision, recall, _ = precision_recall_curve(y_true, result['scores'])\n",
    "            avg_precision = average_precision_score(y_true, result['scores'])\n",
    "            plt.plot(recall, precision, label=f\"{name} (AP = {avg_precision:.3f})\")\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Curve Precision-Recall')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare le curve ROC e Precision-Recall per il dataset con anomalie globali\n",
    "plot_roc_pr_curves(results_global, y_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Rilevamento di Anomalie in Alta Dimensionalità\n",
    "\n",
    "Esploriamo il rilevamento di anomalie in dataset ad alta dimensionalità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare gli algoritmi sul dataset con anomalie in alta dimensionalità\n",
    "results_high_dim, metrics_high_dim = compare_anomaly_detection_algorithms(X_high_dim, y_high_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare le curve ROC e Precision-Recall per il dataset con anomalie in alta dimensionalità\n",
    "plot_roc_pr_curves(results_high_dim, y_high_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Riduzione della dimensionalità prima del rilevamento di anomalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_with_dim_reduction(X, y, n_components=2, contamination=None):\n",
    "    if contamination is None:\n",
    "        contamination = np.sum(y == 1) / len(y)\n",
    "    \n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Applicare PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Visualizzare i dati ridotti\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', s=50, alpha=0.7)\n",
    "    plt.colorbar(label='Anomalia')\n",
    "    plt.title(f'Dati ridotti con PCA (varianza spiegata: {pca.explained_variance_ratio_.sum():.3f})')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Applicare gli algoritmi di rilevamento anomalie sui dati ridotti\n",
    "    results_pca, metrics_pca = compare_anomaly_detection_algorithms(X_pca, y, contamination)\n",
    "    \n",
    "    return results_pca, metrics_pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare la riduzione della dimensionalità prima del rilevamento di anomalie\n",
    "results_high_dim_pca, metrics_high_dim_pca, X_high_dim_pca = detect_anomalies_with_dim_reduction(X_high_dim, y_high_dim, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare le prestazioni prima e dopo la riduzione della dimensionalità\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Confrontare F1-score\n",
    "plt.subplot(1, 2, 1)\n",
    "f1_original = metrics_high_dim.loc['f1']\n",
    "f1_pca = metrics_high_dim_pca.loc['f1']\n",
    "algorithms = f1_original.index\n",
    "\n",
    "x = np.arange(len(algorithms))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, f1_original, width, label='Originale')\n",
    "plt.bar(x + width/2, f1_pca, width, label='Dopo PCA')\n",
    "\n",
    "plt.xlabel('Algoritmo')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score prima e dopo PCA')\n",
    "plt.xticks(x, algorithms, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Confrontare AUC\n",
    "plt.subplot(1, 2, 2)\n",
    "auc_original = metrics_high_dim.loc['auc']\n",
    "auc_pca = metrics_high_dim_pca.loc['auc']\n",
    "\n",
    "plt.bar(x - width/2, auc_original, width, label='Originale')\n",
    "plt.bar(x + width/2, auc_pca, width, label='Dopo PCA')\n",
    "\n",
    "plt.xlabel('Algoritmo')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('AUC prima e dopo PCA')\n",
    "plt.xticks(x, algorithms, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Rilevamento di Anomalie in Serie Temporali\n",
    "\n",
    "Esploriamo il rilevamento di anomalie in serie temporali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generare una serie temporale sintetica con anomalie\n",
    "def generate_time_series_with_anomalies(n_samples=1000, n_anomalies=50, anomaly_std=5, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generare il tempo\n",
    "    t = np.linspace(0, 10, n_samples)\n",
    "    \n",
    "    # Generare il segnale normale (sinusoidale con rumore)\n",
    "    signal = 10 * np.sin(t) + np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Aggiungere un trend\n",
    "    trend = 0.01 * t**2\n",
    "    signal = signal + trend\n",
    "    \n",
    "    # Aggiungere stagionalità\n",
    "    seasonality = 5 * np.sin(t * 10)\n",
    "    signal = signal + seasonality\n",
    "    \n",
    "    # Creare le etichette (0: normale, 1: anomalia)\n",
    "    y = np.zeros(n_samples)\n",
    "    \n",
    "    # Aggiungere anomalie puntuali\n",
    "    anomaly_indices = np.random.choice(n_samples, n_anomalies, replace=False)\n",
    "    signal[anomaly_indices] += np.random.normal(0, anomaly_std, n_anomalies)\n",
    "    y[anomaly_indices] = 1\n",
    "    \n",
    "    return t, signal, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generare e visualizzare la serie temporale\n",
    "t, signal, y_ts = generate_time_series_with_anomalies()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t, signal, 'b-', alpha=0.7, label='Segnale')\n",
    "plt.scatter(t[y_ts == 1], signal[y_ts == 1], color='red', s=50, label='Anomalie')\n",
    "plt.title('Serie Temporale con Anomalie')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Approccio statistico: Z-score su finestra mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_rolling_zscore(signal, window_size=50, threshold=3.0):\n",
    "    # Inizializzare l'array dei punteggi\n",
    "    scores = np.zeros_like(signal)\n",
    "    \n",
    "    # Calcolare lo z-score su finestra mobile\n",
    "    for i in range(len(signal)):\n",
    "        if i < window_size:\n",
    "            # Per i primi punti, usare tutti i dati disponibili\n",
    "            window = signal[:i+1]\n",
    "        else:\n",
    "            # Altrimenti, usare la finestra mobile\n",
    "            window = signal[i-window_size+1:i+1]\n",
    "        \n",
    "        if len(window) > 1:  # Assicurarsi che ci siano abbastanza dati per calcolare media e std\n",
    "            mean = np.mean(window)\n",
    "            std = np.std(window)\n",
    "            if std > 0:  # Evitare divisione per zero\n",
    "                scores[i] = abs((signal[i] - mean) / std)\n",
    "    \n",
    "    # Identificare le anomalie\n",
    "    anomalies = scores > threshold\n",
    "    \n",
    "    return anomalies, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare Z-score su finestra mobile\n",
    "anomalies_rolling, scores_rolling = detect_anomalies_rolling_zscore(signal)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t, signal, 'b-', alpha=0.7, label='Segnale')\n",
    "plt.scatter(t[anomalies_rolling], signal[anomalies_rolling], color='red', s=50, label='Anomalie rilevate')\n",
    "plt.scatter(t[y_ts == 1], signal[y_ts == 1], color='green', s=30, alpha=0.5, label='Anomalie vere')\n",
    "plt.title('Rilevamento Anomalie con Z-score su Finestra Mobile')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(t, scores_rolling, 'r-')\n",
    "plt.axhline(y=3.0, color='k', linestyle='--', alpha=0.7, label='Soglia (Z=3)')\n",
    "plt.title('Punteggi Z-score')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Z-score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_ts, anomalies_rolling))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_ts, anomalies_rolling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Isolation Forest per serie temporali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_anomalies_ts_iforest(signal, window_size=10, contamination=0.05):\n",
    "    # Creare feature da finestre temporali\n",
    "    X = np.zeros((len(signal) - window_size + 1, window_size))\n",
    "    for i in range(len(X)):\n",
    "        X[i] = signal[i:i+window_size]\n",
    "    \n",
    "    # Applicare Isolation Forest\n",
    "    detector = IsolationForest(contamination=contamination, random_state=42)\n",
    "    detector.fit(X)\n",
    "    \n",
    "    # Predire le anomalie (-1 per anomalie, 1 per normali)\n",
    "    y_pred = detector.predict(X)\n",
    "    anomalies_window = y_pred == -1\n",
    "    \n",
    "    # Calcolare i punteggi di anomalia\n",
    "    scores_window = -detector.decision_function(X)  # Negativo per avere punteggi più alti per le anomalie\n",
    "    \n",
    "    # Convertire i risultati alla lunghezza originale della serie\n",
    "    anomalies = np.zeros(len(signal), dtype=bool)\n",
    "    scores = np.zeros(len(signal))\n",
    "    \n",
    "    # Assegnare l'etichetta di anomalia se almeno una finestra che contiene il punto è anomala\n",
    "    for i in range(len(signal)):\n",
    "        # Trovare tutte le finestre che contengono il punto i\n",
    "        window_indices = [j for j in range(max(0, i - window_size + 1), min(i + 1, len(anomalies_window)))]\n",
    "        if window_indices:  # Se ci sono finestre che contengono il punto\n",
    "            # Punto è anomalo se almeno una finestra che lo contiene è anomala\n",
    "            anomalies[i] = np.any(anomalies_window[window_indices])\n",
    "            # Punteggio è il massimo tra i punteggi delle finestre che contengono il punto\n",
    "            scores[i] = np.max(scores_window[window_indices])\n",
    "    \n",
    "    return anomalies, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare Isolation Forest per serie temporali\n",
    "contamination_ts = np.sum(y_ts == 1) / len(y_ts)\n",
    "anomalies_ts_iforest, scores_ts_iforest = detect_anomalies_ts_iforest(signal, contamination=contamination_ts)\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(t, signal, 'b-', alpha=0.7, label='Segnale')\n",
    "plt.scatter(t[anomalies_ts_iforest], signal[anomalies_ts_iforest], color='red', s=50, label='Anomalie rilevate')\n",
    "plt.scatter(t[y_ts == 1], signal[y_ts == 1], color='green', s=30, alpha=0.5, label='Anomalie vere')\n",
    "plt.title('Rilevamento Anomalie con Isolation Forest per Serie Temporali')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Valore')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(t, scores_ts_iforest, 'r-')\n",
    "plt.title('Punteggi Isolation Forest')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Punteggio Anomalia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Valutare le prestazioni\n",
    "print(\"Matrice di confusione:\")\n",
    "print(confusion_matrix(y_ts, anomalies_ts_iforest))\n",
    "print(\"\\nReport di classificazione:\")\n",
    "print(classification_report(y_ts, anomalies_ts_iforest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 9: Caso di Studio - Rilevamento di Frodi nelle Transazioni con Carte di Credito\n",
    "\n",
    "Applichiamo le tecniche di rilevamento di anomalie a un caso di studio reale: il rilevamento di frodi nelle transazioni con carte di credito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Caricare il dataset Credit Card Fraud Detection\n",
    "# Nota: questo dataset è molto grande, quindi utilizziamo solo un sottoinsieme\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Scaricare il dataset (potrebbe richiedere tempo)\n",
    "try:\n",
    "    cc_fraud = fetch_openml(name='credit-card-fraud', version=1, parser='auto')\n",
    "    X_cc = cc_fraud.data.values\n",
    "    y_cc = cc_fraud.target.astype(int).values\n",
    "    \n",
    "    # Prendere solo un sottoinsieme per velocizzare l'esercitazione\n",
    "    n_samples = 5000\n",
    "    indices = np.random.choice(len(X_cc), n_samples, replace=False)\n",
    "    X_cc = X_cc[indices]\n",
    "    y_cc = y_cc[indices]\n",
    "    \n",
    "    print(f\"Dataset Credit Card Fraud: {X_cc.shape[0]} campioni, {X_cc.shape[1]} features\")\n",
    "    print(f\"Numero di frodi: {np.sum(y_cc == 1)} ({np.sum(y_cc == 1) / len(y_cc) * 100:.2f}%)\")\n",
    "    \n",
    "    # Visualizzare il dataset\n",
    "    plot_dataset(X_cc, y_cc, \"Dataset Credit Card Fraud\")\n",
    "    \n",
    "    # Confrontare gli algoritmi\n",
    "    results_cc, metrics_cc = compare_anomaly_detection_algorithms(X_cc, y_cc)\n",
    "    \n",
    "    # Visualizzare le curve ROC e Precision-Recall\n",
    "    plot_roc_pr_curves(results_cc, y_cc)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Errore nel caricamento del dataset: {e}\")\n",
    "    print(\"Utilizziamo un dataset sintetico come alternativa.\")\n",
    "    \n",
    "    # Generare un dataset sintetico che simula frodi\n",
    "    X_cc, y_cc = generate_high_dim_outliers(n_samples=1000, n_outliers=20, n_features=10, random_state=42)\n",
    "    \n",
    "    print(f\"Dataset sintetico: {X_cc.shape[0]} campioni, {X_cc.shape[1]} features\")\n",
    "    print(f\"Numero di frodi: {np.sum(y_cc == 1)} ({np.sum(y_cc == 1) / len(y_cc) * 100:.2f}%)\")\n",
    "    \n",
    "    # Visualizzare il dataset\n",
    "    plot_dataset(X_cc, y_cc, \"Dataset Sintetico (Simulazione Frodi)\")\n",
    "    \n",
    "    # Confrontare gli algoritmi\n",
    "    results_cc, metrics_cc = compare_anomaly_detection_algorithms(X_cc, y_cc)\n",
    "    \n",
    "    # Visualizzare le curve ROC e Precision-Recall\n",
    "    plot_roc_pr_curves(results_cc, y_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "In questa esercitazione pratica, abbiamo esplorato diverse tecniche di rilevamento di anomalie e le abbiamo applicate a vari dataset. Abbiamo visto come:\n",
    "\n",
    "1. **Approcci statistici** come Z-score ed Elliptic Envelope sono semplici ed efficaci per anomalie globali, ma possono avere difficoltà con distribuzioni complesse.\n",
    "2. **Metodi basati sulla densità** come LOF e DBSCAN eccellono nell'identificare anomalie locali in regioni a bassa densità.\n",
    "3. **Isolation Forest** è efficiente e scalabile, particolarmente adatto per dataset ad alta dimensionalità.\n",
    "4. **One-Class SVM** può catturare confini di decisione complessi, ma è sensibile alla scelta dei parametri.\n",
    "5. **Tecniche per serie temporali** richiedono approcci specifici che tengano conto della dipendenza temporale.\n",
    "\n",
    "Abbiamo anche imparato l'importanza di:\n",
    "- Scegliere l'algoritmo appropriato in base alla natura delle anomalie e dei dati\n",
    "- Ottimizzare i parametri degli algoritmi per ottenere i migliori risultati\n",
    "- Valutare le prestazioni con metriche appropriate come precision, recall, F1-score e AUC\n",
    "- Considerare la riduzione della dimensionalità come preprocessing per dataset ad alta dimensionalità\n",
    "\n",
    "Il rilevamento di anomalie è un campo vasto con numerose applicazioni pratiche, dalla sicurezza informatica alla finanza, dalla medicina all'industria. La scelta dell'approccio giusto dipende fortemente dal contesto specifico e dalla natura dei dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizi Aggiuntivi\n",
    "\n",
    "1. Implementa un ensemble di metodi di rilevamento di anomalie e confronta le prestazioni con i singoli algoritmi.\n",
    "2. Applica le tecniche di rilevamento di anomalie a un dataset reale di tua scelta (ad esempio, dati di rete, log di sistema, dati medici).\n",
    "3. Implementa un autoencoder per il rilevamento di anomalie e confrontalo con gli altri metodi.\n",
    "4. Esplora l'effetto della normalizzazione dei dati sulle prestazioni dei diversi algoritmi di rilevamento di anomalie.\n",
    "5. Implementa una versione semplificata di Isolation Forest da zero (senza utilizzare scikit-learn) e confronta i risultati con l'implementazione di scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
