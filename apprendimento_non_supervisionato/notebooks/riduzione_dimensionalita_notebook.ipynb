{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercitazione Pratica: Tecniche di Riduzione della Dimensionalità\n",
    "\n",
    "## Corso di Machine Learning: Apprendimento Non Supervisionato\n",
    "\n",
    "Questa esercitazione pratica ti guiderà attraverso l'implementazione e l'applicazione delle principali tecniche di riduzione della dimensionalità discusse nelle lezioni teoriche. Esploreremo diversi dataset e vedremo come applicare e valutare vari metodi per ridurre la dimensionalità dei dati.\n",
    "\n",
    "### Obiettivi dell'esercitazione:\n",
    "- Implementare e applicare PCA, t-SNE, UMAP e Autoencoders\n",
    "- Visualizzare i risultati della riduzione della dimensionalità\n",
    "- Valutare la qualità delle rappresentazioni a bassa dimensione\n",
    "- Confrontare le prestazioni delle diverse tecniche\n",
    "- Applicare la riduzione della dimensionalità a casi di studio reali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurazione dell'ambiente\n",
    "\n",
    "Iniziamo importando le librerie necessarie per questa esercitazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Installare le librerie necessarie\n",
    "!pip install umap-learn\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn matplotlib pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importazione delle librerie necessarie\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.datasets import load_digits, fetch_openml, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Impostazioni di visualizzazione\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Caricamento e Preparazione dei Dataset\n",
    "\n",
    "Utilizzeremo diversi dataset per testare le tecniche di riduzione della dimensionalità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Funzione per visualizzare i dataset\n",
    "def plot_dataset(X, y, title=\"Dataset\", discrete_cmap=True):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if X.shape[1] > 2:\n",
    "        # Se il dataset ha più di 2 dimensioni, utilizziamo PCA per visualizzarlo\n",
    "        pca = PCA(n_components=2)\n",
    "        X_2d = pca.fit_transform(X)\n",
    "        plt.title(f\"{title} (PCA 2D)\\nVarianza spiegata: {pca.explained_variance_ratio_.sum():.2f}\", fontsize=14)\n",
    "    else:\n",
    "        X_2d = X\n",
    "        plt.title(title, fontsize=14)\n",
    "    \n",
    "    # Utilizziamo una colormap discreta o continua in base al tipo di target\n",
    "    if discrete_cmap:\n",
    "        scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='tab10', s=50, alpha=0.8)\n",
    "        plt.colorbar(scatter, label='Classe')\n",
    "    else:\n",
    "        scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='viridis', s=50, alpha=0.8)\n",
    "        plt.colorbar(scatter, label='Valore')\n",
    "    \n",
    "    plt.xlabel('Componente 1', fontsize=12)\n",
    "    plt.ylabel('Componente 2', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Dataset Iris (bassa dimensionalità, 4 features)\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "feature_names_iris = iris.feature_names\n",
    "target_names_iris = iris.target_names\n",
    "\n",
    "print(f\"Dataset Iris: {X_iris.shape[0]} campioni, {X_iris.shape[1]} features\")\n",
    "print(f\"Features: {feature_names_iris}\")\n",
    "print(f\"Classi: {target_names_iris}\")\n",
    "plot_dataset(X_iris, y_iris, \"Dataset Iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Dataset Digits (media dimensionalità, 64 features)\n",
    "digits = load_digits()\n",
    "X_digits, y_digits = digits.data, digits.target\n",
    "\n",
    "print(f\"Dataset Digits: {X_digits.shape[0]} campioni, {X_digits.shape[1]} features\")\n",
    "print(f\"Classi: {np.unique(y_digits)}\")\n",
    "plot_dataset(X_digits, y_digits, \"Dataset Digits (Cifre 0-9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare alcune immagini del dataset Digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    # Trovare un esempio di ogni cifra\n",
    "    idx = np.where(y_digits == i)[0][0]\n",
    "    axes[i].imshow(digits.images[idx], cmap='gray')\n",
    "    axes[i].set_title(f\"Cifra: {i}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Dataset MNIST (alta dimensionalità, 784 features)\n",
    "# Carichiamo solo un sottoinsieme per velocizzare l'esercitazione\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X_mnist = mnist.data.astype('float32').values[:5000]  # Prendiamo solo 5000 campioni\n",
    "y_mnist = mnist.target.astype('int').values[:5000]\n",
    "\n",
    "print(f\"Dataset MNIST (sottoinsieme): {X_mnist.shape[0]} campioni, {X_mnist.shape[1]} features\")\n",
    "print(f\"Classi: {np.unique(y_mnist)}\")\n",
    "plot_dataset(X_mnist, y_mnist, \"Dataset MNIST (sottoinsieme)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare alcune immagini del dataset MNIST\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    # Trovare un esempio di ogni cifra\n",
    "    idx = np.where(y_mnist == i)[0][0]\n",
    "    axes[i].imshow(X_mnist[idx].reshape(28, 28), cmap='gray')\n",
    "    axes[i].set_title(f\"Cifra: {i}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Principal Component Analysis (PCA)\n",
    "\n",
    "Implementiamo e applichiamo PCA ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementazione di PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_pca(X, n_components=2, standardize=True, title=\"PCA\"):\n",
    "    # Standardizzare i dati se richiesto\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = X.copy()\n",
    "    \n",
    "    # Applicare PCA\n",
    "    start_time = time.time()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Tempo di esecuzione PCA: {end_time - start_time:.3f} secondi\")\n",
    "    print(f\"Varianza spiegata dalle prime {n_components} componenti: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    return X_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare PCA al dataset Iris\n",
    "X_iris_pca, pca_iris = apply_pca(X_iris, n_components=2, title=\"PCA su Iris\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Classe')\n",
    "plt.title(f\"PCA su Iris\\nVarianza spiegata: {pca_iris.explained_variance_ratio_.sum():.4f}\", fontsize=14)\n",
    "plt.xlabel(f\"PC1 ({pca_iris.explained_variance_ratio_[0]:.4f})\", fontsize=12)\n",
    "plt.ylabel(f\"PC2 ({pca_iris.explained_variance_ratio_[1]:.4f})\", fontsize=12)\n",
    "\n",
    "# Aggiungere i nomi delle classi\n",
    "for i, target_name in enumerate(target_names_iris):\n",
    "    plt.annotate(target_name,\n",
    "                 xy=(X_iris_pca[y_iris == i, 0].mean(), X_iris_pca[y_iris == i, 1].mean()),\n",
    "                 xytext=(3, 3),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom',\n",
    "                 fontsize=12,\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analisi delle Componenti Principali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare i loadings delle componenti principali per Iris\n",
    "plt.figure(figsize=(12, 6))\n",
    "components = pd.DataFrame(pca_iris.components_, columns=feature_names_iris)\n",
    "sns.heatmap(components, cmap='coolwarm', annot=True, fmt='.3f')\n",
    "plt.title('Loadings delle Componenti Principali (Iris)', fontsize=14)\n",
    "plt.ylabel('Componente Principale')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare la varianza spiegata cumulativa\n",
    "def plot_explained_variance(X, max_components=None, title=\"Varianza Spiegata Cumulativa\"):\n",
    "    if max_components is None:\n",
    "        max_components = min(X.shape[0], X.shape[1])\n",
    "    \n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Applicare PCA con tutte le componenti\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Calcolare la varianza spiegata cumulativa\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(1, max_components + 1), pca.explained_variance_ratio_, alpha=0.7)\n",
    "    plt.step(range(1, max_components + 1), cumulative_variance, where='mid', color='red', alpha=0.7)\n",
    "    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=0.9, color='k', linestyle=':', alpha=0.7)\n",
    "    plt.text(max_components/2, 0.96, '95% varianza spiegata', ha='center', va='bottom')\n",
    "    plt.text(max_components/2, 0.91, '90% varianza spiegata', ha='center', va='bottom')\n",
    "    plt.title('Varianza Spiegata per Componente')\n",
    "    plt.xlabel('Componente Principale')\n",
    "    plt.ylabel('Varianza Spiegata')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, max_components + 1), cumulative_variance, 'o-', markersize=5)\n",
    "    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=0.9, color='k', linestyle=':', alpha=0.7)\n",
    "    plt.text(max_components/2, 0.96, '95% varianza spiegata', ha='center', va='bottom')\n",
    "    plt.text(max_components/2, 0.91, '90% varianza spiegata', ha='center', va='bottom')\n",
    "    plt.title('Varianza Spiegata Cumulativa')\n",
    "    plt.xlabel('Numero di Componenti')\n",
    "    plt.ylabel('Varianza Spiegata Cumulativa')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Trovare il numero di componenti necessarie per spiegare il 90% e il 95% della varianza\n",
    "    n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "    n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "    \n",
    "    print(f\"Numero di componenti necessarie per spiegare il 90% della varianza: {n_components_90}\")\n",
    "    print(f\"Numero di componenti necessarie per spiegare il 95% della varianza: {n_components_95}\")\n",
    "    \n",
    "    return n_components_90, n_components_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analizzare la varianza spiegata per il dataset Iris\n",
    "n_components_90_iris, n_components_95_iris = plot_explained_variance(X_iris, title=\"Varianza Spiegata (Iris)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analizzare la varianza spiegata per il dataset Digits\n",
    "n_components_90_digits, n_components_95_digits = plot_explained_variance(X_digits, title=\"Varianza Spiegata (Digits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualizzazione delle Componenti Principali per Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare PCA al dataset Digits\n",
    "X_digits_pca, pca_digits = apply_pca(X_digits, n_components=2, title=\"PCA su Digits\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_digits_pca[:, 0], X_digits_pca[:, 1], c=y_digits, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(f\"PCA su Digits\\nVarianza spiegata: {pca_digits.explained_variance_ratio_.sum():.4f}\", fontsize=14)\n",
    "plt.xlabel(f\"PC1 ({pca_digits.explained_variance_ratio_[0]:.4f})\", fontsize=12)\n",
    "plt.ylabel(f\"PC2 ({pca_digits.explained_variance_ratio_[1]:.4f})\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualizzare le prime componenti principali come immagini per Digits\n",
    "n_components = 10\n",
    "pca_digits_full = PCA(n_components=n_components).fit(X_digits)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_components):\n",
    "    component = pca_digits_full.components_[i].reshape(8, 8)\n",
    "    axes[i].imshow(component, cmap='viridis')\n",
    "    axes[i].set_title(f\"PC {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Prime 10 Componenti Principali (Digits)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Ricostruzione delle immagini originali da PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def reconstruct_from_pca(X, n_components_list, sample_indices=None, n_samples=5):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Se non vengono forniti indici specifici, selezionare casualmente n_samples\n",
    "    if sample_indices is None:\n",
    "        sample_indices = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "    else:\n",
    "        n_samples = len(sample_indices)\n",
    "    \n",
    "    # Preparare la figura\n",
    "    n_rows = n_samples\n",
    "    n_cols = len(n_components_list) + 1  # +1 per l'immagine originale\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "    \n",
    "    # Per ogni campione\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Visualizzare l'immagine originale\n",
    "        original_img = X[idx].reshape(8, 8)\n",
    "        axes[i, 0].imshow(original_img, cmap='gray')\n",
    "        axes[i, 0].set_title(\"Originale\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Per ogni numero di componenti\n",
    "        for j, n_comp in enumerate(n_components_list):\n",
    "            # Applicare PCA e ricostruire\n",
    "            pca = PCA(n_components=n_comp)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "            X_reconstructed = pca.inverse_transform(X_pca)\n",
    "            \n",
    "            # Destandardizzare\n",
    "            X_reconstructed = scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Visualizzare l'immagine ricostruita\n",
    "            reconstructed_img = X_reconstructed[idx].reshape(8, 8)\n",
    "            axes[i, j+1].imshow(reconstructed_img, cmap='gray')\n",
    "            axes[i, j+1].set_title(f\"{n_comp} comp.\\n({pca.explained_variance_ratio_.sum():.2f})\")\n",
    "            axes[i, j+1].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Ricostruzione delle immagini con diverse componenti PCA\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selezionare alcuni esempi di cifre diverse\n",
    "sample_indices = [np.where(y_digits == i)[0][0] for i in range(5)]\n",
    "\n",
    "# Ricostruire le immagini con diverse componenti PCA\n",
    "reconstruct_from_pca(X_digits, [5, 10, 20, 30, 40], sample_indices=sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "Implementiamo e applichiamo t-SNE ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implementazione di t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_tsne(X, n_components=2, perplexity=30, n_iter=1000, title=\"t-SNE\"):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Applicare t-SNE\n",
    "    start_time = time.time()\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Tempo di esecuzione t-SNE: {end_time - start_time:.3f} secondi\")\n",
    "    \n",
    "    return X_tsne, tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare t-SNE al dataset Iris\n",
    "X_iris_tsne, tsne_iris = apply_tsne(X_iris, perplexity=10, title=\"t-SNE su Iris\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_iris_tsne[:, 0], X_iris_tsne[:, 1], c=y_iris, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Classe')\n",
    "plt.title(\"t-SNE su Iris\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE 2\", fontsize=12)\n",
    "\n",
    "# Aggiungere i nomi delle classi\n",
    "for i, target_name in enumerate(target_names_iris):\n",
    "    plt.annotate(target_name,\n",
    "                 xy=(X_iris_tsne[y_iris == i, 0].mean(), X_iris_tsne[y_iris == i, 1].mean()),\n",
    "                 xytext=(3, 3),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom',\n",
    "                 fontsize=12,\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare t-SNE al dataset Digits\n",
    "X_digits_tsne, tsne_digits = apply_tsne(X_digits, title=\"t-SNE su Digits\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1], c=y_digits, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(\"t-SNE su Digits\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Effetto dei parametri di t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def explore_tsne_parameters(X, y, perplexity_values=[5, 30, 50], title=\"Effetto della Perplexity in t-SNE\"):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Preparare la figura\n",
    "    fig, axes = plt.subplots(1, len(perplexity_values), figsize=(15, 5))\n",
    "    \n",
    "    # Per ogni valore di perplexity\n",
    "    for i, perplexity in enumerate(perplexity_values):\n",
    "        # Applicare t-SNE\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X_scaled)\n",
    "        \n",
    "        # Visualizzare i risultati\n",
    "        scatter = axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=30, alpha=0.8)\n",
    "        axes[i].set_title(f\"Perplexity = {perplexity}\")\n",
    "        axes[i].set_xlabel(\"t-SNE 1\")\n",
    "        axes[i].set_ylabel(\"t-SNE 2\")\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(scatter, ax=axes, label='Classe')\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare l'effetto della perplexity su Iris\n",
    "explore_tsne_parameters(X_iris, y_iris, perplexity_values=[5, 15, 30], title=\"Effetto della Perplexity in t-SNE (Iris)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare l'effetto della perplexity su Digits\n",
    "explore_tsne_parameters(X_digits, y_digits, perplexity_values=[5, 30, 50], title=\"Effetto della Perplexity in t-SNE (Digits)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "Implementiamo e applichiamo UMAP ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Implementazione di UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_umap(X, n_components=2, n_neighbors=15, min_dist=0.1, title=\"UMAP\"):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Applicare UMAP\n",
    "    start_time = time.time()\n",
    "    reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Tempo di esecuzione UMAP: {end_time - start_time:.3f} secondi\")\n",
    "    \n",
    "    return X_umap, reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare UMAP al dataset Iris\n",
    "X_iris_umap, umap_iris = apply_umap(X_iris, n_neighbors=10, title=\"UMAP su Iris\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_iris_umap[:, 0], X_iris_umap[:, 1], c=y_iris, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Classe')\n",
    "plt.title(\"UMAP su Iris\", fontsize=14)\n",
    "plt.xlabel(\"UMAP 1\", fontsize=12)\n",
    "plt.ylabel(\"UMAP 2\", fontsize=12)\n",
    "\n",
    "# Aggiungere i nomi delle classi\n",
    "for i, target_name in enumerate(target_names_iris):\n",
    "    plt.annotate(target_name,\n",
    "                 xy=(X_iris_umap[y_iris == i, 0].mean(), X_iris_umap[y_iris == i, 1].mean()),\n",
    "                 xytext=(3, 3),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom',\n",
    "                 fontsize=12,\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare UMAP al dataset Digits\n",
    "X_digits_umap, umap_digits = apply_umap(X_digits, title=\"UMAP su Digits\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_digits_umap[:, 0], X_digits_umap[:, 1], c=y_digits, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(\"UMAP su Digits\", fontsize=14)\n",
    "plt.xlabel(\"UMAP 1\", fontsize=12)\n",
    "plt.ylabel(\"UMAP 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Effetto dei parametri di UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def explore_umap_parameters(X, y, n_neighbors_values=[5, 15, 30], min_dist_values=[0.01, 0.1, 0.5]):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Preparare la figura\n",
    "    fig, axes = plt.subplots(len(n_neighbors_values), len(min_dist_values), figsize=(15, 12))\n",
    "    \n",
    "    # Per ogni combinazione di parametri\n",
    "    for i, n_neighbors in enumerate(n_neighbors_values):\n",
    "        for j, min_dist in enumerate(min_dist_values):\n",
    "            # Applicare UMAP\n",
    "            reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)\n",
    "            X_umap = reducer.fit_transform(X_scaled)\n",
    "            \n",
    "            # Visualizzare i risultati\n",
    "            scatter = axes[i, j].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=30, alpha=0.8)\n",
    "            axes[i, j].set_title(f\"n_neighbors={n_neighbors}, min_dist={min_dist}\")\n",
    "            axes[i, j].set_xlabel(\"UMAP 1\")\n",
    "            axes[i, j].set_ylabel(\"UMAP 2\")\n",
    "            axes[i, j].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(scatter, ax=axes, label='Classe')\n",
    "    plt.suptitle(\"Effetto dei parametri in UMAP\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Esplorare l'effetto dei parametri di UMAP su Digits\n",
    "explore_umap_parameters(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Autoencoders per la Riduzione della Dimensionalità\n",
    "\n",
    "Implementiamo e applichiamo Autoencoders ai nostri dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Implementazione di un Autoencoder semplice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_autoencoder(input_dim, encoding_dim=2):\n",
    "    # Definire l'architettura dell'autoencoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(128, activation='relu')(input_layer)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    # Modelli\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    # Compilare il modello\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def apply_autoencoder(X, encoding_dim=2, epochs=50, batch_size=32, title=\"Autoencoder\"):\n",
    "    # Normalizzare i dati tra 0 e 1\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Creare l'autoencoder\n",
    "    autoencoder, encoder = create_autoencoder(X.shape[1], encoding_dim)\n",
    "    \n",
    "    # Addestrare l'autoencoder\n",
    "    start_time = time.time()\n",
    "    history = autoencoder.fit(X_scaled, X_scaled, epochs=epochs, batch_size=batch_size, \n",
    "                             validation_split=0.2, verbose=0)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Tempo di addestramento Autoencoder: {end_time - start_time:.3f} secondi\")\n",
    "    \n",
    "    # Visualizzare la curva di loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Curva di Loss dell\\'Autoencoder')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Codificare i dati\n",
    "    X_encoded = encoder.predict(X_scaled)\n",
    "    \n",
    "    return X_encoded, autoencoder, encoder, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare l'autoencoder al dataset Digits\n",
    "X_digits_ae, autoencoder_digits, encoder_digits, history_digits = apply_autoencoder(X_digits, epochs=50, title=\"Autoencoder su Digits\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_digits_ae[:, 0], X_digits_ae[:, 1], c=y_digits, cmap='tab10', s=50, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(\"Autoencoder su Digits\", fontsize=14)\n",
    "plt.xlabel(\"Dimensione Latente 1\", fontsize=12)\n",
    "plt.ylabel(\"Dimensione Latente 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Ricostruzione delle immagini con Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def reconstruct_from_autoencoder(X, autoencoder, sample_indices=None, n_samples=5):\n",
    "    # Normalizzare i dati tra 0 e 1\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Se non vengono forniti indici specifici, selezionare casualmente n_samples\n",
    "    if sample_indices is None:\n",
    "        sample_indices = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "    else:\n",
    "        n_samples = len(sample_indices)\n",
    "    \n",
    "    # Ricostruire le immagini\n",
    "    X_reconstructed = autoencoder.predict(X_scaled)\n",
    "    \n",
    "    # Visualizzare le immagini originali e ricostruite\n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(6, n_samples * 3))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Immagine originale\n",
    "        axes[i, 0].imshow(X[idx].reshape(8, 8), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Originale\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Immagine ricostruita\n",
    "        axes[i, 1].imshow(X_reconstructed[idx].reshape(8, 8), cmap='gray')\n",
    "        axes[i, 1].set_title(\"Ricostruita\")\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Ricostruzione delle immagini con Autoencoder\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selezionare alcuni esempi di cifre diverse\n",
    "sample_indices = [np.where(y_digits == i)[0][0] for i in range(5)]\n",
    "\n",
    "# Ricostruire le immagini con l'autoencoder\n",
    "reconstruct_from_autoencoder(X_digits, autoencoder_digits, sample_indices=sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Confronto tra Tecniche di Riduzione della Dimensionalità\n",
    "\n",
    "Confrontiamo le prestazioni delle diverse tecniche di riduzione della dimensionalità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_dimensionality_reduction(X, y, title=\"Confronto tra Tecniche di Riduzione della Dimensionalità\"):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Applicare PCA\n",
    "    start_time = time.time()\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    pca_time = time.time() - start_time\n",
    "    \n",
    "    # Applicare t-SNE\n",
    "    start_time = time.time()\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    tsne_time = time.time() - start_time\n",
    "    \n",
    "    # Applicare UMAP\n",
    "    start_time = time.time()\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    umap_time = time.time() - start_time\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # PCA\n",
    "    scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=30, alpha=0.8)\n",
    "    axes[0].set_title(f\"PCA\\nTempo: {pca_time:.2f}s\\nVarianza spiegata: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "    axes[0].set_xlabel(\"Componente 1\")\n",
    "    axes[0].set_ylabel(\"Componente 2\")\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # t-SNE\n",
    "    axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', s=30, alpha=0.8)\n",
    "    axes[1].set_title(f\"t-SNE\\nTempo: {tsne_time:.2f}s\")\n",
    "    axes[1].set_xlabel(\"t-SNE 1\")\n",
    "    axes[1].set_ylabel(\"t-SNE 2\")\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # UMAP\n",
    "    axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='tab10', s=30, alpha=0.8)\n",
    "    axes[2].set_title(f\"UMAP\\nTempo: {umap_time:.2f}s\")\n",
    "    axes[2].set_xlabel(\"UMAP 1\")\n",
    "    axes[2].set_ylabel(\"UMAP 2\")\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(scatter, ax=axes, label='Classe')\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "    # Stampare i tempi di esecuzione\n",
    "    print(f\"Tempi di esecuzione:\")\n",
    "    print(f\"PCA: {pca_time:.3f} secondi\")\n",
    "    print(f\"t-SNE: {tsne_time:.3f} secondi\")\n",
    "    print(f\"UMAP: {umap_time:.3f} secondi\")\n",
    "    \n",
    "    return X_pca, X_tsne, X_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare le tecniche sul dataset Iris\n",
    "X_pca_iris, X_tsne_iris, X_umap_iris = compare_dimensionality_reduction(X_iris, y_iris, title=\"Confronto su Iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confrontare le tecniche sul dataset Digits\n",
    "X_pca_digits, X_tsne_digits, X_umap_digits = compare_dimensionality_reduction(X_digits, y_digits, title=\"Confronto su Digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Valutazione della Qualità della Riduzione della Dimensionalità\n",
    "\n",
    "Valutiamo la qualità delle rappresentazioni a bassa dimensione utilizzando un classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_with_classifier(X_original, X_reduced_list, y, method_names, test_size=0.3):\n",
    "    results = {}\n",
    "    \n",
    "    # Dividere i dati in training e test\n",
    "    X_train_orig, X_test_orig, y_train, y_test = train_test_split(X_original, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Standardizzare i dati originali\n",
    "    scaler = StandardScaler()\n",
    "    X_train_orig = scaler.fit_transform(X_train_orig)\n",
    "    X_test_orig = scaler.transform(X_test_orig)\n",
    "    \n",
    "    # Addestrare e valutare un classificatore sui dati originali\n",
    "    knn_orig = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_orig.fit(X_train_orig, y_train)\n",
    "    y_pred_orig = knn_orig.predict(X_test_orig)\n",
    "    accuracy_orig = accuracy_score(y_test, y_pred_orig)\n",
    "    results['Originale'] = accuracy_orig\n",
    "    \n",
    "    # Per ogni metodo di riduzione della dimensionalità\n",
    "    for X_reduced, method_name in zip(X_reduced_list, method_names):\n",
    "        # Dividere i dati ridotti in training e test\n",
    "        X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_reduced, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # Addestrare e valutare un classificatore sui dati ridotti\n",
    "        knn_red = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn_red.fit(X_train_red, y_train_red)\n",
    "        y_pred_red = knn_red.predict(X_test_red)\n",
    "        accuracy_red = accuracy_score(y_test_red, y_pred_red)\n",
    "        results[method_name] = accuracy_red\n",
    "    \n",
    "    # Visualizzare i risultati\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(results.keys(), results.values(), color='skyblue')\n",
    "    plt.title('Accuratezza del Classificatore KNN', fontsize=14)\n",
    "    plt.xlabel('Metodo di Riduzione della Dimensionalità')\n",
    "    plt.ylabel('Accuratezza')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Aggiungere i valori sopra le barre\n",
    "    for i, (key, value) in enumerate(results.items()):\n",
    "        plt.text(i, value + 0.02, f\"{value:.3f}\", ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Valutare la qualità della riduzione della dimensionalità sul dataset Digits\n",
    "results_digits = evaluate_with_classifier(X_digits, [X_pca_digits, X_tsne_digits, X_umap_digits, X_digits_ae], \n",
    "                                         y_digits, ['PCA', 't-SNE', 'UMAP', 'Autoencoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Caso di Studio - Visualizzazione di MNIST\n",
    "\n",
    "Applichiamo le tecniche di riduzione della dimensionalità al dataset MNIST per visualizzare le cifre scritte a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare PCA a MNIST\n",
    "X_mnist_pca, pca_mnist = apply_pca(X_mnist, title=\"PCA su MNIST\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_mnist_pca[:, 0], X_mnist_pca[:, 1], c=y_mnist, cmap='tab10', s=5, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(f\"PCA su MNIST\\nVarianza spiegata: {pca_mnist.explained_variance_ratio_.sum():.4f}\", fontsize=14)\n",
    "plt.xlabel(f\"PC1 ({pca_mnist.explained_variance_ratio_[0]:.4f})\", fontsize=12)\n",
    "plt.ylabel(f\"PC2 ({pca_mnist.explained_variance_ratio_[1]:.4f})\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare t-SNE a MNIST\n",
    "X_mnist_tsne, tsne_mnist = apply_tsne(X_mnist, title=\"t-SNE su MNIST\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_mnist_tsne[:, 0], X_mnist_tsne[:, 1], c=y_mnist, cmap='tab10', s=5, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(\"t-SNE su MNIST\", fontsize=14)\n",
    "plt.xlabel(\"t-SNE 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Applicare UMAP a MNIST\n",
    "X_mnist_umap, umap_mnist = apply_umap(X_mnist, title=\"UMAP su MNIST\")\n",
    "\n",
    "# Visualizzare i risultati\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_mnist_umap[:, 0], X_mnist_umap[:, 1], c=y_mnist, cmap='tab10', s=5, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Cifra')\n",
    "plt.title(\"UMAP su MNIST\", fontsize=14)\n",
    "plt.xlabel(\"UMAP 1\", fontsize=12)\n",
    "plt.ylabel(\"UMAP 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 9: Applicazione Pratica - Compressione delle Immagini con PCA\n",
    "\n",
    "Utilizziamo PCA per comprimere le immagini e valutare il compromesso tra dimensionalità e qualità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compress_images_with_pca(X, n_components_list, sample_indices=None, n_samples=5):\n",
    "    # Standardizzare i dati\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Se non vengono forniti indici specifici, selezionare casualmente n_samples\n",
    "    if sample_indices is None:\n",
    "        sample_indices = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "    else:\n",
    "        n_samples = len(sample_indices)\n",
    "    \n",
    "    # Calcolare il rapporto di compressione e l'errore di ricostruzione per ogni numero di componenti\n",
    "    compression_ratios = []\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    for n_comp in n_components_list:\n",
    "        # Calcolare il rapporto di compressione\n",
    "        original_size = X.shape[1]  # Dimensione originale\n",
    "        compressed_size = n_comp * (1 + X.shape[1])  # Dimensione compressa (componenti + loadings)\n",
    "        compression_ratio = original_size / compressed_size\n",
    "        compression_ratios.append(compression_ratio)\n",
    "        \n",
    "        # Applicare PCA e ricostruire\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        X_reconstructed = pca.inverse_transform(X_pca)\n",
    "        \n",
    "        # Destandardizzare\n",
    "        X_reconstructed = scaler.inverse_transform(X_reconstructed)\n",
    "        \n",
    "        # Calcolare l'errore di ricostruzione (MSE)\n",
    "        mse = np.mean((X - X_reconstructed) ** 2)\n",
    "        reconstruction_errors.append(mse)\n",
    "    \n",
    "    # Visualizzare il compromesso tra compressione e qualità\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_components_list, compression_ratios, 'o-')\n",
    "    plt.title('Rapporto di Compressione')\n",
    "    plt.xlabel('Numero di Componenti')\n",
    "    plt.ylabel('Rapporto di Compressione')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_components_list, reconstruction_errors, 'o-')\n",
    "    plt.title('Errore di Ricostruzione (MSE)')\n",
    "    plt.xlabel('Numero di Componenti')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizzare esempi di immagini compresse\n",
    "    fig, axes = plt.subplots(n_samples, len(n_components_list) + 1, figsize=(2 * (len(n_components_list) + 1), 2 * n_samples))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Visualizzare l'immagine originale\n",
    "        axes[i, 0].imshow(X[idx].reshape(8, 8), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Originale\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Visualizzare le immagini compresse\n",
    "        for j, n_comp in enumerate(n_components_list):\n",
    "            # Applicare PCA e ricostruire\n",
    "            pca = PCA(n_components=n_comp)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "            X_reconstructed = pca.inverse_transform(X_pca)\n",
    "            \n",
    "            # Destandardizzare\n",
    "            X_reconstructed = scaler.inverse_transform(X_reconstructed)\n",
    "            \n",
    "            # Visualizzare l'immagine ricostruita\n",
    "            axes[i, j+1].imshow(X_reconstructed[idx].reshape(8, 8), cmap='gray')\n",
    "            axes[i, j+1].set_title(f\"{n_comp} comp.\\nCR: {compression_ratios[j]:.2f}\")\n",
    "            axes[i, j+1].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Compressione delle Immagini con PCA\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "    \n",
    "    return compression_ratios, reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selezionare alcuni esempi di cifre diverse\n",
    "sample_indices = [np.where(y_digits == i)[0][0] for i in range(5)]\n",
    "\n",
    "# Comprimere le immagini con PCA\n",
    "compression_ratios, reconstruction_errors = compress_images_with_pca(X_digits, [5, 10, 20, 30, 40], sample_indices=sample_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusioni\n",
    "\n",
    "In questa esercitazione pratica, abbiamo esplorato diverse tecniche di riduzione della dimensionalità e le abbiamo applicate a vari dataset. Abbiamo visto come:\n",
    "\n",
    "1. **PCA** è efficace per la compressione dei dati e la rimozione di caratteristiche ridondanti, ma è limitata a trasformazioni lineari.\n",
    "2. **t-SNE** eccelle nella visualizzazione dei dati e nella preservazione della struttura locale, ma è computazionalmente costosa e non generalizzabile a nuovi dati.\n",
    "3. **UMAP** offre un buon compromesso tra preservazione della struttura locale e globale, con tempi di esecuzione migliori rispetto a t-SNE.\n",
    "4. **Autoencoders** sono flessibili e possono catturare relazioni non lineari complesse, ma richiedono più dati e tempo per l'addestramento.\n",
    "\n",
    "Abbiamo anche imparato l'importanza di:\n",
    "- Scegliere la tecnica appropriata in base all'obiettivo (visualizzazione, compressione, estrazione di caratteristiche)\n",
    "- Valutare la qualità della riduzione della dimensionalità con metriche appropriate\n",
    "- Considerare il compromesso tra dimensionalità e informazione preservata\n",
    "- Ottimizzare i parametri delle diverse tecniche per ottenere i migliori risultati\n",
    "\n",
    "La riduzione della dimensionalità è uno strumento potente nel machine learning, che ci permette di visualizzare, comprendere e lavorare con dati complessi in modo più efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizi Aggiuntivi\n",
    "\n",
    "1. Applica PCA, t-SNE e UMAP all'intero dataset MNIST e confronta i risultati.\n",
    "2. Implementa un Autoencoder Variazionale (VAE) e confrontalo con un Autoencoder tradizionale.\n",
    "3. Esplora l'effetto della normalizzazione dei dati sulle diverse tecniche di riduzione della dimensionalità.\n",
    "4. Applica la riduzione della dimensionalità a un dataset di testi (ad esempio, utilizzando TF-IDF o word embeddings).\n",
    "5. Implementa una versione semplificata di PCA da zero (senza utilizzare scikit-learn) e confronta i risultati con l'implementazione di scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
